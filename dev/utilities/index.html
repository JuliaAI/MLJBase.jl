<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Utilities · MLJBase.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">MLJBase.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../measures/">Measures</a></li><li><a class="tocitem" href="../resampling/">Resampling</a></li><li><a class="tocitem" href="../composition/">Composition</a></li><li><a class="tocitem" href="../datasets/">Datasets</a></li><li><a class="tocitem" href="../distributions/">Distributions</a></li><li><a class="tocitem" href="../openml/">OpenML</a></li><li class="is-active"><a class="tocitem" href>Utilities</a><ul class="internal"><li><a class="tocitem" href="#Machines-1"><span>Machines</span></a></li><li><a class="tocitem" href="#Parameter-Inspection-1"><span>Parameter Inspection</span></a></li><li><a class="tocitem" href="#Show-1"><span>Show</span></a></li><li><a class="tocitem" href="#Utility-functions-1"><span>Utility functions</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Utilities</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Utilities</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/master/docs/src/utilities.md#L" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Utilities-1"><a class="docs-heading-anchor" href="#Utilities-1">Utilities</a><a class="docs-heading-anchor-permalink" href="#Utilities-1" title="Permalink"></a></h1><h2 id="Machines-1"><a class="docs-heading-anchor" href="#Machines-1">Machines</a><a class="docs-heading-anchor-permalink" href="#Machines-1" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="MLJBase.fit_only!-Union{Tuple{Machine{var&quot;#s13&quot;,cache_data} where var&quot;#s13&quot;&lt;:Model}, Tuple{cache_data}} where cache_data" href="#MLJBase.fit_only!-Union{Tuple{Machine{var&quot;#s13&quot;,cache_data} where var&quot;#s13&quot;&lt;:Model}, Tuple{cache_data}} where cache_data"><code>MLJBase.fit_only!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">MLJBase.fit_only!(mach::Machine; rows=nothing, verbosity=1, force=false)</code></pre><p>Without mutating any other machine on which it may depend, perform one of the following actions to the machine <code>mach</code>, using the data and model bound to it, and restricting the data to <code>rows</code> if specified:</p><ul><li><p><em>Ab initio training.</em> Ignoring any previous learned parameters and cache, compute and store new learned parameters. Increment <code>mach.state</code>.</p></li><li><p><em>Training update.</em> Making use of previous learned parameters and/or  cache, replace or mutate existing learned parameters. The effect is  the same (or nearly the same) as in ab initio training, but may be  faster or use less memory, assuming the model supports an update  option (implements <code>MLJBase.update</code>). Increment <code>mach.state</code>.</p></li><li><p><em>No-operation.</em> Leave existing learned parameters untouched. Do not  increment <code>mach.state</code>.</p></li></ul><p><strong>Training action logic</strong></p><p>For the action to be a no-operation, either <code>mach.frozen == true</code> or or none of the following apply:</p><ul><li><p>(i) <code>mach</code> has never been trained (<code>mach.state == 0</code>).</p></li><li><p>(ii) <code>force == true</code>.</p></li><li><p>(iii) The <code>state</code> of some other machine on which <code>mach</code> depends has changed since the last time <code>mach</code> was trained (ie, the last time <code>mach.state</code> was last incremented).</p></li><li><p>(iv) The specified <code>rows</code> have changed since the last retraining and <code>mach.model</code> does not have <code>Static</code> type.</p></li><li><p>(v) <code>mach.model</code> has changed since the last retraining.</p></li></ul><p>In any of the cases (i) - (iv), <code>mach</code> is trained ab initio. If only (v) fails, then a training update is applied.</p><p>To freeze or unfreeze <code>mach</code>, use <code>freeze!(mach)</code> or <code>thaw!(mach)</code>.</p><p><strong>Implementation detail</strong></p><p>The data to which a machine is bound is stored in <code>mach.args</code>. Each element of <code>args</code> is either a <code>Node</code> object, or, in the case that concrete data was bound to the machine, it is concrete data wrapped in a <code>Source</code> node. In all cases, to obtain concrete data for actual training, each argument <code>N</code> is called, as in <code>N()</code> or <code>N(rows=rows)</code>, and either <code>MLJBase.fit</code> (ab initio training) or <code>MLJBase.update</code> (training update) is dispatched on <code>mach.model</code> and this data. See the &quot;Adding models for general use&quot; section of the MLJ documentation for more on these lower-level training methods.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/machines.jl#LL360-L416">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.freeze!-Tuple{Machine}" href="#MLJBase.freeze!-Tuple{Machine}"><code>MLJBase.freeze!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">freeze!(mach)</code></pre><p>Freeze the machine <code>mach</code> so that it will never be retrained (unless thawed).</p><p>See also <a href="#MLJBase.thaw!-Tuple{Machine}"><code>thaw!</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/machines.jl#LL264-L271">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.glb-Tuple{Machine{var&quot;#s255&quot;,C} where C where var&quot;#s255&quot;&lt;:Union{DeterministicSurrogate, IntervalSurrogate, JointProbabilisticSurrogate, ProbabilisticSurrogate, StaticSurrogate, UnsupervisedSurrogate, DeterministicComposite, IntervalComposite, JointProbabilisticComposite, ProbabilisticComposite, StaticComposite, UnsupervisedComposite}}" href="#MLJBase.glb-Tuple{Machine{var&quot;#s255&quot;,C} where C where var&quot;#s255&quot;&lt;:Union{DeterministicSurrogate, IntervalSurrogate, JointProbabilisticSurrogate, ProbabilisticSurrogate, StaticSurrogate, UnsupervisedSurrogate, DeterministicComposite, IntervalComposite, JointProbabilisticComposite, ProbabilisticComposite, StaticComposite, UnsupervisedComposite}}"><code>MLJBase.glb</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">N = glb(mach::Machine{&lt;:Surrogate})</code></pre><p>A greatest lower bound for the nodes appearing in the signature of <code>mach</code>.</p><p><strong>Private method.</strong></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/composition/learning_networks/machines.jl#LL111-L119">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.machine" href="#MLJBase.machine"><code>MLJBase.machine</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">machine(model, args...; cache=true)</code></pre><p>Construct a <code>Machine</code> object binding a <code>model</code>, storing hyper-parameters of some machine learning algorithm, to some data, <code>args</code>. When building a learning network, <code>Node</code> objects can be substituted for concrete data. Specify <code>cache=false</code> to prioritize memory managment over speed, and to guarantee data anonymity when serializing composite models.</p><pre><code class="language-none">machine(Xs; oper1=node1, oper2=node2)
machine(Xs, ys; oper1=node1, oper2=node2)
machine(Xs, ys, extras...; oper1=node1, oper2=node2, ...)</code></pre><p>Construct a special machine called a <em>learning network machine</em>, that &quot;wraps&quot; a learning network, usually in preparation to export the network as a stand-alone composite model type. The keyword arguments declare what nodes are called when operations, such as <code>predict</code> and <code>transform</code>, are called on the machine.</p><p>In addition to the operations named in the constructor, the methods <code>fit!</code>, <code>report</code>, and <code>fitted_params</code> can be applied as usual to the machine constructed.</p><pre><code class="language-none">machine(Probablistic(), args...; kwargs...)
machine(Deterministic(), args...; kwargs...)
machine(Unsupervised(), args...; kwargs...)
machine(Static(), args...; kwargs...)</code></pre><p>Same as above, but specifying explicitly the kind of model the learning network is to meant to represent.</p><p>Learning network machines are not to be confused with an ordinary machine that happens to be bound to a stand-alone composite model (i.e., an <em>exported</em> learning network).</p><p><strong>Examples</strong></p><p>Supposing a supervised learning network&#39;s final predictions are obtained by calling a node <code>yhat</code>, then the code</p><pre><code class="language-julia">mach = machine(Deterministic(), Xs, ys; predict=yhat)
fit!(mach; rows=train)
predictions = predict(mach, Xnew) # `Xnew` concrete data</code></pre><p>is  equivalent to</p><pre><code class="language-julia">fit!(yhat, rows=train)
predictions = yhat(Xnew)</code></pre><p>Here <code>Xs</code> and <code>ys</code> are the source nodes receiving, respectively, the input and target data.</p><p>In a unsupervised learning network for clustering, with single source node <code>Xs</code> for inputs, and in which the node <code>Xout</code> delivers the output of dimension reduction, and <code>yhat</code> the class labels, one can write</p><pre><code class="language-julia">mach = machine(Unsupervised(), Xs; transform=Xout, predict=yhat)
fit!(mach)
transformed = transform(mach, Xnew) # `Xnew` concrete data
predictions = predict(mach, Xnew)</code></pre><p>which is equivalent to</p><pre><code class="language-julia">fit!(Xout)
fit!(yhat)
transformed = Xout(Xnew)
predictions = yhat(Xnew)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/machines.jl#LL138-L216">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.report-Tuple{Machine}" href="#MLJBase.report-Tuple{Machine}"><code>MLJBase.report</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">report(mach)</code></pre><p>Return the report for a machine <code>mach</code> that has been <code>fit!</code>, for example the coefficients in a linear model.</p><p>This is a named tuple and human-readable if possible.</p><p>If <code>mach</code> is a machine for a composite model, such as a model constructed using <code>@pipeline</code>, then the returned named tuple has the composite type&#39;s field names as keys. The corresponding value is the report for the machine in the underlying learning network bound to that model. (If multiple machines share the same model, then the value is a vector.)</p><pre><code class="language-julia">using MLJ
@load LinearBinaryClassifier pkg=GLM
X, y = @load_crabs;
pipe = @pipeline Standardizer LinearBinaryClassifier
mach = machine(pipe, X, y) |&gt; fit!

julia&gt; report(mach).linear_binary_classifier
(deviance = 3.8893386087844543e-7,
 dof_residual = 195.0,
 stderror = [18954.83496713119, 6502.845740757159, 48484.240246060406, 34971.131004997274, 20654.82322484894, 2111.1294584763386],
 vcov = [3.592857686311793e8 9.122732393971942e6 … -8.454645589364915e7 5.38856837634321e6; 9.122732393971942e6 4.228700272808351e7 … -4.978433790526467e7 -8.442545425533723e6; … ; -8.454645589364915e7 -4.978433790526467e7 … 4.2662172244975924e8 2.1799125705781363e7; 5.38856837634321e6 -8.442545425533723e6 … 2.1799125705781363e7 4.456867590446599e6],)
</code></pre><p>Additional keys, <code>machines</code> and <code>report_given_machine</code>, give a list of <em>all</em> machines in the underlying network, and a dictionary of reports keyed on those machines.</p><p>```</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/machines.jl#LL615-L651">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.return!-Tuple{Machine{var&quot;#s255&quot;,C} where C where var&quot;#s255&quot;&lt;:Union{DeterministicSurrogate, IntervalSurrogate, JointProbabilisticSurrogate, ProbabilisticSurrogate, StaticSurrogate, UnsupervisedSurrogate},Union{Nothing, Model},Any}" href="#MLJBase.return!-Tuple{Machine{var&quot;#s255&quot;,C} where C where var&quot;#s255&quot;&lt;:Union{DeterministicSurrogate, IntervalSurrogate, JointProbabilisticSurrogate, ProbabilisticSurrogate, StaticSurrogate, UnsupervisedSurrogate},Union{Nothing, Model},Any}"><code>MLJBase.return!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">return!(mach::Machine{&lt;:Surrogate}, model, verbosity)</code></pre><p>The last call in custom code defining the <code>MLJBase.fit</code> method for a new composite model type. Here <code>model</code> is the instance of the new type appearing in the <code>MLJBase.fit</code> signature, while <code>mach</code> is a learning network machine constructed using <code>model</code>. Not relevant when defining composite models using <code>@pipeline</code> or <code>@from_network</code>.</p><p>For usage, see the example given below. Specificlly, the call does the following:</p><ul><li><p>Determines which fields of <code>model</code> point to model instances in the learning network wrapped by <code>mach</code>, for recording in an object called <code>cache</code>, for passing onto the MLJ logic that handles smart updating (namely, an <code>MLJBase.update</code> fallback for composite models).</p></li><li><p>Calls <code>fit!(mach, verbosity=verbosity)</code>.</p></li><li><p>Moves any data in source nodes of the learning network into <code>cache</code> (for data-anonymization purposes).</p></li><li><p>Records a copy of <code>model</code> in <code>cache</code>.</p></li><li><p>Returns <code>cache</code> and outcomes of training in an appropriate form (specifically, <code>(mach.fitresult, cache, mach.report)</code>; see <a href="https://alan-turing-institute.github.io/MLJ.jl/dev/adding_models_for_general_use/">Adding Models for General Use</a> for technical details.)</p></li></ul><p><strong>Example</strong></p><p>The following code defines, &quot;by hand&quot;, a new model type <code>MyComposite</code> for composing standardization (whitening) with a deterministic regressor:</p><pre><code class="language-none">mutable struct MyComposite &lt;: DeterministicComposite
    regressor
end

function MLJBase.fit(model::MyComposite, verbosity, X, y)
    Xs = source(X)
    ys = source(y)

    mach1 = machine(Standardizer(), Xs)
    Xwhite = transform(mach1, Xs)

    mach2 = machine(model.regressor, Xwhite, ys)
    yhat = predict(mach2, Xwhite)

    mach = machine(Deterministic(), Xs, ys; predict=yhat)
    return!(mach, model, verbosity)
end</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/composition/learning_networks/machines.jl#LL214-L272">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.thaw!-Tuple{Machine}" href="#MLJBase.thaw!-Tuple{Machine}"><code>MLJBase.thaw!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">thaw!(mach)</code></pre><p>Unfreeze the machine <code>mach</code> so that it can be retrained.</p><p>See also <a href="#MLJBase.freeze!-Tuple{Machine}"><code>freeze!</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/machines.jl#LL276-L282">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJModelInterface.fitted_params-Tuple{Machine}" href="#MLJModelInterface.fitted_params-Tuple{Machine}"><code>MLJModelInterface.fitted_params</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">fitted_params(mach)</code></pre><p>Return the learned parameters for a machine <code>mach</code> that has been <code>fit!</code>, for example the coefficients in a linear model.</p><p>This is a named tuple and human-readable if possible.</p><p>If <code>mach</code> is a machine for a composite model, such as a model constructed using <code>@pipeline</code>, then the returned named tuple has the composite type&#39;s field names as keys. The corresponding value is the fitted parameters for the machine in the underlying learning network bound to that model. (If multiple machines share the same model, then the value is a vector.)</p><pre><code class="language-julia">using MLJ
@load LogisticClassifier pkg=MLJLinearModels
X, y = @load_crabs;
pipe = @pipeline Standardizer LogisticClassifier
mach = machine(pipe, X, y) |&gt; fit!

julia&gt; fitted_params(mach).logistic_classifier
(classes = CategoricalArrays.CategoricalValue{String,UInt32}[&quot;B&quot;, &quot;O&quot;],
 coefs = Pair{Symbol,Float64}[:FL =&gt; 3.7095037897680405, :RW =&gt; 0.1135739140854546, :CL =&gt; -1.6036892745322038, :CW =&gt; -4.415667573486482, :BD =&gt; 3.238476051092471],
 intercept = 0.0883301599726305,)</code></pre><p>Additional keys, <code>machines</code> and <code>fitted_params_given_machine</code>, give a list of <em>all</em> machines in the underlying network, and a dictionary of fitted parameters keyed on those machines.</p><p>```</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/machines.jl#LL572-L606">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="StatsBase.fit!-Tuple{Machine{var&quot;#s252&quot;,C} where C where var&quot;#s252&quot;&lt;:Union{DeterministicSurrogate, IntervalSurrogate, JointProbabilisticSurrogate, ProbabilisticSurrogate, StaticSurrogate, UnsupervisedSurrogate}}" href="#StatsBase.fit!-Tuple{Machine{var&quot;#s252&quot;,C} where C where var&quot;#s252&quot;&lt;:Union{DeterministicSurrogate, IntervalSurrogate, JointProbabilisticSurrogate, ProbabilisticSurrogate, StaticSurrogate, UnsupervisedSurrogate}}"><code>StatsBase.fit!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">fit!(mach::Machine{&lt;:Surrogate};
     rows=nothing,
     acceleration=CPU1(),
     verbosity=1,
     force=false))</code></pre><p>Train the complete learning network wrapped by the machine <code>mach</code>.</p><p>More precisely, if <code>s</code> is the learning network signature used to construct <code>mach</code>, then call <code>fit!(N)</code>, where <code>N = glb(values(s)...)</code> is a greatest lower bound on the nodes appearing in the signature. For example, if <code>s = (predict=yhat, transform=W)</code>, then call <code>fit!(glb(yhat, W))</code>. Here <code>glb</code> is <code>tuple</code> overloaded for nodes.</p><p>See also <a href="#MLJBase.machine"><code>machine</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/composition/learning_networks/machines.jl#LL124-L142">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="StatsBase.fit!-Tuple{Machine}" href="#StatsBase.fit!-Tuple{Machine}"><code>StatsBase.fit!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">fit!(mach::Machine, rows=nothing, verbosity=1, force=false)</code></pre><p>Fit the machine <code>mach</code>. In the case that <code>mach</code> has <code>Node</code> arguments, first train all other machines on which <code>mach</code> depends.</p><p>To attempt to fit a machine without touching any other machine, use <code>fit_only!</code>. For more on the internal logic of fitting see <a href="#MLJBase.fit_only!-Union{Tuple{Machine{var&quot;#s13&quot;,cache_data} where var&quot;#s13&quot;&lt;:Model}, Tuple{cache_data}} where cache_data"><code>fit_only!</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/machines.jl#LL521-L532">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.replace-Tuple{Machine{var&quot;#s251&quot;,C} where C where var&quot;#s251&quot;&lt;:Union{DeterministicSurrogate, IntervalSurrogate, JointProbabilisticSurrogate, ProbabilisticSurrogate, StaticSurrogate, UnsupervisedSurrogate},Vararg{Pair,N} where N}" href="#Base.replace-Tuple{Machine{var&quot;#s251&quot;,C} where C where var&quot;#s251&quot;&lt;:Union{DeterministicSurrogate, IntervalSurrogate, JointProbabilisticSurrogate, ProbabilisticSurrogate, StaticSurrogate, UnsupervisedSurrogate},Vararg{Pair,N} where N}"><code>Base.replace</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">replace(mach, a1=&gt;b1, a2=&gt;b2, ...; empty_unspecified_sources=false)</code></pre><p>Create a deep copy of a learning network machine <code>mach</code> but replacing any specified sources and models <code>a1, a2, ...</code> of the original underlying network with <code>b1, b2, ...</code>.</p><p>If <code>empty_unspecified_sources=true</code> then any source nodes not specified are replaced with empty source nodes.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/composition/learning_networks/machines.jl#LL322-L332">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.ancestors-Tuple{Machine}" href="#MLJBase.ancestors-Tuple{Machine}"><code>MLJBase.ancestors</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">ancestors(mach::Machine; self=false)</code></pre><p>All ancestors of <code>mach</code>, including <code>mach</code> if <code>self=true</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/machines.jl#LL47-L52">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.model_supertype-Tuple{Any}" href="#MLJBase.model_supertype-Tuple{Any}"><code>MLJBase.model_supertype</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">model_supertype(signature)</code></pre><p>Return, if this can be deduced, which of <code>Deterministic</code>, <code>Probabilistic</code> and <code>Unsupervised</code> is the appropriate supertype for a composite model obtained by exporting a learning network with the specified <code>signature</code>.</p><p>A learning network <em>signature</em> is a named tuple, such as <code>(predict=yhat, transfrom=W)</code>, specifying what nodes of the network are called to produce output of each operation represented by the keys, in an exported version of the network.</p><p>If a supertype cannot be deduced, <code>nothing</code> is returned.</p><p>If the network with given <code>signature</code> is not exportable, this method will not error but it will not a give meaningful return value either.</p><p><strong>Private method.</strong></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/composition/learning_networks/machines.jl#LL9-L29">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJModelInterface.save-Tuple{Union{IO, String},Machine}" href="#MLJModelInterface.save-Tuple{Union{IO, String},Machine}"><code>MLJModelInterface.save</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">MLJ.save(filename, mach::Machine; kwargs...)
MLJ.save(io, mach::Machine; kwargs...)

MLJBase.save(filename, mach::Machine; kwargs...)
MLJBase.save(io, mach::Machine; kwargs...)</code></pre><p>Serialize the machine <code>mach</code> to a file with path <code>filename</code>, or to an input/output stream <code>io</code> (at least <code>IOBuffer</code> instances are supported).</p><p>The format is JLSO (a wrapper for julia native or BSON serialization). For some model types, a custom serialization will be additionally performed.</p><p><strong>Keyword arguments</strong></p><p>These keyword arguments are passed to the JLSO serializer:</p><table><tr><th style="text-align: right">keyword</th><th style="text-align: right">values</th><th style="text-align: right">default</th></tr><tr><td style="text-align: right"><code>format</code></td><td style="text-align: right"><code>:julia_serialize</code>, <code>:BSON</code></td><td style="text-align: right"><code>:julia_serialize</code></td></tr><tr><td style="text-align: right"><code>compression</code></td><td style="text-align: right"><code>:gzip</code>, <code>:none</code></td><td style="text-align: right"><code>:none</code></td></tr></table><p>See <a href="https://github.com/invenia/JLSO.jl">https://github.com/invenia/JLSO.jl</a> for details.</p><p>Any additional keyword arguments are passed to model-specific serializers.</p><p>Machines are de-serialized using the <code>machine</code> constructor as shown in the example below. Data (or nodes) may be optionally passed to the constructor for retraining on new data using the saved model.</p><p><strong>Example</strong></p><pre><code class="language-none">using MLJ
tree = @load DecisionTreeClassifier
X, y = @load_iris
mach = fit!(machine(tree, X, y))

MLJ.save(&quot;tree.jlso&quot;, mach, compression=:none)
mach_predict_only = machine(&quot;tree.jlso&quot;)
predict(mach_predict_only, X)

mach2 = machine(&quot;tree.jlso&quot;, selectrows(X, 1:100), y[1:100])
predict(mach2, X) # same as above

fit!(mach2) # saved learned parameters are over-written
predict(mach2, X) # not same as above

# using a buffer:
io = IOBuffer()
MLJ.save(io, mach)
seekstart(io)
predict_only_mach = machine(io)
predict(predict_only_mach, X)</code></pre><div class="admonition is-warning"><header class="admonition-header">Only load files from trusted sources</header><div class="admonition-body"><p>Maliciously constructed JLSO files, like pickles, and most other general purpose serialization formats, can allow for arbitrary code execution during loading. This means it is possible for someone to use a JLSO file that looks like a serialized MLJ machine as a <a href="https://en.wikipedia.org/wiki/Trojan_horse_(computing)">Trojan horse</a>.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/machines.jl#LL662-L728">source</a></section></article><h2 id="Parameter-Inspection-1"><a class="docs-heading-anchor" href="#Parameter-Inspection-1">Parameter Inspection</a><a class="docs-heading-anchor-permalink" href="#Parameter-Inspection-1" title="Permalink"></a></h2><h2 id="Show-1"><a class="docs-heading-anchor" href="#Show-1">Show</a><a class="docs-heading-anchor-permalink" href="#Show-1" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="MLJBase.color_off-Tuple{}" href="#MLJBase.color_off-Tuple{}"><code>MLJBase.color_off</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">color_off()</code></pre><p>Suppress color and bold output at the REPL for displaying MLJ objects.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/show.jl#LL12-L17">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.color_on-Tuple{}" href="#MLJBase.color_on-Tuple{}"><code>MLJBase.color_on</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">color_on()</code></pre><p>Enable color and bold output at the REPL, for enhanced display of MLJ objects.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/show.jl#LL5-L10">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.@constant-Tuple{Any}" href="#MLJBase.@constant-Tuple{Any}"><code>MLJBase.@constant</code></a> — <span class="docstring-category">Macro</span></header><section><div><pre><code class="language-julia">@constant x = value</code></pre><p>Equivalent to <code>const x = value</code> but registers the binding thus:</p><pre><code class="language-none">MLJBase.HANDLE_GIVEN_ID[objectid(value)] = :x</code></pre><p>Registered objects get displayed using the variable name to which it was bound in calls to <code>show(x)</code>, etc.</p><p>WARNING: As with any <code>const</code> declaration, binding <code>x</code> to new value of the same type is not prevented and the registration will not be updated.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/show.jl#LL25-L38">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.@more-Tuple{}" href="#MLJBase.@more-Tuple{}"><code>MLJBase.@more</code></a> — <span class="docstring-category">Macro</span></header><section><div><pre><code class="language-julia">@more</code></pre><p>Entered at the REPL, equivalent to <code>show(ans, 100)</code>. Use to get a recursive description of all fields of the last REPL value.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/show.jl#LL210-L216">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase._recursive_show-Tuple{IO,MLJType,Any,Any}" href="#MLJBase._recursive_show-Tuple{IO,MLJType,Any,Any}"><code>MLJBase._recursive_show</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">_recursive_show(stream, object, current_depth, depth)</code></pre><p>Generate a table of the field values of the <code>MLJType</code> object, dislaying each value by calling the method <code>_show</code> on it. The behaviour of <code>_show(stream, f)</code> is as follows:</p><ol><li>If <code>f</code> is itself a <code>MLJType</code> object, then its short form is shown</li></ol><p>and <code>_recursive_show</code> generates as separate table for each of its field values (and so on, up to a depth of argument <code>depth</code>).</p><ol><li>Otherwise <code>f</code> is displayed as &quot;(omitted T)&quot; where <code>T = typeof(f)</code>,</li></ol><p>unless <code>istoobig(f)</code> is false (the <code>istoobig</code> fall-back for arbitrary types being <code>true</code>). In the latter case, the long (ie, MIME&quot;plain/text&quot;) form of <code>f</code> is shown. To override this behaviour, overload the <code>_show</code> method for the type in question.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/show.jl#LL308-L325">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.abbreviated-Tuple{Any}" href="#MLJBase.abbreviated-Tuple{Any}"><code>MLJBase.abbreviated</code></a> — <span class="docstring-category">Method</span></header><section><div><p>to display abbreviated versions of integers</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/show.jl#LL62">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.handle-Tuple{Any}" href="#MLJBase.handle-Tuple{Any}"><code>MLJBase.handle</code></a> — <span class="docstring-category">Method</span></header><section><div><p>return abbreviated object id (as string) or it&#39;s registered handle (as string) if this exists</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/show.jl#LL68-L70">source</a></section></article><h2 id="Utility-functions-1"><a class="docs-heading-anchor" href="#Utility-functions-1">Utility functions</a><a class="docs-heading-anchor-permalink" href="#Utility-functions-1" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="LossFunctions.DWDMarginLoss" href="#LossFunctions.DWDMarginLoss"><code>LossFunctions.DWDMarginLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">LossFunctions.DWDMarginLoss</code></pre><p>A measure type for distance weighted discrimination loss, which includes the instance(s), <code>dwd_margin_loss</code>.</p><pre><code class="language-none">DWDMarginLoss()(ŷ, y)
DWDMarginLoss()(ŷ, y, w)</code></pre><p>Evaluate the default instance of DWDMarginLoss on observations <code>ŷ</code>, given ground truth values <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p>See above for original LossFunctions.jl documentation. </p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Finite{2}}</code> (binary classification); <code>ŷ</code> must be a probabilistic prediction. </p><p>Constructor signature: <code>DWDMarginLoss(; q=1.0)</code></p><p>For more information, run <code>info(DWDMarginLoss)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/measures/meta_utilities.jl#LL76-L92">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LossFunctions.ExpLoss" href="#LossFunctions.ExpLoss"><code>LossFunctions.ExpLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">LossFunctions.ExpLoss</code></pre><p>A measure type for exp loss, which includes the instance(s), <code>exp_loss</code>.</p><pre><code class="language-none">ExpLoss()(ŷ, y)
ExpLoss()(ŷ, y, w)</code></pre><p>Evaluate the exp loss on observations <code>ŷ</code>, given ground truth values <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p>See above for original LossFunctions.jl documentation. </p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Finite{2}}</code> (binary classification); <code>ŷ</code> must be a probabilistic prediction. </p><p>For more information, run <code>info(ExpLoss)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/measures/meta_utilities.jl#LL76-L90">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LossFunctions.HuberLoss" href="#LossFunctions.HuberLoss"><code>LossFunctions.HuberLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">LossFunctions.HuberLoss</code></pre><p>A measure type for huber loss, which includes the instance(s), <code>huber_loss</code>.</p><pre><code class="language-none">HuberLoss()(ŷ, y)
HuberLoss()(ŷ, y, w)</code></pre><p>Evaluate the default instance of HuberLoss on observations <code>ŷ</code>, given ground truth values <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p>See above for original LossFunctions.jl documentation. </p><p>Requires <code>scitype(y)</code> to be a subtype of Union{AbstractArray{ScientificTypes.Continuous,1}, AbstractArray{ScientificTypes.Count,1}}; <code>ŷ</code> must be a deterministic prediction. </p><p>For more information, run <code>info(HuberLoss)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/measures/meta_utilities.jl#LL76-L90">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LossFunctions.L1EpsilonInsLoss" href="#LossFunctions.L1EpsilonInsLoss"><code>LossFunctions.L1EpsilonInsLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">LossFunctions.L1EpsilonInsLoss</code></pre><p>A measure type for l1 ϵ-insensitive loss, which includes the instance(s), <code>l1_epsilon_ins_loss</code>.</p><pre><code class="language-none">L1EpsilonInsLoss()(ŷ, y)
L1EpsilonInsLoss()(ŷ, y, w)</code></pre><p>Evaluate the default instance of L1EpsilonInsLoss on observations <code>ŷ</code>, given ground truth values <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p>See above for original LossFunctions.jl documentation. </p><p>Requires <code>scitype(y)</code> to be a subtype of Union{AbstractArray{ScientificTypes.Continuous,1}, AbstractArray{ScientificTypes.Count,1}}; <code>ŷ</code> must be a deterministic prediction. </p><p>Constructor signature: <code>L1EpsilonInsLoss(; ϵ=1.0)</code></p><p>For more information, run <code>info(L1EpsilonInsLoss)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/measures/meta_utilities.jl#LL76-L92">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LossFunctions.L1HingeLoss" href="#LossFunctions.L1HingeLoss"><code>LossFunctions.L1HingeLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">LossFunctions.L1HingeLoss</code></pre><p>A measure type for l1 hinge loss, which includes the instance(s), <code>l1_hinge_loss</code>.</p><pre><code class="language-none">L1HingeLoss()(ŷ, y)
L1HingeLoss()(ŷ, y, w)</code></pre><p>Evaluate the l1 hinge loss on observations <code>ŷ</code>, given ground truth values <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p>See above for original LossFunctions.jl documentation. </p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Finite{2}}</code> (binary classification); <code>ŷ</code> must be a probabilistic prediction. </p><p>For more information, run <code>info(L1HingeLoss)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/measures/meta_utilities.jl#LL76-L90">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LossFunctions.L2EpsilonInsLoss" href="#LossFunctions.L2EpsilonInsLoss"><code>LossFunctions.L2EpsilonInsLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">LossFunctions.L2EpsilonInsLoss</code></pre><p>A measure type for l2 ϵ-insensitive loss, which includes the instance(s), <code>l2_epsilon_ins_loss</code>.</p><pre><code class="language-none">L2EpsilonInsLoss()(ŷ, y)
L2EpsilonInsLoss()(ŷ, y, w)</code></pre><p>Evaluate the default instance of L2EpsilonInsLoss on observations <code>ŷ</code>, given ground truth values <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p>See above for original LossFunctions.jl documentation. </p><p>Requires <code>scitype(y)</code> to be a subtype of Union{AbstractArray{ScientificTypes.Continuous,1}, AbstractArray{ScientificTypes.Count,1}}; <code>ŷ</code> must be a deterministic prediction. </p><p>Constructor signature: <code>L2EpsilonInsLoss(; ϵ=1.0)</code></p><p>For more information, run <code>info(L2EpsilonInsLoss)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/measures/meta_utilities.jl#LL76-L92">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LossFunctions.L2HingeLoss" href="#LossFunctions.L2HingeLoss"><code>LossFunctions.L2HingeLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">LossFunctions.L2HingeLoss</code></pre><p>A measure type for l2 hinge loss, which includes the instance(s), <code>l2_hinge_loss</code>.</p><pre><code class="language-none">L2HingeLoss()(ŷ, y)
L2HingeLoss()(ŷ, y, w)</code></pre><p>Evaluate the l2 hinge loss on observations <code>ŷ</code>, given ground truth values <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p>See above for original LossFunctions.jl documentation. </p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Finite{2}}</code> (binary classification); <code>ŷ</code> must be a probabilistic prediction. </p><p>For more information, run <code>info(L2HingeLoss)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/measures/meta_utilities.jl#LL76-L90">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LossFunctions.L2MarginLoss" href="#LossFunctions.L2MarginLoss"><code>LossFunctions.L2MarginLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">LossFunctions.L2MarginLoss</code></pre><p>A measure type for l2 margin loss, which includes the instance(s), <code>l2_margin_loss</code>.</p><pre><code class="language-none">L2MarginLoss()(ŷ, y)
L2MarginLoss()(ŷ, y, w)</code></pre><p>Evaluate the l2 margin loss on observations <code>ŷ</code>, given ground truth values <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p>See above for original LossFunctions.jl documentation. </p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Finite{2}}</code> (binary classification); <code>ŷ</code> must be a probabilistic prediction. </p><p>For more information, run <code>info(L2MarginLoss)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/measures/meta_utilities.jl#LL76-L90">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LossFunctions.LPDistLoss" href="#LossFunctions.LPDistLoss"><code>LossFunctions.LPDistLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">LossFunctions.LPDistLoss</code></pre><p>A measure type for lp dist loss, which includes the instance(s), <code>lp_dist_loss</code>.</p><pre><code class="language-none">LPDistLoss()(ŷ, y)
LPDistLoss()(ŷ, y, w)</code></pre><p>Evaluate the lp dist loss on observations <code>ŷ</code>, given ground truth values <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p>See above for original LossFunctions.jl documentation. </p><p>Requires <code>scitype(y)</code> to be a subtype of Union{AbstractArray{ScientificTypes.Continuous,1}, AbstractArray{ScientificTypes.Count,1}}; <code>ŷ</code> must be a deterministic prediction. </p><p>Constructor signature: <code>LPDistLoss(; P=2)</code></p><p>For more information, run <code>info(LPDistLoss)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/measures/meta_utilities.jl#LL76-L92">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LossFunctions.LogitDistLoss" href="#LossFunctions.LogitDistLoss"><code>LossFunctions.LogitDistLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">LossFunctions.LogitDistLoss</code></pre><p>A measure type for logit dist loss, which includes the instance(s), <code>logit_dist_loss</code>.</p><pre><code class="language-none">LogitDistLoss()(ŷ, y)
LogitDistLoss()(ŷ, y, w)</code></pre><p>Evaluate the logit dist loss on observations <code>ŷ</code>, given ground truth values <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p>See above for original LossFunctions.jl documentation. </p><p>Requires <code>scitype(y)</code> to be a subtype of Union{AbstractArray{ScientificTypes.Continuous,1}, AbstractArray{ScientificTypes.Count,1}}; <code>ŷ</code> must be a deterministic prediction. </p><p>For more information, run <code>info(LogitDistLoss)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/measures/meta_utilities.jl#LL76-L90">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LossFunctions.LogitMarginLoss" href="#LossFunctions.LogitMarginLoss"><code>LossFunctions.LogitMarginLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">LossFunctions.LogitMarginLoss</code></pre><p>A measure type for logit margin loss, which includes the instance(s), <code>logit_margin_loss</code>.</p><pre><code class="language-none">LogitMarginLoss()(ŷ, y)
LogitMarginLoss()(ŷ, y, w)</code></pre><p>Evaluate the logit margin loss on observations <code>ŷ</code>, given ground truth values <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p>See above for original LossFunctions.jl documentation. </p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Finite{2}}</code> (binary classification); <code>ŷ</code> must be a probabilistic prediction. </p><p>For more information, run <code>info(LogitMarginLoss)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/measures/meta_utilities.jl#LL76-L90">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LossFunctions.ModifiedHuberLoss" href="#LossFunctions.ModifiedHuberLoss"><code>LossFunctions.ModifiedHuberLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">LossFunctions.ModifiedHuberLoss</code></pre><p>A measure type for modified huber loss, which includes the instance(s), <code>modified_huber_loss</code>.</p><pre><code class="language-none">ModifiedHuberLoss()(ŷ, y)
ModifiedHuberLoss()(ŷ, y, w)</code></pre><p>Evaluate the modified huber loss on observations <code>ŷ</code>, given ground truth values <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p>See above for original LossFunctions.jl documentation. </p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Finite{2}}</code> (binary classification); <code>ŷ</code> must be a probabilistic prediction. </p><p>For more information, run <code>info(ModifiedHuberLoss)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/measures/meta_utilities.jl#LL76-L90">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LossFunctions.PerceptronLoss" href="#LossFunctions.PerceptronLoss"><code>LossFunctions.PerceptronLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">LossFunctions.PerceptronLoss</code></pre><p>A measure type for perceptron loss, which includes the instance(s), <code>perceptron_loss</code>.</p><pre><code class="language-none">PerceptronLoss()(ŷ, y)
PerceptronLoss()(ŷ, y, w)</code></pre><p>Evaluate the perceptron loss on observations <code>ŷ</code>, given ground truth values <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p>See above for original LossFunctions.jl documentation. </p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Finite{2}}</code> (binary classification); <code>ŷ</code> must be a probabilistic prediction. </p><p>For more information, run <code>info(PerceptronLoss)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/measures/meta_utilities.jl#LL76-L90">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LossFunctions.PeriodicLoss" href="#LossFunctions.PeriodicLoss"><code>LossFunctions.PeriodicLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">LossFunctions.PeriodicLoss</code></pre><p>A measure type for periodic loss, which includes the instance(s), <code>periodic_loss</code>.</p><pre><code class="language-none">PeriodicLoss()(ŷ, y)
PeriodicLoss()(ŷ, y, w)</code></pre><p>Evaluate the default instance of PeriodicLoss on observations <code>ŷ</code>, given ground truth values <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p>See above for original LossFunctions.jl documentation. </p><p>Requires <code>scitype(y)</code> to be a subtype of Union{AbstractArray{ScientificTypes.Continuous,1}, AbstractArray{ScientificTypes.Count,1}}; <code>ŷ</code> must be a deterministic prediction. </p><p>For more information, run <code>info(PeriodicLoss)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/measures/meta_utilities.jl#LL76-L90">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LossFunctions.QuantileLoss" href="#LossFunctions.QuantileLoss"><code>LossFunctions.QuantileLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">LossFunctions.QuantileLoss</code></pre><p>A measure type for quantile loss, which includes the instance(s), <code>quantile_loss</code>.</p><pre><code class="language-none">QuantileLoss()(ŷ, y)
QuantileLoss()(ŷ, y, w)</code></pre><p>Evaluate the default instance of QuantileLoss on observations <code>ŷ</code>, given ground truth values <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p>See above for original LossFunctions.jl documentation. </p><p>Requires <code>scitype(y)</code> to be a subtype of Union{AbstractArray{ScientificTypes.Continuous,1}, AbstractArray{ScientificTypes.Count,1}}; <code>ŷ</code> must be a deterministic prediction. </p><p>Constructor signature: <code>QuantileLoss(; τ=0.7)</code></p><p>For more information, run <code>info(QuantileLoss)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/measures/meta_utilities.jl#LL76-L92">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LossFunctions.SigmoidLoss" href="#LossFunctions.SigmoidLoss"><code>LossFunctions.SigmoidLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">LossFunctions.SigmoidLoss</code></pre><p>A measure type for sigmoid loss, which includes the instance(s), <code>sigmoid_loss</code>.</p><pre><code class="language-none">SigmoidLoss()(ŷ, y)
SigmoidLoss()(ŷ, y, w)</code></pre><p>Evaluate the sigmoid loss on observations <code>ŷ</code>, given ground truth values <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p>See above for original LossFunctions.jl documentation. </p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Finite{2}}</code> (binary classification); <code>ŷ</code> must be a probabilistic prediction. </p><p>For more information, run <code>info(SigmoidLoss)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/measures/meta_utilities.jl#LL76-L90">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LossFunctions.SmoothedL1HingeLoss" href="#LossFunctions.SmoothedL1HingeLoss"><code>LossFunctions.SmoothedL1HingeLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">LossFunctions.SmoothedL1HingeLoss</code></pre><p>A measure type for smoothed l1 hinge loss, which includes the instance(s), <code>smoothed_l1_hinge_loss</code>.</p><pre><code class="language-none">SmoothedL1HingeLoss()(ŷ, y)
SmoothedL1HingeLoss()(ŷ, y, w)</code></pre><p>Evaluate the default instance of SmoothedL1HingeLoss on observations <code>ŷ</code>, given ground truth values <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p>See above for original LossFunctions.jl documentation. </p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Finite{2}}</code> (binary classification); <code>ŷ</code> must be a probabilistic prediction. </p><p>Constructor signature: <code>SmoothedL1HingeLoss(; γ=1.0)</code></p><p>For more information, run <code>info(SmoothedL1HingeLoss)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/measures/meta_utilities.jl#LL76-L92">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="LossFunctions.ZeroOneLoss" href="#LossFunctions.ZeroOneLoss"><code>LossFunctions.ZeroOneLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">LossFunctions.ZeroOneLoss</code></pre><p>A measure type for zero one loss, which includes the instance(s), <code>zero_one_loss</code>.</p><pre><code class="language-none">ZeroOneLoss()(ŷ, y)
ZeroOneLoss()(ŷ, y, w)</code></pre><p>Evaluate the zero one loss on observations <code>ŷ</code>, given ground truth values <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p>See above for original LossFunctions.jl documentation. </p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Finite{2}}</code> (binary classification); <code>ŷ</code> must be a probabilistic prediction. </p><p>For more information, run <code>info(ZeroOneLoss)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/measures/meta_utilities.jl#LL76-L90">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.Accuracy" href="#MLJBase.Accuracy"><code>MLJBase.Accuracy</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">MLJBase.Accuracy</code></pre><p>A measure type for accuracy, which includes the instance(s), <code>accuracy</code>.</p><pre><code class="language-none">Accuracy()(ŷ, y)
Accuracy()(ŷ, y, w)</code></pre><p>Evaluate the accuracy on observations <code>ŷ</code>, given ground truth values <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p>Accuracy is proportion of correct predictions <code>ŷ[i]</code> that match the ground truth <code>y[i]</code> observations. This metric is invariant to class reordering.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Finite}</code> (multiclass classification); <code>ŷ</code> must be a deterministic prediction. </p><p>For more information, run <code>info(Accuracy)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/measures/meta_utilities.jl#LL76-L92">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.AreaUnderCurve" href="#MLJBase.AreaUnderCurve"><code>MLJBase.AreaUnderCurve</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">MLJBase.AreaUnderCurve</code></pre><p>A measure type for area under the ROC, which includes the instance(s), <code>area_under_curve</code>, <code>auc</code>.</p><pre><code class="language-none">AreaUnderCurve()(ŷ, y)</code></pre><p>Evaluate the area under the ROC on observations <code>ŷ</code>, given ground truth values <code>y</code>. </p><p>Returns the area under the ROC (<a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic">receiver operator characteristic</a>) This metric is invariant to class reordering.</p><p>Requires <code>scitype(y)</code> to be a subtype of AbstractArray{var&quot;#s584&quot;,1} where var&quot;#s584&quot;&lt;:ScientificTypes.Finite{2}; <code>ŷ</code> must be a probabilistic prediction. </p><p>For more information, run <code>info(AreaUnderCurve)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/measures/meta_utilities.jl#LL76-L92">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.BalancedAccuracy" href="#MLJBase.BalancedAccuracy"><code>MLJBase.BalancedAccuracy</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">MLJBase.BalancedAccuracy</code></pre><p>A measure type for balanced accuracy, which includes the instance(s), <code>balanced_accuracy</code>, <code>bacc</code>, <code>bac</code>.</p><pre><code class="language-none">BalancedAccuracy()(ŷ, y)
BalancedAccuracy()(ŷ, y, w)</code></pre><p>Evaluate the balanced accuracy on observations <code>ŷ</code>, given ground truth values <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p>Balanced accuracy compensates standard <a href="#MLJBase.Accuracy"><code>Accuracy</code></a> for class imbalance. See <a href="https://en.wikipedia.org/wiki/Precision_and_recall#Imbalanced_data">https://en.wikipedia.org/wiki/Precision<em>and</em>recall#Imbalanced_data</a>. This metric is invariant to class reordering.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Finite}</code> (multiclass classification); <code>ŷ</code> must be a deterministic prediction. </p><p>For more information, run <code>info(BalancedAccuracy)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/measures/meta_utilities.jl#LL76-L93">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.BrierLoss" href="#MLJBase.BrierLoss"><code>MLJBase.BrierLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">MLJBase.BrierLoss</code></pre><p>A measure type for Brier loss (a.k.a. quadratic loss), which includes the instance(s), <code>brier_loss</code>.</p><pre><code class="language-none">BrierLoss()(ŷ, y)
BrierLoss()(ŷ, y, w)</code></pre><p>Evaluate the Brier loss (a.k.a. quadratic loss) on observations <code>ŷ</code>, given ground truth values <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p>If <code>p(y)</code> is the predicted probability for a <em>single</em> observation <code>y</code>, and <code>C</code> all possible classes, then the corresponding Brier score for that observation is given by</p><p><span>$\left(\sum_{η ∈ C} p(η)^2\right) - 2p(y) + 1$</span></p><p><em>Warning.</em> In Brier&#39;s original 1950 paper, what is implemented here is called a &quot;loss&quot;. It is, however, a &quot;score&quot; in the contemporary use of that term: smaller is better (with <code>0</code> optimal, and all other values positive).  Note also the present implementation does not treat the binary case as special, so that the loss may differ, in that case, by a factor of two from usage elsewhere.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Finite}</code> (multiclass classification); <code>ŷ</code> must be a probabilistic prediction. </p><p>For more information, run <code>info(BrierLoss)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/measures/meta_utilities.jl#LL76-L102">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.BrierScore" href="#MLJBase.BrierScore"><code>MLJBase.BrierScore</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">MLJBase.BrierScore</code></pre><p>A measure type for Brier score (a.k.a. quadratic score), which includes the instance(s), <code>brier_score</code>.</p><pre><code class="language-none">BrierScore()(ŷ, y)
BrierScore()(ŷ, y, w)</code></pre><p>Evaluate the Brier score (a.k.a. quadratic score) on observations <code>ŷ</code>, given ground truth values <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p>If <code>p(y)</code> is the predicted probability for a <em>single</em> observation <code>y</code>, and <code>C</code> all possible classes, then the corresponding Brier score for that observation is given by</p><p><span>$2p(y) - \left(\sum_{η ∈ C} p(η)^2\right) - 1$</span></p><p><em>Warning.</em> <code>BrierScore()</code> is a &quot;score&quot; in the sense that bigger is better (with <code>0</code> optimal, and all other values negative). In Brier&#39;s original 1950 paper, and many other places, it has the opposite sign, despite the name. Moreover, the present implementation does not treat the binary case as special, so that the score may differ, in that case, by a factor of two from usage elsewhere.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Finite}</code> (multiclass classification); <code>ŷ</code> must be a probabilistic prediction. </p><p>For more information, run <code>info(BrierScore)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/measures/meta_utilities.jl#LL76-L102">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.ConfusionMatrix" href="#MLJBase.ConfusionMatrix"><code>MLJBase.ConfusionMatrix</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">MLJBase.ConfusionMatrix</code></pre><p>A measure type for confusion matrix, which includes the instance(s), <code>confusion_matrix</code>, <code>confmat</code>.</p><pre><code class="language-none">ConfusionMatrix()(ŷ, y)</code></pre><p>Evaluate the default instance of ConfusionMatrix on observations <code>ŷ</code>, given ground truth values <code>y</code>. </p><p>If <code>r</code> is the return value, then the raw confusion matrix is <code>r.mat</code>, whose rows correspond to predictions, and columns to ground truth. The ordering follows that of <code>levels(y)</code>.</p><p>Use <code>ConfusionMatrix(perm=[2, 1])</code> to reverse the class order for binary data. For more than two classes, specify an appropriate permutation, as in <code>ConfusionMatrix(perm=[2, 3, 1])</code>.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:OrderedFactor{2}}</code> (binary classification where choice of &quot;true&quot; effects the measure); <code>ŷ</code> must be a deterministic prediction. </p><p>For more information, run <code>info(ConfusionMatrix)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/measures/meta_utilities.jl#LL76-L96">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.FScore" href="#MLJBase.FScore"><code>MLJBase.FScore</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">MLJBase.FScore</code></pre><p>A measure type for F-Score, which includes the instance(s), <code>f1score</code>.</p><pre><code class="language-none">FScore()(ŷ, y)</code></pre><p>Evaluate the default instance of FScore on observations <code>ŷ</code>, given ground truth values <code>y</code>. </p><p>This is the one-parameter generalization, <span>$F_β$</span>, of the F-measure or balanced F-score.</p><p><a href="https://en.wikipedia.org/wiki/F1_score">https://en.wikipedia.org/wiki/F1_score</a></p><p>Constructor signature: <code>FScore(; β=1.0, rev=true)</code>.</p><p>By default, the second element of <code>levels(y)</code> is designated as <code>true</code>. To reverse roles, specify <code>rev=true</code>.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:OrderedFactor{2}}</code> (binary classification where choice of &quot;true&quot; effects the measure); <code>ŷ</code> must be a deterministic prediction. </p><p>Constructor signature: <code>FScore(β=1.0, rev=false)</code>. </p><p>For more information, run <code>info(FScore)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/measures/meta_utilities.jl#LL76-L100">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.FalseDiscoveryRate" href="#MLJBase.FalseDiscoveryRate"><code>MLJBase.FalseDiscoveryRate</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">MLJBase.FalseDiscoveryRate</code></pre><p>A measure type for false discovery rate, which includes the instance(s), <code>false_discovery_rate</code>, <code>falsediscovery_rate</code>, <code>fdr</code>.</p><pre><code class="language-none">FalseDiscoveryRate()(ŷ, y)</code></pre><p>Evaluate the default instance of FalseDiscoveryRate on observations <code>ŷ</code>, given ground truth values <code>y</code>. </p><p>Assigns <code>false</code> to first element of <code>levels(y)</code>. To reverse roles, use <code>FalseDiscoveryRate(rev=true)</code>.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:OrderedFactor{2}}</code> (binary classification where choice of &quot;true&quot; effects the measure); <code>ŷ</code> must be a deterministic prediction. </p><p>For more information, run <code>info(FalseDiscoveryRate)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/measures/meta_utilities.jl#LL76-L91">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.FalseNegative" href="#MLJBase.FalseNegative"><code>MLJBase.FalseNegative</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">MLJBase.FalseNegative</code></pre><p>A measure type for number of false negatives, which includes the instance(s), <code>false_negative</code>, <code>falsenegative</code>.</p><pre><code class="language-none">FalseNegative()(ŷ, y)</code></pre><p>Evaluate the default instance of FalseNegative on observations <code>ŷ</code>, given ground truth values <code>y</code>. </p><p>Assigns <code>false</code> to first element of <code>levels(y)</code>. To reverse roles, use <code>FalseNegative(rev=true)</code>.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:OrderedFactor{2}}</code> (binary classification where choice of &quot;true&quot; effects the measure); <code>ŷ</code> must be a deterministic prediction. </p><p>For more information, run <code>info(FalseNegative)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/measures/meta_utilities.jl#LL76-L91">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.FalseNegativeRate" href="#MLJBase.FalseNegativeRate"><code>MLJBase.FalseNegativeRate</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">MLJBase.FalseNegativeRate</code></pre><p>A measure type for false negative rate, which includes the instance(s), <code>false_negative_rate</code>, <code>falsenegative_rate</code>, <code>fnr</code>, <code>miss_rate</code>.</p><pre><code class="language-none">FalseNegativeRate()(ŷ, y)</code></pre><p>Evaluate the default instance of FalseNegativeRate on observations <code>ŷ</code>, given ground truth values <code>y</code>. </p><p>Assigns <code>false</code> to first element of <code>levels(y)</code>. To reverse roles, use <code>FalseNegativeRate(rev=true)</code>.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:OrderedFactor{2}}</code> (binary classification where choice of &quot;true&quot; effects the measure); <code>ŷ</code> must be a deterministic prediction. </p><p>For more information, run <code>info(FalseNegativeRate)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/measures/meta_utilities.jl#LL76-L91">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.FalsePositive" href="#MLJBase.FalsePositive"><code>MLJBase.FalsePositive</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">MLJBase.FalsePositive</code></pre><p>A measure type for number of false positives, which includes the instance(s), <code>false_positive</code>, <code>falsepositive</code>.</p><pre><code class="language-none">FalsePositive()(ŷ, y)</code></pre><p>Evaluate the default instance of FalsePositive on observations <code>ŷ</code>, given ground truth values <code>y</code>. </p><p>Assigns <code>false</code> to first element of <code>levels(y)</code>. To reverse roles, use <code>FalsePositive(rev=true)</code>.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:OrderedFactor{2}}</code> (binary classification where choice of &quot;true&quot; effects the measure); <code>ŷ</code> must be a deterministic prediction. </p><p>For more information, run <code>info(FalsePositive)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/measures/meta_utilities.jl#LL76-L91">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.FalsePositiveRate" href="#MLJBase.FalsePositiveRate"><code>MLJBase.FalsePositiveRate</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">MLJBase.FalsePositiveRate</code></pre><p>A measure type for false positive rate, which includes the instance(s), <code>false_positive_rate</code>, <code>falsepositive_rate</code>, <code>fpr</code>, <code>fallout</code>.</p><pre><code class="language-none">FalsePositiveRate()(ŷ, y)</code></pre><p>Evaluate the default instance of FalsePositiveRate on observations <code>ŷ</code>, given ground truth values <code>y</code>. </p><p>Assigns <code>false</code> to first element of <code>levels(y)</code>. To reverse roles, use <code>FalsePositiveRate(rev=true)</code>.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:OrderedFactor{2}}</code> (binary classification where choice of &quot;true&quot; effects the measure); <code>ŷ</code> must be a deterministic prediction. </p><p>For more information, run <code>info(FalsePositiveRate)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/measures/meta_utilities.jl#LL76-L91">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.LPLoss" href="#MLJBase.LPLoss"><code>MLJBase.LPLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">MLJBase.LPLoss</code></pre><p>A measure type for lp loss, which includes the instance(s), <code>l1</code>, <code>l2</code>.</p><pre><code class="language-none">LPLoss()(ŷ, y)
LPLoss()(ŷ, y, w)</code></pre><p>Evaluate the default instance of LPLoss on observations <code>ŷ</code>, given ground truth values <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p>Constructor signature: <code>LPLoss(p=2)</code>. Reports <code>|ŷ[i] - y[i]|^p</code> for every index <code>i</code>.</p><p>Requires <code>scitype(y)</code> to be a subtype of Union{AbstractArray{ScientificTypes.Continuous,1}, AbstractArray{ScientificTypes.Count,1}}; <code>ŷ</code> must be a deterministic prediction. </p><p>For more information, run <code>info(LPLoss)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/measures/meta_utilities.jl#LL76-L92">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.LogCoshLoss" href="#MLJBase.LogCoshLoss"><code>MLJBase.LogCoshLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">MLJBase.LogCoshLoss</code></pre><p>A measure type for log cosh loss, which includes the instance(s), <code>log_cosh</code>, <code>log_cosh_loss</code>.</p><pre><code class="language-none">LogCoshLoss()(ŷ, y)</code></pre><p>Evaluate the log cosh loss on observations <code>ŷ</code>, given ground truth values <code>y</code>. </p><p>Reports <span>$\log(\cosh(ŷᵢ-yᵢ))$</span> for each index <code>i</code>. </p><p>Requires <code>scitype(y)</code> to be a subtype of Union{AbstractArray{ScientificTypes.Continuous,1}, AbstractArray{ScientificTypes.Count,1}}; <code>ŷ</code> must be a deterministic prediction. </p><p>For more information, run <code>info(LogCoshLoss)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/measures/meta_utilities.jl#LL76-L89">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.LogLoss" href="#MLJBase.LogLoss"><code>MLJBase.LogLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">MLJBase.LogLoss</code></pre><p>A measure type for log loss, which includes the instance(s), <code>log_loss</code>, <code>cross_entropy</code>.</p><pre><code class="language-none">LogLoss()(ŷ, y)</code></pre><p>Evaluate the default instance of LogLoss on observations <code>ŷ</code>, given ground truth values <code>y</code>. </p><p>Since the score is undefined in the case that the true observation is predicted to occur with probability zero, probablities are clipped between <code>tol</code> and <code>1-tol</code>, where <code>tol</code> is a constructor key-word argument.</p><p>If <code>sᵢ</code> is the predicted probability for the true class <code>yᵢ</code> then the score for that example is given by</p><pre><code class="language-none">-log(clamp(sᵢ, tol), 1 - tol)</code></pre><p>A score is reported for every observation.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Finite}</code> (multiclass classification); <code>ŷ</code> must be a probabilistic prediction. </p><p>For more information, run <code>info(LogLoss)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/measures/meta_utilities.jl#LL76-L100">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.MatthewsCorrelation" href="#MLJBase.MatthewsCorrelation"><code>MLJBase.MatthewsCorrelation</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">MLJBase.MatthewsCorrelation</code></pre><p>A measure type for matthews correlation, which includes the instance(s), <code>matthews_correlation</code>, <code>mcc</code>.</p><pre><code class="language-none">MatthewsCorrelation()(ŷ, y)</code></pre><p>Evaluate the matthews correlation on observations <code>ŷ</code>, given ground truth values <code>y</code>. </p><p><a href="https://en.wikipedia.org/wiki/Matthews_correlation_coefficient">https://en.wikipedia.org/wiki/Matthews<em>correlation</em>coefficient</a> This metric is invariant to class reordering.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Finite{2}}</code> (binary classification); <code>ŷ</code> must be a deterministic prediction. </p><p>For more information, run <code>info(MatthewsCorrelation)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/measures/meta_utilities.jl#LL76-L91">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.MeanAbsoluteError" href="#MLJBase.MeanAbsoluteError"><code>MLJBase.MeanAbsoluteError</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">MLJBase.MeanAbsoluteError</code></pre><p>A measure type for mean absolute error, which includes the instance(s), <code>mae</code>, <code>mav</code>, <code>mean_absolute_error</code>, <code>mean_absolute_value</code>.</p><pre><code class="language-none">MeanAbsoluteError()(ŷ, y)
MeanAbsoluteError()(ŷ, y, w)</code></pre><p>Evaluate the mean absolute error on observations <code>ŷ</code>, given ground truth values <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p><span>$\text{mean absolute error} =  n^{-1}∑ᵢ|yᵢ-ŷᵢ|$</span> or <span>$\text{mean absolute error} = n^{-1}∑ᵢwᵢ|yᵢ-ŷᵢ|$</span></p><p>Requires <code>scitype(y)</code> to be a subtype of Union{AbstractArray{ScientificTypes.Continuous,1}, AbstractArray{ScientificTypes.Count,1}}; <code>ŷ</code> must be a deterministic prediction. </p><p>For more information, run <code>info(MeanAbsoluteError)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/measures/meta_utilities.jl#LL76-L92">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.MeanAbsoluteProportionalError" href="#MLJBase.MeanAbsoluteProportionalError"><code>MLJBase.MeanAbsoluteProportionalError</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">MLJBase.MeanAbsoluteProportionalError</code></pre><p>A measure type for mean absolute proportional error, which includes the instance(s), <code>mape</code>.</p><pre><code class="language-none">MeanAbsoluteProportionalError()(ŷ, y)</code></pre><p>Evaluate the default instance of MeanAbsoluteProportionalError on observations <code>ŷ</code>, given ground truth values <code>y</code>. </p><p>Constructor key-word arguments: <code>tol</code> (default = <code>eps()</code>).</p><p><span>$\text{mean absolute proportional error} =  m^{-1}∑ᵢ|{(yᵢ-ŷᵢ) \over yᵢ}|$</span></p><p>where the sum is over indices such that <code>abs(yᵢ) &gt; tol</code> and <code>m</code> is the number of such indices.</p><p>Requires <code>scitype(y)</code> to be a subtype of Union{AbstractArray{ScientificTypes.Continuous,1}, AbstractArray{ScientificTypes.Count,1}}; <code>ŷ</code> must be a deterministic prediction. </p><p>For more information, run <code>info(MeanAbsoluteProportionalError)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/measures/meta_utilities.jl#LL76-L95">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.MisclassificationRate" href="#MLJBase.MisclassificationRate"><code>MLJBase.MisclassificationRate</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">MLJBase.MisclassificationRate</code></pre><p>A measure type for misclassification rate, which includes the instance(s), <code>misclassification_rate</code>, <code>mcr</code>.</p><pre><code class="language-none">MisclassificationRate()(ŷ, y)
MisclassificationRate()(ŷ, y, w)</code></pre><p>Evaluate the misclassification rate on observations <code>ŷ</code>, given ground truth values <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p>A confusion matrix can also be passed as argument. This metric is invariant to class reordering.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Finite}</code> (multiclass classification); <code>ŷ</code> must be a deterministic prediction. </p><p>For more information, run <code>info(MisclassificationRate)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/measures/meta_utilities.jl#LL76-L92">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.NegativePredictiveValue" href="#MLJBase.NegativePredictiveValue"><code>MLJBase.NegativePredictiveValue</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">MLJBase.NegativePredictiveValue</code></pre><p>A measure type for negative predictive value, which includes the instance(s), <code>negative_predictive_value</code>, <code>negativepredictive_value</code>, <code>npv</code>.</p><pre><code class="language-none">NegativePredictiveValue()(ŷ, y)</code></pre><p>Evaluate the default instance of NegativePredictiveValue on observations <code>ŷ</code>, given ground truth values <code>y</code>. </p><p>Assigns <code>false</code> to first element of <code>levels(y)</code>. To reverse roles, use <code>NegativePredictiveValue(rev=true)</code>.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:OrderedFactor{2}}</code> (binary classification where choice of &quot;true&quot; effects the measure); <code>ŷ</code> must be a deterministic prediction. </p><p>For more information, run <code>info(NegativePredictiveValue)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/measures/meta_utilities.jl#LL76-L91">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.Precision" href="#MLJBase.Precision"><code>MLJBase.Precision</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">MLJBase.Precision</code></pre><p>A measure type for precision (a.k.a. positive predictive value), which includes the instance(s), <code>positive_predictive_value</code>, <code>ppv</code>, <code>positivepredictive_value</code>, <code>precision</code>.</p><pre><code class="language-none">Precision()(ŷ, y)</code></pre><p>Evaluate the default instance of Precision on observations <code>ŷ</code>, given ground truth values <code>y</code>. </p><p>Assigns <code>false</code> to first element of <code>levels(y)</code>. To reverse roles, use <code>Precision(rev=true)</code>.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:OrderedFactor{2}}</code> (binary classification where choice of &quot;true&quot; effects the measure); <code>ŷ</code> must be a deterministic prediction. </p><p>For more information, run <code>info(Precision)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/measures/meta_utilities.jl#LL76-L91">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.RootMeanSquaredError" href="#MLJBase.RootMeanSquaredError"><code>MLJBase.RootMeanSquaredError</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">MLJBase.RootMeanSquaredError</code></pre><p>A measure type for root mean squared error, which includes the instance(s), <code>rms</code>, <code>rmse</code>, <code>root_mean_squared_error</code>.</p><pre><code class="language-none">RootMeanSquaredError()(ŷ, y)
RootMeanSquaredError()(ŷ, y, w)</code></pre><p>Evaluate the root mean squared error on observations <code>ŷ</code>, given ground truth values <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p><span>$\text{root mean squared error} = \sqrt{n^{-1}∑ᵢ|yᵢ-ŷᵢ|^2}$</span> or <span>$\text{root mean squared error} = \sqrt{\frac{∑ᵢwᵢ|yᵢ-ŷᵢ|^2}{∑ᵢwᵢ}}$</span></p><p>Requires <code>scitype(y)</code> to be a subtype of Union{AbstractArray{ScientificTypes.Continuous,1}, AbstractArray{ScientificTypes.Count,1}}; <code>ŷ</code> must be a deterministic prediction. </p><p>For more information, run <code>info(RootMeanSquaredError)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/measures/meta_utilities.jl#LL76-L92">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.RootMeanSquaredLogError" href="#MLJBase.RootMeanSquaredLogError"><code>MLJBase.RootMeanSquaredLogError</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">MLJBase.RootMeanSquaredLogError</code></pre><p>A measure type for root mean squared log error, which includes the instance(s), <code>rmsl</code>, <code>rmsle</code>, <code>root_mean_squared_log_error</code>.</p><pre><code class="language-none">RootMeanSquaredLogError()(ŷ, y)</code></pre><p>Evaluate the root mean squared log error on observations <code>ŷ</code>, given ground truth values <code>y</code>. </p><p><span>$\text{root mean squared log error} = n^{-1}∑ᵢ\log\left({yᵢ \over ŷᵢ}\right)$</span></p><p>Requires <code>scitype(y)</code> to be a subtype of Union{AbstractArray{ScientificTypes.Continuous,1}, AbstractArray{ScientificTypes.Count,1}}; <code>ŷ</code> must be a deterministic prediction. </p><p>See also <a href="@ref"><code>rmslp1</code></a>.</p><p>For more information, run <code>info(RootMeanSquaredLogError)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/measures/meta_utilities.jl#LL76-L93">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.RootMeanSquaredLogProportionalError" href="#MLJBase.RootMeanSquaredLogProportionalError"><code>MLJBase.RootMeanSquaredLogProportionalError</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">MLJBase.RootMeanSquaredLogProportionalError</code></pre><p>A measure type for root mean squared log proportional error, which includes the instance(s), <code>rmslp1</code>.</p><pre><code class="language-none">RootMeanSquaredLogProportionalError()(ŷ, y)</code></pre><p>Evaluate the default instance of RootMeanSquaredLogProportionalError on observations <code>ŷ</code>, given ground truth values <code>y</code>. </p><p>Constructor signature: <code>RootMeanSquaredLogProportionalError(; offset = 1.0)</code>.</p><p><span>$\text{root mean squared log proportional error} = n^{-1}∑ᵢ\log\left({yᵢ + \text{offset} \over ŷᵢ + \text{offset}}\right)$</span></p><p>Requires <code>scitype(y)</code> to be a subtype of Union{AbstractArray{ScientificTypes.Continuous,1}, AbstractArray{ScientificTypes.Count,1}}; <code>ŷ</code> must be a deterministic prediction. </p><p>See also <a href="@ref"><code>rmsl</code></a>. </p><p>For more information, run <code>info(RootMeanSquaredLogProportionalError)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/measures/meta_utilities.jl#LL76-L95">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.RootMeanSquaredProportionalError" href="#MLJBase.RootMeanSquaredProportionalError"><code>MLJBase.RootMeanSquaredProportionalError</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">MLJBase.RootMeanSquaredProportionalError</code></pre><p>A measure type for root mean squared proportional error, which includes the instance(s), <code>rmsp</code>.</p><pre><code class="language-none">RootMeanSquaredProportionalError()(ŷ, y)</code></pre><p>Evaluate the default instance of RootMeanSquaredProportionalError on observations <code>ŷ</code>, given ground truth values <code>y</code>. </p><p>Constructor keyword arguments: <code>tol</code> (default = <code>eps()</code>).</p><p><span>$\text{root mean squared proportional error} = m^{-1}∑ᵢ \left({yᵢ-ŷᵢ \over yᵢ}\right)^2$</span></p><p>where the sum is over indices such that <code>abs(yᵢ) &gt; tol</code> and <code>m</code> is the number of such indices.</p><p>Requires <code>scitype(y)</code> to be a subtype of Union{AbstractArray{ScientificTypes.Continuous,1}, AbstractArray{ScientificTypes.Count,1}}; <code>ŷ</code> must be a deterministic prediction. </p><p>For more information, run <code>info(RootMeanSquaredProportionalError)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/measures/meta_utilities.jl#LL76-L97">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.TrueNegative" href="#MLJBase.TrueNegative"><code>MLJBase.TrueNegative</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">MLJBase.TrueNegative</code></pre><p>A measure type for number of true negatives, which includes the instance(s), <code>true_negative</code>, <code>truenegative</code>.</p><pre><code class="language-none">TrueNegative()(ŷ, y)</code></pre><p>Evaluate the default instance of TrueNegative on observations <code>ŷ</code>, given ground truth values <code>y</code>. </p><p>Assigns <code>false</code> to first element of <code>levels(y)</code>. To reverse roles, use <code>TrueNegative(rev=true)</code>.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:OrderedFactor{2}}</code> (binary classification where choice of &quot;true&quot; effects the measure); <code>ŷ</code> must be a deterministic prediction. </p><p>For more information, run <code>info(TrueNegative)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/measures/meta_utilities.jl#LL76-L91">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.TrueNegativeRate" href="#MLJBase.TrueNegativeRate"><code>MLJBase.TrueNegativeRate</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">MLJBase.TrueNegativeRate</code></pre><p>A measure type for true negative rate, which includes the instance(s), <code>true_negative_rate</code>, <code>truenegative_rate</code>, <code>tnr</code>, <code>specificity</code>, <code>selectivity</code>.</p><pre><code class="language-none">TrueNegativeRate()(ŷ, y)</code></pre><p>Evaluate the default instance of TrueNegativeRate on observations <code>ŷ</code>, given ground truth values <code>y</code>. </p><p>Assigns <code>false</code> to first element of <code>levels(y)</code>. To reverse roles, use <code>TrueNegativeRate(rev=true)</code>.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:OrderedFactor{2}}</code> (binary classification where choice of &quot;true&quot; effects the measure); <code>ŷ</code> must be a deterministic prediction. </p><p>For more information, run <code>info(TrueNegativeRate)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/measures/meta_utilities.jl#LL76-L91">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.TruePositive" href="#MLJBase.TruePositive"><code>MLJBase.TruePositive</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">MLJBase.TruePositive</code></pre><p>A measure type for number of true positives, which includes the instance(s), <code>true_positive</code>, <code>truepositive</code>.</p><pre><code class="language-none">TruePositive()(ŷ, y)</code></pre><p>Evaluate the default instance of TruePositive on observations <code>ŷ</code>, given ground truth values <code>y</code>. </p><p>Assigns <code>false</code> to first element of <code>levels(y)</code>. To reverse roles, use <code>TruePositive(rev=true)</code>.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:OrderedFactor{2}}</code> (binary classification where choice of &quot;true&quot; effects the measure); <code>ŷ</code> must be a deterministic prediction. </p><p>For more information, run <code>info(TruePositive)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/measures/meta_utilities.jl#LL76-L91">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.TruePositiveRate" href="#MLJBase.TruePositiveRate"><code>MLJBase.TruePositiveRate</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">MLJBase.TruePositiveRate</code></pre><p>A measure type for true positive rate (a.k.a recall), which includes the instance(s), <code>true_positive_rate</code>, <code>truepositive_rate</code>, <code>tpr</code>, <code>sensitivity</code>, <code>recall</code>, <code>hit_rate</code>.</p><pre><code class="language-none">TruePositiveRate()(ŷ, y)</code></pre><p>Evaluate the default instance of TruePositiveRate on observations <code>ŷ</code>, given ground truth values <code>y</code>. </p><p>Assigns <code>false</code> to first element of <code>levels(y)</code>. To reverse roles, use <code>TruePositiveRate(rev=true)</code>.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:OrderedFactor{2}}</code> (binary classification where choice of &quot;true&quot; effects the measure); <code>ŷ</code> must be a deterministic prediction. </p><p>For more information, run <code>info(TruePositiveRate)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/measures/meta_utilities.jl#LL76-L91">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.flat_values-Tuple{NamedTuple}" href="#MLJBase.flat_values-Tuple{NamedTuple}"><code>MLJBase.flat_values</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">flat_values(t::NamedTuple)</code></pre><p>View a nested named tuple <code>t</code> as a tree and return, as a tuple, the values at the leaves, in the order they appear in the original tuple.</p><pre><code class="language-julia-repl">julia&gt; t = (X = (x = 1, y = 2), Y = 3)
julia&gt; flat_values(t)
(1, 2, 3)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/utilities.jl#LL81-L93">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.metadata_measure-Tuple{Any}" href="#MLJBase.metadata_measure-Tuple{Any}"><code>MLJBase.metadata_measure</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">metadata_measure(T; kw...)</code></pre><p>Helper function to write the metadata for a single measure.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/measures/meta_utilities.jl#LL83-L87">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.recursive_getproperty-Tuple{Any,Symbol}" href="#MLJBase.recursive_getproperty-Tuple{Any,Symbol}"><code>MLJBase.recursive_getproperty</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">recursive_getproperty(object, nested_name::Expr)</code></pre><p>Call getproperty recursively on <code>object</code> to extract the value of some nested property, as in the following example:</p><pre><code class="language-none">julia&gt; object = (X = (x = 1, y = 2), Y = 3)
julia&gt; recursive_getproperty(object, :(X.y))
2</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/utilities.jl#LL120-L130">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.recursive_setproperty!-Tuple{Any,Symbol,Any}" href="#MLJBase.recursive_setproperty!-Tuple{Any,Symbol,Any}"><code>MLJBase.recursive_setproperty!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">recursively_setproperty!(object, nested_name::Expr, value)</code></pre><p>Set a nested property of an <code>object</code> to <code>value</code>, as in the following example:</p><pre><code class="language-none">julia&gt; mutable struct Foo
           X
           Y
       end

julia&gt; mutable struct Bar
           x
           y
       end

julia&gt; object = Foo(Bar(1, 2), 3)
Foo(Bar(1, 2), 3)

julia&gt; recursively_setproperty!(object, :(X.y), 42)
42

julia&gt; object
Foo(Bar(1, 42), 3)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/utilities.jl#LL137-L163">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.unwind-Tuple" href="#MLJBase.unwind-Tuple"><code>MLJBase.unwind</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">unwind(iterators...)</code></pre><p>Represent all possible combinations of values generated by <code>iterators</code> as rows of a matrix <code>A</code>. In more detail, <code>A</code> has one column for each iterator in <code>iterators</code> and one row for each distinct possible combination of values taken on by the iterators. Elements in the first column cycle fastest, those in the last clolumn slowest.</p><p><strong>Example</strong></p><pre><code class="language-julia">julia&gt; iterators = ([1, 2], [&quot;a&quot;,&quot;b&quot;], [&quot;x&quot;, &quot;y&quot;, &quot;z&quot;]);
julia&gt; MLJTuning.unwind(iterators...)
12×3 Array{Any,2}:
 1  &quot;a&quot;  &quot;x&quot;
 2  &quot;a&quot;  &quot;x&quot;
 1  &quot;b&quot;  &quot;x&quot;
 2  &quot;b&quot;  &quot;x&quot;
 1  &quot;a&quot;  &quot;y&quot;
 2  &quot;a&quot;  &quot;y&quot;
 1  &quot;b&quot;  &quot;y&quot;
 2  &quot;b&quot;  &quot;y&quot;
 1  &quot;a&quot;  &quot;z&quot;
 2  &quot;a&quot;  &quot;z&quot;
 1  &quot;b&quot;  &quot;z&quot;
 2  &quot;b&quot;  &quot;z&quot;</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/utilities.jl#LL248-L277">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase._permute_rows-Tuple{Union{AbstractArray{T,1}, AbstractArray{T,2}} where T,Array{Int64,1}}" href="#MLJBase._permute_rows-Tuple{Union{AbstractArray{T,1}, AbstractArray{T,2}} where T,Array{Int64,1}}"><code>MLJBase._permute_rows</code></a> — <span class="docstring-category">Method</span></header><section><div><p><em>permute</em>rows(obj, perm)</p><p>Internal function to return a vector or matrix with permuted rows given the permutation <code>perm</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/utilities.jl#LL184-L189">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.available_name-Tuple{Any,Any}" href="#MLJBase.available_name-Tuple{Any,Any}"><code>MLJBase.available_name</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">available_name(modl::Module, name::Symbol)</code></pre><p>Function to replace, if necessary, a given <code>name</code> with a modified one that ensures it is not the name of any existing object in the global scope of <code>modl</code>. Modifications are created with numerical suffixes.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/utilities.jl#LL343-L350">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.check_dimensions-Tuple{Union{AbstractArray{T,1}, AbstractArray{T,2}} where T,Union{AbstractArray{T,1}, AbstractArray{T,2}} where T}" href="#MLJBase.check_dimensions-Tuple{Union{AbstractArray{T,1}, AbstractArray{T,2}} where T,Union{AbstractArray{T,1}, AbstractArray{T,2}} where T}"><code>MLJBase.check_dimensions</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">check_dimension(X, Y)</code></pre><p>Check that two vectors or matrices have matching dimensions</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/utilities.jl#LL172-L176">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.chunks-Tuple{AbstractRange,Integer}" href="#MLJBase.chunks-Tuple{AbstractRange,Integer}"><code>MLJBase.chunks</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">chunks(range, n)</code></pre><p>Split an <code>AbstractRange</code>  into <code>n</code> subranges of approximately equal length.</p><p><strong>Example</strong></p><pre><code class="language-julia">julia&gt; collect(chunks(1:5, 2))
2-element Array{UnitRange{Int64},1}:
 1:3
 4:5

**Private method**
</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/utilities.jl#LL300-L315">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.shuffle_rows-Tuple{Union{AbstractArray{T,1}, AbstractArray{T,2}} where T,Union{AbstractArray{T,1}, AbstractArray{T,2}} where T}" href="#MLJBase.shuffle_rows-Tuple{Union{AbstractArray{T,1}, AbstractArray{T,2}} where T,Union{AbstractArray{T,1}, AbstractArray{T,2}} where T}"><code>MLJBase.shuffle_rows</code></a> — <span class="docstring-category">Method</span></header><section><div><p>shuffle_rows(X, Y, ...; rng=)</p><p>Return a shuffled view of a vector or  matrix <code>X</code> (or set of such) using a random permutation (which can be seeded specifying <code>rng</code>).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/alan-turing-institute/MLJBase.jl/blob/8d9615e4ca3bff6ce5beca44ee598db32a2facb6/src/utilities.jl#LL196-L201">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../openml/">« OpenML</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Monday 8 March 2021 18:45">Monday 8 March 2021</span>. Using Julia version 1.5.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
