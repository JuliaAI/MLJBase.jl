<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Utilities · MLJBase.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">MLJBase.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../measures/">Measures</a></li><li><a class="tocitem" href="../resampling/">Resampling</a></li><li><a class="tocitem" href="../composition/">Composition</a></li><li><a class="tocitem" href="../datasets/">Datasets</a></li><li><a class="tocitem" href="../distributions/">Distributions</a></li><li class="is-active"><a class="tocitem" href>Utilities</a><ul class="internal"><li><a class="tocitem" href="#Machines"><span>Machines</span></a></li><li><a class="tocitem" href="#Parameter-Inspection"><span>Parameter Inspection</span></a></li><li><a class="tocitem" href="#Show"><span>Show</span></a></li><li><a class="tocitem" href="#Utility-functions"><span>Utility functions</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Utilities</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Utilities</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaAI/MLJBase.jl/blob/dev/docs/src/utilities.md#L" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Utilities"><a class="docs-heading-anchor" href="#Utilities">Utilities</a><a id="Utilities-1"></a><a class="docs-heading-anchor-permalink" href="#Utilities" title="Permalink"></a></h1><h2 id="Machines"><a class="docs-heading-anchor" href="#Machines">Machines</a><a id="Machines-1"></a><a class="docs-heading-anchor-permalink" href="#Machines" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Base.replace-Union{Tuple{C}, Tuple{Machine{&lt;:Any, C}, Vararg{Pair}}} where C" href="#Base.replace-Union{Tuple{C}, Tuple{Machine{&lt;:Any, C}, Vararg{Pair}}} where C"><code>Base.replace</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">replace(mach::Machine, field1 =&gt; value1, field2 =&gt; value2, ...)</code></pre><p><strong>Private method.</strong></p><p>Return a shallow copy of the machine <code>mach</code> with the specified field replacements. Undefined field values are preserved. Unspecified fields have identically equal values, with the exception of <code>mach.fit_okay</code>, which is always a new instance <code>Channel{Bool}(1)</code>.</p><p>The following example returns a machine with no traces of training data (but also removes any upstream dependencies in a learning network):</p><p>```julia replace(mach, :args =&gt; (), :data =&gt; (), :data<em>resampled</em>data =&gt; (), :cache =&gt; nothing)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/machines.jl#LL101-L117">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.age-Tuple{Machine}" href="#MLJBase.age-Tuple{Machine}"><code>MLJBase.age</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">age(mach::Machine)</code></pre><p>Return an integer representing the number of times <code>mach</code> has been trained or updated. For more detail, see the discussion of training logic at <a href="#MLJBase.fit_only!-Union{Tuple{Machine{&lt;:Any, cache_data}}, Tuple{cache_data}} where cache_data"><code>fit_only!</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/machines.jl#LL92-L98">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.ancestors-Tuple{Machine}" href="#MLJBase.ancestors-Tuple{Machine}"><code>MLJBase.ancestors</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">ancestors(mach::Machine; self=false)</code></pre><p>All ancestors of <code>mach</code>, including <code>mach</code> if <code>self=true</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/machines.jl#LL144-L149">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.default_scitype_check_level" href="#MLJBase.default_scitype_check_level"><code>MLJBase.default_scitype_check_level</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">default_scitype_check_level()</code></pre><p>Return the current global default value for scientific type checking when constructing machines.</p><pre><code class="nohighlight hljs">default_scitype_check_level(i::Integer)</code></pre><p>Set the global default value for scientific type checking to <code>i</code>.</p><p>The effect of the <code>scitype_check_level</code> option in calls of the form <code>machine(model, data, scitype_check_level=...)</code> is summarized below:</p><table><tr><th style="text-align: center"><code>scitype_check_level</code></th><th style="text-align: center">Inspect scitypes?</th><th style="text-align: center">If <code>Unknown</code> in scitypes</th><th style="text-align: center">If other scitype mismatch</th></tr><tr><td style="text-align: center">0</td><td style="text-align: center">×</td><td style="text-align: center"></td><td style="text-align: center"></td></tr><tr><td style="text-align: center">1 (value at startup)</td><td style="text-align: center">✓</td><td style="text-align: center"></td><td style="text-align: center">warning</td></tr><tr><td style="text-align: center">2</td><td style="text-align: center">✓</td><td style="text-align: center">warning</td><td style="text-align: center">warning</td></tr><tr><td style="text-align: center">3</td><td style="text-align: center">✓</td><td style="text-align: center">warning</td><td style="text-align: center">error</td></tr><tr><td style="text-align: center">4</td><td style="text-align: center">✓</td><td style="text-align: center">error</td><td style="text-align: center">error</td></tr></table><p>See also <a href="#MLJBase.machine"><code>machine</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/machines.jl#LL3-L26">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.fit_only!-Union{Tuple{Machine{&lt;:Any, cache_data}}, Tuple{cache_data}} where cache_data" href="#MLJBase.fit_only!-Union{Tuple{Machine{&lt;:Any, cache_data}}, Tuple{cache_data}} where cache_data"><code>MLJBase.fit_only!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">MLJBase.fit_only!(
    mach::Machine;
    rows=nothing,
    verbosity=1,
    force=false,
    composite=nothing
)</code></pre><p>Without mutating any other machine on which it may depend, perform one of the following actions to the machine <code>mach</code>, using the data and model bound to it, and restricting the data to <code>rows</code> if specified:</p><ul><li><p><em>Ab initio training.</em> Ignoring any previous learned parameters and cache, compute and store new learned parameters. Increment <code>mach.state</code>.</p></li><li><p><em>Training update.</em> Making use of previous learned parameters and/or  cache, replace or mutate existing learned parameters. The effect is  the same (or nearly the same) as in ab initio training, but may be  faster or use less memory, assuming the model supports an update  option (implements <code>MLJBase.update</code>). Increment <code>mach.state</code>.</p></li><li><p><em>No-operation.</em> Leave existing learned parameters untouched. Do not  increment <code>mach.state</code>.</p></li></ul><p>If the model, <code>model</code>, bound to <code>mach</code> is a symbol, then instead perform the action using the true model <code>getproperty(composite, model)</code>.</p><p><strong>Training action logic</strong></p><p>For the action to be a no-operation, either <code>mach.frozen == true</code> or or none of the following apply:</p><ul><li><p>(i) <code>mach</code> has never been trained (<code>mach.state == 0</code>).</p></li><li><p>(ii) <code>force == true</code>.</p></li><li><p>(iii) The <code>state</code> of some other machine on which <code>mach</code> depends has changed since the last time <code>mach</code> was trained (ie, the last time <code>mach.state</code> was last incremented).</p></li><li><p>(iv) The specified <code>rows</code> have changed since the last retraining and <code>mach.model</code> does not have <code>Static</code> type.</p></li><li><p>(v) <code>mach.model</code> is a model and different from the last model used for training, but has the same type.</p></li><li><p>(vi) <code>mach.model</code> is a model but has a type different from the last model used for training.</p></li><li><p>(vii) <code>mach.model</code> is a symbol and <code>(composite, mach.model)</code> is different from the last model used for training, but has the same type.</p></li><li><p>(viii) <code>mach.model</code> is a symbol and <code>(composite, mach.model)</code> has a different type from the last model used for training.</p></li></ul><p>In any of the cases (i) - (iv), (vi), or (viii), <code>mach</code> is trained ab initio. If (v) or (vii) is true, then a training update is applied.</p><p>To freeze or unfreeze <code>mach</code>, use <code>freeze!(mach)</code> or <code>thaw!(mach)</code>.</p><p><strong>Implementation details</strong></p><p>The data to which a machine is bound is stored in <code>mach.args</code>. Each element of <code>args</code> is either a <code>Node</code> object, or, in the case that concrete data was bound to the machine, it is concrete data wrapped in a <code>Source</code> node. In all cases, to obtain concrete data for actual training, each argument <code>N</code> is called, as in <code>N()</code> or <code>N(rows=rows)</code>, and either <code>MLJBase.fit</code> (ab initio training) or <code>MLJBase.update</code> (training update) is dispatched on <code>mach.model</code> and this data. See the &quot;Adding models for general use&quot; section of the MLJ documentation for more on these lower-level training methods.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/machines.jl#LL530-L605">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.freeze!-Tuple{Machine}" href="#MLJBase.freeze!-Tuple{Machine}"><code>MLJBase.freeze!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">freeze!(mach)</code></pre><p>Freeze the machine <code>mach</code> so that it will never be retrained (unless thawed).</p><p>See also <a href="#MLJBase.thaw!-Tuple{Machine}"><code>thaw!</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/machines.jl#LL408-L415">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.glb-Tuple{Machine{&lt;:Union{Composite, Surrogate}}}" href="#MLJBase.glb-Tuple{Machine{&lt;:Union{Composite, Surrogate}}}"><code>MLJBase.glb</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">N = glb(mach::Machine{&lt;:Union{Composite,Surrogate}})</code></pre><p>A greatest lower bound for the nodes appearing in the learning network interface of <code>mach</code>.</p><p>A <em>learning network interface</em> is a named tuple declaring certain interface points in  a learning network, to be used when &quot;exporting&quot; the network as a new stand-alone model  type. Examples are</p><pre><code class="nohighlight hljs"> (predict=yhat,)
 (transform=Xsmall, acceleration=CPUThreads())
 (predict=yhat, transform=W, report=(loss=loss_node,))</code></pre><p>Here <code>yhat</code>, <code>Xsmall</code>, <code>W</code> and <code>loss_node</code> are nodes in the network.</p><p>The keys of the learning network interface always one of the following:</p><ul><li><p>The name of an operation, such as <code>:predict</code>, <code>:predict_mode</code>, <code>:transform</code>, <code>:inverse_transform</code>. See &quot;Operation keys&quot; below.</p></li><li><p><code>:report</code>, for exposing results of calling a node <em>with no arguments</em> in the composite model report. See &quot;Including report nodes&quot; below.</p></li><li><p><code>:fitted_params</code>, for exposing results of calling a node <em>with no arguments</em> as fitted parameters of the composite model. See &quot;Including fitted parameter nodes&quot; below.</p></li><li><p><code>:acceleration</code>, for articulating acceleration mode for training the network, e.g., <code>CPUThreads()</code>. Corresponding value must be an <code>AbstractResource</code>. If not included, <code>CPU1()</code> is used.</p></li></ul><p><strong>Operation keys</strong></p><p>If the key is an operation, then the value must be a node <code>n</code> in the network with a  unique origin (<code>length(origins(n)) === 1</code>). The intention of a declaration such as  <code>predict=yhat</code> is that the exported model type implements <code>predict</code>, which, when  applied to new data <code>Xnew</code>, should return <code>yhat(Xnew)</code>.</p><p><strong>Including report nodes</strong></p><p>If the key is <code>:report</code>, then the corresponding value must be a named tuple</p><pre><code class="nohighlight hljs"> (k1=n1, k2=n2, ...)</code></pre><p>whose values are all nodes. For each <code>k=n</code> pair, the key <code>k</code> will appear as a key in  the composite model report, with a corresponding value of <code>deepcopy(n())</code>, called  immediatately after training or updating the network.  For examples, refer to the  &quot;Learning Networks&quot; section of the MLJ manual.</p><p><strong>Including fitted parameter nodes</strong></p><p>If the key is <code>:fitted_params</code>, then the behaviour is as for report nodes but results  are exposed as fitted parameters of the composite model instead of the report.</p><p><strong>Private method.</strong></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/composition/learning_networks/deprecated_machines.jl#LL191-L201">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.last_model-Tuple{Any}" href="#MLJBase.last_model-Tuple{Any}"><code>MLJBase.last_model</code></a> — <span class="docstring-category">Method</span></header><section><div><p>last_model(mach::Machine)</p><p>Return the last model used to train the machine <code>mach</code>. This is a bona fide model, even if <code>mach.model</code> is a symbol.</p><p>Returns <code>nothing</code> if <code>mach</code> has not been trained.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/machines.jl#LL520-L527">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.machine" href="#MLJBase.machine"><code>MLJBase.machine</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">machine(model, args...; cache=true, scitype_check_level=1)</code></pre><p>Construct a <code>Machine</code> object binding a <code>model</code>, storing hyper-parameters of some machine learning algorithm, to some data, <code>args</code>. Calling <a href="#StatsAPI.fit!-Tuple{Machine{&lt;:Surrogate}}"><code>fit!</code></a> on a <code>Machine</code> instance <code>mach</code> stores outcomes of applying the algorithm in <code>mach</code>, which can be inspected using <code>fitted_params(mach)</code> (learned paramters) and <code>report(mach)</code> (other outcomes). This in turn enables generalization to new data using operations such as <code>predict</code> or <code>transform</code>:</p><pre><code class="language-julia hljs">using MLJModels
X, y = make_regression()

PCA = @load PCA pkg=MultivariateStats
model = PCA()
mach = machine(model, X)
fit!(mach, rows=1:50)
transform(mach, selectrows(X, 51:100)) # or transform(mach, rows=51:100)

DecisionTreeRegressor = @load DecisionTreeRegressor pkg=DecisionTree
model = DecisionTreeRegressor()
mach = machine(model, X, y)
fit!(mach, rows=1:50)
predict(mach, selectrows(X, 51:100)) # or predict(mach, rows=51:100)</code></pre><p>Specify <code>cache=false</code> to prioritize memory management over speed.</p><p>When building a learning network, <code>Node</code> objects can be substituted for the concrete data but no type or dimension checks are applied.</p><p><strong>Checks on the types of training data</strong></p><p>A model articulates its data requirements using <a href="https://juliaai.github.io/ScientificTypes.jl/dev/">scientific types</a>, i.e., using the <a href="@ref"><code>scitype</code></a> function instead of the <code>typeof</code> function.</p><p>If <code>scitype_check_level &gt; 0</code> then the scitype of each <code>arg</code> in <code>args</code> is computed, and this is compared with the scitypes expected by the model, unless <code>args</code> contains <code>Unknown</code> scitypes and <code>scitype_check_level &lt; 4</code>, in which case no further action is taken. Whether warnings are issued or errors thrown depends the level. For details, see <a href="#MLJBase.default_scitype_check_level"><code>default_scitype_check_level</code></a>, a method to inspect or change the default level (<code>1</code> at startup).</p><p><strong>Machines with model placeholders</strong></p><p>A symbol can be substituted for a model in machine constructors to act as a placeholder for a model specified at training time. The symbol must be the field name for a struct whose corresponding value is a model, as shown in the following example:</p><pre><code class="language-julia hljs">mutable struct MyComposite
    transformer
    classifier
end

my_composite = MyComposite(Standardizer(), ConstantClassifier)

X, y = make_blobs()
mach = machine(:classifier, X, y)
fit!(mach, composite=my_composite)</code></pre><p>The last two lines are equivalent to</p><pre><code class="language-julia hljs">mach = machine(ConstantClassifier(), X, y)
fit!(mach)</code></pre><p>Delaying model specification is used when exporting learning networks as new stand-alone model types. See <a href="@ref"><code>prefit</code></a> and the MLJ documentation on learning networks.</p><p>See also <a href="#StatsAPI.fit!-Tuple{Machine{&lt;:Surrogate}}"><code>fit!</code></a>, <a href="#MLJBase.default_scitype_check_level"><code>default_scitype_check_level</code></a>, <a href="#MLJModelInterface.save-Tuple{Union{IO, String}, Machine}"><code>MLJBase.save</code></a>, <a href="#MLJBase.serializable-Union{Tuple{Machine{&lt;:Any, C}}, Tuple{C}} where C"><code>serializable</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/machines.jl#LL247-L326">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.machine-Tuple{Union{IO, String}}" href="#MLJBase.machine-Tuple{Union{IO, String}}"><code>MLJBase.machine</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">machine(file::Union{String, IO})</code></pre><p>Rebuild from a file a machine that has been serialized using the default Serialization module.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/machines.jl#LL390-L395">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.model_supertype-Tuple{Any}" href="#MLJBase.model_supertype-Tuple{Any}"><code>MLJBase.model_supertype</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">model_supertype(interface)</code></pre><p>Return, if this can be inferred, which of <code>Deterministic</code>, <code>Probabilistic</code> and <code>Unsupervised</code> is the appropriate supertype for a composite model obtained by exporting a learning network with the specified learning network interface.</p><p>A <em>learning network interface</em> is a named tuple declaring certain interface points in  a learning network, to be used when &quot;exporting&quot; the network as a new stand-alone model  type. Examples are</p><pre><code class="nohighlight hljs"> (predict=yhat,)
 (transform=Xsmall, acceleration=CPUThreads())
 (predict=yhat, transform=W, report=(loss=loss_node,))</code></pre><p>Here <code>yhat</code>, <code>Xsmall</code>, <code>W</code> and <code>loss_node</code> are nodes in the network.</p><p>The keys of the learning network interface always one of the following:</p><ul><li><p>The name of an operation, such as <code>:predict</code>, <code>:predict_mode</code>, <code>:transform</code>, <code>:inverse_transform</code>. See &quot;Operation keys&quot; below.</p></li><li><p><code>:report</code>, for exposing results of calling a node <em>with no arguments</em> in the composite model report. See &quot;Including report nodes&quot; below.</p></li><li><p><code>:fitted_params</code>, for exposing results of calling a node <em>with no arguments</em> as fitted parameters of the composite model. See &quot;Including fitted parameter nodes&quot; below.</p></li><li><p><code>:acceleration</code>, for articulating acceleration mode for training the network, e.g., <code>CPUThreads()</code>. Corresponding value must be an <code>AbstractResource</code>. If not included, <code>CPU1()</code> is used.</p></li></ul><p><strong>Operation keys</strong></p><p>If the key is an operation, then the value must be a node <code>n</code> in the network with a  unique origin (<code>length(origins(n)) === 1</code>). The intention of a declaration such as  <code>predict=yhat</code> is that the exported model type implements <code>predict</code>, which, when  applied to new data <code>Xnew</code>, should return <code>yhat(Xnew)</code>.</p><p><strong>Including report nodes</strong></p><p>If the key is <code>:report</code>, then the corresponding value must be a named tuple</p><pre><code class="nohighlight hljs"> (k1=n1, k2=n2, ...)</code></pre><p>whose values are all nodes. For each <code>k=n</code> pair, the key <code>k</code> will appear as a key in  the composite model report, with a corresponding value of <code>deepcopy(n())</code>, called  immediatately after training or updating the network.  For examples, refer to the  &quot;Learning Networks&quot; section of the MLJ manual.</p><p><strong>Including fitted parameter nodes</strong></p><p>If the key is <code>:fitted_params</code>, then the behaviour is as for report nodes but results  are exposed as fitted parameters of the composite model instead of the report.</p><p>If a supertype cannot be inferred, <code>nothing</code> is returned.</p><p>If the network with given <code>signature</code> is not exportable, this method will not error but it will not a give meaningful return value either.</p><p><strong>Private method.</strong></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/composition/learning_networks/deprecated_machines.jl#LL26-L43">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.report-Tuple{MLJBase.CompositeFitresult}" href="#MLJBase.report-Tuple{MLJBase.CompositeFitresult}"><code>MLJBase.report</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">report(fitresult::CompositeFitresult)</code></pre><p>Return a tuple combining the report from <code>fitresult.glb</code> (a <code>Node</code> report) with the additions coming from nodes declared as report nodes in <code>fitresult.signature</code>, but without merging the two.</p><p>A <em>learning network interface</em> is a named tuple declaring certain interface points in  a learning network, to be used when &quot;exporting&quot; the network as a new stand-alone model  type. Examples are</p><pre><code class="nohighlight hljs"> (predict=yhat,)
 (transform=Xsmall, acceleration=CPUThreads())
 (predict=yhat, transform=W, report=(loss=loss_node,))</code></pre><p>Here <code>yhat</code>, <code>Xsmall</code>, <code>W</code> and <code>loss_node</code> are nodes in the network.</p><p>The keys of the learning network interface always one of the following:</p><ul><li><p>The name of an operation, such as <code>:predict</code>, <code>:predict_mode</code>, <code>:transform</code>, <code>:inverse_transform</code>. See &quot;Operation keys&quot; below.</p></li><li><p><code>:report</code>, for exposing results of calling a node <em>with no arguments</em> in the composite model report. See &quot;Including report nodes&quot; below.</p></li><li><p><code>:fitted_params</code>, for exposing results of calling a node <em>with no arguments</em> as fitted parameters of the composite model. See &quot;Including fitted parameter nodes&quot; below.</p></li><li><p><code>:acceleration</code>, for articulating acceleration mode for training the network, e.g., <code>CPUThreads()</code>. Corresponding value must be an <code>AbstractResource</code>. If not included, <code>CPU1()</code> is used.</p></li></ul><p><strong>Operation keys</strong></p><p>If the key is an operation, then the value must be a node <code>n</code> in the network with a  unique origin (<code>length(origins(n)) === 1</code>). The intention of a declaration such as  <code>predict=yhat</code> is that the exported model type implements <code>predict</code>, which, when  applied to new data <code>Xnew</code>, should return <code>yhat(Xnew)</code>.</p><p><strong>Including report nodes</strong></p><p>If the key is <code>:report</code>, then the corresponding value must be a named tuple</p><pre><code class="nohighlight hljs"> (k1=n1, k2=n2, ...)</code></pre><p>whose values are all nodes. For each <code>k=n</code> pair, the key <code>k</code> will appear as a key in  the composite model report, with a corresponding value of <code>deepcopy(n())</code>, called  immediatately after training or updating the network.  For examples, refer to the  &quot;Learning Networks&quot; section of the MLJ manual.</p><p><strong>Including fitted parameter nodes</strong></p><p>If the key is <code>:fitted_params</code>, then the behaviour is as for report nodes but results  are exposed as fitted parameters of the composite model instead of the report.</p><p><strong>Private method</strong></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/composition/learning_networks/deprecated_machines.jl#LL204-L214">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.report-Tuple{Machine}" href="#MLJBase.report-Tuple{Machine}"><code>MLJBase.report</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">report(mach)</code></pre><p>Return the report for a machine <code>mach</code> that has been <code>fit!</code>, for example the coefficients in a linear model.</p><p>This is a named tuple and human-readable if possible.</p><p>If <code>mach</code> is a machine for a composite model, such as a model constructed using <code>@pipeline</code>, then the returned named tuple has the composite type&#39;s field names as keys. The corresponding value is the report for the machine in the underlying learning network bound to that model. (If multiple machines share the same model, then the value is a vector.)</p><pre><code class="language-julia hljs">using MLJ
@load LinearBinaryClassifier pkg=GLM
X, y = @load_crabs;
pipe = @pipeline Standardizer LinearBinaryClassifier
mach = machine(pipe, X, y) |&gt; fit!

julia&gt; report(mach).linear_binary_classifier
(deviance = 3.8893386087844543e-7,
 dof_residual = 195.0,
 stderror = [18954.83496713119, 6502.845740757159, 48484.240246060406, 34971.131004997274, 20654.82322484894, 2111.1294584763386],
 vcov = [3.592857686311793e8 9.122732393971942e6 … -8.454645589364915e7 5.38856837634321e6; 9.122732393971942e6 4.228700272808351e7 … -4.978433790526467e7 -8.442545425533723e6; … ; -8.454645589364915e7 -4.978433790526467e7 … 4.2662172244975924e8 2.1799125705781363e7; 5.38856837634321e6 -8.442545425533723e6 … 2.1799125705781363e7 4.456867590446599e6],)
</code></pre><p>Additional keys, <code>machines</code> and <code>report_given_machine</code>, give a list of <em>all</em> machines in the underlying network, and a dictionary of reports keyed on those machines.</p><p>See also <a href="#MLJModelInterface.fitted_params-Tuple{Machine}"><code>fitted_params</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/machines.jl#LL827-L863">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.report_given_method-Tuple{Machine}" href="#MLJBase.report_given_method-Tuple{Machine}"><code>MLJBase.report_given_method</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">report_given_method(mach::Machine)</code></pre><p>Same as <code>report(mach)</code> but broken down by the method (<code>fit</code>, <code>predict</code>, etc) that contributed the report.</p><p>A specialized method intended for learning network applications.</p><p>The return value is a dictionary keyed on the symbol representing the method (<code>:fit</code>, <code>:predict</code>, etc) and the values report contributed by that method.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/machines.jl#LL872-L884">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.restore!-Tuple{Machine}" href="#MLJBase.restore!-Tuple{Machine}"><code>MLJBase.restore!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">restore!(mach::Machine)</code></pre><p>Restore the state of a machine that is currently serializable but which may not be otherwise usable. For such a machine, <code>mach</code>, one has <code>mach.state=1</code>. Intended for restoring deserialized machine objects to a useable form.</p><p>For an example see <a href="#MLJBase.serializable-Union{Tuple{Machine{&lt;:Any, C}}, Tuple{C}} where C"><code>serializable</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/machines.jl#LL1011-L1021">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.return!-Tuple{Machine{&lt;:Surrogate}, Union{Nothing, Model}, Any}" href="#MLJBase.return!-Tuple{Machine{&lt;:Surrogate}, Union{Nothing, Model}, Any}"><code>MLJBase.return!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">return!(mach::Machine{&lt;:Surrogate}, model, verbosity; acceleration=CPU1())</code></pre><p>The last call in custom code defining the <code>MLJBase.fit</code> method for a new composite model type. Here <code>model</code> is the instance of the new type appearing in the <code>MLJBase.fit</code> signature, while <code>mach</code> is a learning network machine constructed using <code>model</code>. Not relevant when defining composite models using <code>@pipeline</code> (deprecated) or <code>@from_network</code>.</p><p>For usage, see the example given below. Specifically, the call does the following:</p><ul><li><p>Determines which hyper-parameters of <code>model</code> point to model instances in the learning network wrapped by <code>mach</code>, for recording in an object called <code>cache</code>, for passing onto the MLJ logic that handles smart updating (namely, an <code>MLJBase.update</code> fallback for composite models).</p></li><li><p>Calls <code>fit!(mach, verbosity=verbosity, acceleration=acceleration)</code>.</p></li><li><p>Records (among other things) a copy of <code>model</code> in a variable called <code>cache</code></p></li><li><p>Returns <code>cache</code> and outcomes of training in an appropriate form (specifically, <code>(mach.fitresult, cache, mach.report)</code>; see <a href="https://alan-turing-institute.github.io/MLJ.jl/dev/adding_models_for_general_use/">Adding Models for General Use</a> for technical details.)</p></li></ul><p><strong>Example</strong></p><p>The following code defines, &quot;by hand&quot;, a new model type <code>MyComposite</code> for composing standardization (whitening) with a deterministic regressor:</p><pre><code class="nohighlight hljs">mutable struct MyComposite &lt;: DeterministicComposite
    regressor
end

function MLJBase.fit(model::MyComposite, verbosity, X, y)
    Xs = source(X)
    ys = source(y)

    mach1 = machine(Standardizer(), Xs)
    Xwhite = transform(mach1, Xs)

    mach2 = machine(model.regressor, Xwhite, ys)
    yhat = predict(mach2, Xwhite)

    mach = machine(Deterministic(), Xs, ys; predict=yhat)
    return!(mach, model, verbosity)
end</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/composition/learning_networks/deprecated_machines.jl#LL326-L382">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.serializable-Union{Tuple{Machine{&lt;:Any, C}}, Tuple{C}} where C" href="#MLJBase.serializable-Union{Tuple{Machine{&lt;:Any, C}}, Tuple{C}} where C"><code>MLJBase.serializable</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">serializable(mach::Machine)</code></pre><p>Returns a shallow copy of the machine to make it serializable. In particular, all training data is removed and, if necessary, learned parameters are replaced with persistent representations.</p><p>Any general purpose Julia serializer may be applied to the output of <code>serializable</code> (eg, JLSO, BSON, JLD) but you must call <code>restore!(mach)</code> on the deserialised object <code>mach</code> before using it. See the example below.</p><p>If using Julia&#39;s standard Serialization library, a shorter workflow is available using the <a href="#MLJModelInterface.save-Tuple{Union{IO, String}, Machine}"><code>MLJBase.save</code></a> (or <code>MLJ.save</code>) method.</p><p>A machine returned by <code>serializable</code> is characterized by the property <code>mach.state == -1</code>.</p><p><strong>Example using <a href="https://invenia.github.io/JLSO.jl/stable/">JLSO</a></strong></p><pre><code class="nohighlight hljs">using MLJ
using JLSO
Tree = @load DecisionTreeClassifier
tree = Tree()
X, y = @load_iris
mach = fit!(machine(tree, X, y))

# This machine can now be serialized
smach = serializable(mach)
JLSO.save(&quot;machine.jlso&quot;, :machine =&gt; smach)

# Deserialize and restore learned parameters to useable form:
loaded_mach = JLSO.load(&quot;machine.jlso&quot;)[:machine]
restore!(loaded_mach)

predict(loaded_mach, X)
predict(mach, X)</code></pre><p>See also <a href="#MLJBase.restore!-Tuple{Machine}"><code>restore!</code></a>, <a href="#MLJModelInterface.save-Tuple{Union{IO, String}, Machine}"><code>MLJBase.save</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/machines.jl#LL939-L979">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.thaw!-Tuple{Machine}" href="#MLJBase.thaw!-Tuple{Machine}"><code>MLJBase.thaw!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">thaw!(mach)</code></pre><p>Unfreeze the machine <code>mach</code> so that it can be retrained.</p><p>See also <a href="#MLJBase.freeze!-Tuple{Machine}"><code>freeze!</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/machines.jl#LL420-L426">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJModelInterface.feature_importances-Tuple{Machine}" href="#MLJModelInterface.feature_importances-Tuple{Machine}"><code>MLJModelInterface.feature_importances</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">feature_importances(mach::Machine)</code></pre><p>Return a list of <code>feature =&gt; importance</code> pairs for a fitted machine, <code>mach</code>, for supported models. Otherwise return <code>nothing</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/machines.jl#LL905-L911">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJModelInterface.fitted_params-Tuple{Machine}" href="#MLJModelInterface.fitted_params-Tuple{Machine}"><code>MLJModelInterface.fitted_params</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">fitted_params(mach)</code></pre><p>Return the learned parameters for a machine <code>mach</code> that has been <code>fit!</code>, for example the coefficients in a linear model.</p><p>This is a named tuple and human-readable if possible.</p><p>If <code>mach</code> is a machine for a composite model, such as a model constructed using <code>@pipeline</code>, then the returned named tuple has the composite type&#39;s field names as keys. The corresponding value is the fitted parameters for the machine in the underlying learning network bound to that model. (If multiple machines share the same model, then the value is a vector.)</p><pre><code class="language-julia hljs">using MLJ
@load LogisticClassifier pkg=MLJLinearModels
X, y = @load_crabs;
pipe = @pipeline Standardizer LogisticClassifier
mach = machine(pipe, X, y) |&gt; fit!

julia&gt; fitted_params(mach).logistic_classifier
(classes = CategoricalArrays.CategoricalValue{String,UInt32}[&quot;B&quot;, &quot;O&quot;],
 coefs = Pair{Symbol,Float64}[:FL =&gt; 3.7095037897680405, :RW =&gt; 0.1135739140854546, :CL =&gt; -1.6036892745322038, :CW =&gt; -4.415667573486482, :BD =&gt; 3.238476051092471],
 intercept = 0.0883301599726305,)</code></pre><p>Additional keys, <code>machines</code> and <code>fitted_params_given_machine</code>, give a list of <em>all</em> machines in the underlying network, and a dictionary of fitted parameters keyed on those machines.</p><p>See also <a href="#MLJBase.report-Tuple{MLJBase.CompositeFitresult}"><code>report</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/machines.jl#LL784-L818">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJModelInterface.save-Tuple{Union{IO, String}, Machine}" href="#MLJModelInterface.save-Tuple{Union{IO, String}, Machine}"><code>MLJModelInterface.save</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">MLJ.save(filename, mach::Machine)
MLJ.save(io, mach::Machine)

MLJBase.save(filename, mach::Machine)
MLJBase.save(io, mach::Machine)</code></pre><p>Serialize the machine <code>mach</code> to a file with path <code>filename</code>, or to an input/output stream <code>io</code> (at least <code>IOBuffer</code> instances are supported) using the Serialization module.</p><p>To serialise using a different format, see <a href="#MLJBase.serializable-Union{Tuple{Machine{&lt;:Any, C}}, Tuple{C}} where C"><code>serializable</code></a>.</p><p>Machines are deserialized using the <code>machine</code> constructor as shown in the example below.</p><blockquote><p>The implementation of <code>save</code> for machines changed in MLJ 0.18  (MLJBase 0.20). You can only restore a machine saved using older  versions of MLJ using an older version.</p></blockquote><p><strong>Example</strong></p><pre><code class="nohighlight hljs">using MLJ
Tree = @load DecisionTreeClassifier
X, y = @load_iris
mach = fit!(machine(Tree(), X, y))

MLJ.save(&quot;tree.jls&quot;, mach)
mach_predict_only = machine(&quot;tree.jls&quot;)
predict(mach_predict_only, X)

# using a buffer:
io = IOBuffer()
MLJ.save(io, mach)
seekstart(io)
predict_only_mach = machine(io)
predict(predict_only_mach, X)</code></pre><div class="admonition is-warning"><header class="admonition-header">Only load files from trusted sources</header><div class="admonition-body"><p>Maliciously constructed JLS files, like pickles, and most other general purpose serialization formats, can allow for arbitrary code execution during loading. This means it is possible for someone to use a JLS file that looks like a serialized MLJ machine as a <a href="https://en.wikipedia.org/wiki/Trojan_horse_(computing)">Trojan horse</a>.</p></div></div><p>See also <a href="#MLJBase.serializable-Union{Tuple{Machine{&lt;:Any, C}}, Tuple{C}} where C"><code>serializable</code></a>, <a href="#MLJBase.machine"><code>machine</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/machines.jl#LL1030-L1078">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="StatsAPI.fit!-Tuple{Machine{&lt;:Surrogate}}" href="#StatsAPI.fit!-Tuple{Machine{&lt;:Surrogate}}"><code>StatsAPI.fit!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">fit!(mach::Machine{&lt;:Surrogate};
     rows=nothing,
     acceleration=CPU1(),
     verbosity=1,
     force=false))</code></pre><p>Train the complete learning network wrapped by the machine <code>mach</code>.</p><p>More precisely, if <code>s</code> is the learning network signature used to construct <code>mach</code>, then call <code>fit!(N)</code>, where <code>N</code> is a greatest lower bound of the nodes appearing in the signature (values in the signature that are not <code>AbstractNode</code> are ignored). For example, if <code>s = (predict=yhat, transform=W)</code>, then call <code>fit!(glb(yhat, W))</code>.</p><p>See also <a href="#MLJBase.machine"><code>machine</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/composition/learning_networks/deprecated_machines.jl#LL221-L238">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="StatsAPI.fit!-Tuple{Machine}" href="#StatsAPI.fit!-Tuple{Machine}"><code>StatsAPI.fit!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">fit!(mach::Machine, rows=nothing, verbosity=1, force=false)</code></pre><p>Fit the machine <code>mach</code>. In the case that <code>mach</code> has <code>Node</code> arguments, first train all other machines on which <code>mach</code> depends.</p><p>To attempt to fit a machine without touching any other machine, use <code>fit_only!</code>. For more on the internal logic of fitting see <a href="#MLJBase.fit_only!-Union{Tuple{Machine{&lt;:Any, cache_data}}, Tuple{cache_data}} where cache_data"><code>fit_only!</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/machines.jl#LL763-L774">source</a></section></article><h2 id="Parameter-Inspection"><a class="docs-heading-anchor" href="#Parameter-Inspection">Parameter Inspection</a><a id="Parameter-Inspection-1"></a><a class="docs-heading-anchor-permalink" href="#Parameter-Inspection" title="Permalink"></a></h2><h2 id="Show"><a class="docs-heading-anchor" href="#Show">Show</a><a id="Show-1"></a><a class="docs-heading-anchor-permalink" href="#Show" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="MLJBase._recursive_show-Tuple{IO, MLJType, Any, Any}" href="#MLJBase._recursive_show-Tuple{IO, MLJType, Any, Any}"><code>MLJBase._recursive_show</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">_recursive_show(stream, object, current_depth, depth)</code></pre><p>Generate a table of the properties of the <code>MLJType</code> object, dislaying each property value by calling the method <code>_show</code> on it. The behaviour of <code>_show(stream, f)</code> is as follows:</p><ol><li>If <code>f</code> is itself a <code>MLJType</code> object, then its short form is shown</li></ol><p>and <code>_recursive_show</code> generates as separate table for each of its properties (and so on, up to a depth of argument <code>depth</code>).</p><ol><li>Otherwise <code>f</code> is displayed as &quot;(omitted T)&quot; where <code>T = typeof(f)</code>,</li></ol><p>unless <code>istoobig(f)</code> is false (the <code>istoobig</code> fall-back for arbitrary types being <code>true</code>). In the latter case, the long (ie, MIME&quot;plain/text&quot;) form of <code>f</code> is shown. To override this behaviour, overload the <code>_show</code> method for the type in question.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/show.jl#LL320-L337">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.abbreviated-Tuple{Any}" href="#MLJBase.abbreviated-Tuple{Any}"><code>MLJBase.abbreviated</code></a> — <span class="docstring-category">Method</span></header><section><div><p>to display abbreviated versions of integers</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/show.jl#LL51">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.color_off-Tuple{}" href="#MLJBase.color_off-Tuple{}"><code>MLJBase.color_off</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">color_off()</code></pre><p>Suppress color and bold output at the REPL for displaying MLJ objects.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/show.jl#LL8-L13">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.color_on-Tuple{}" href="#MLJBase.color_on-Tuple{}"><code>MLJBase.color_on</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">color_on()</code></pre><p>Enable color and bold output at the REPL, for enhanced display of MLJ objects.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/show.jl#LL1-L6">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.handle-Tuple{Any}" href="#MLJBase.handle-Tuple{Any}"><code>MLJBase.handle</code></a> — <span class="docstring-category">Method</span></header><section><div><p>return abbreviated object id (as string) or it&#39;s registered handle (as string) if this exists</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/show.jl#LL57-L59">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.@constant-Tuple{Any}" href="#MLJBase.@constant-Tuple{Any}"><code>MLJBase.@constant</code></a> — <span class="docstring-category">Macro</span></header><section><div><pre><code class="language-julia hljs">@constant x = value</code></pre><p>Private method (used in testing).</p><p>Equivalent to <code>const x = value</code> but registers the binding thus:</p><pre><code class="nohighlight hljs">MLJBase.HANDLE_GIVEN_ID[objectid(value)] = :x</code></pre><p>Registered objects get displayed using the variable name to which it was bound in calls to <code>show(x)</code>, etc.</p><p>WARNING: As with any <code>const</code> declaration, binding <code>x</code> to new value of the same type is not prevented and the registration will not be updated.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/show.jl#LL23-L38">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.@more-Tuple{}" href="#MLJBase.@more-Tuple{}"><code>MLJBase.@more</code></a> — <span class="docstring-category">Macro</span></header><section><div><pre><code class="language-julia hljs">@more</code></pre><p>Entered at the REPL, equivalent to <code>show(ans, 100)</code>. Use to get a recursive description of all properties of the last REPL value.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/show.jl#LL222-L228">source</a></section></article><h2 id="Utility-functions"><a class="docs-heading-anchor" href="#Utility-functions">Utility functions</a><a id="Utility-functions-1"></a><a class="docs-heading-anchor-permalink" href="#Utility-functions" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="MLJBase.Accuracy" href="#MLJBase.Accuracy"><code>MLJBase.Accuracy</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MLJBase.Accuracy</code></pre><p>A measure type for accuracy, which includes the instance(s): <code>accuracy</code>.</p><pre><code class="nohighlight hljs">Accuracy()(ŷ, y)
Accuracy()(ŷ, y, w)</code></pre><p>Evaluate the accuracy on predictions <code>ŷ</code>, given ground truth observations <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p>Accuracy is proportion of correct predictions <code>ŷ[i]</code> that match the ground truth <code>y[i]</code> observations. This metric is invariant to class reordering.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Union{Finite,Missing}</code> (multiclass classification); <code>ŷ</code> must be an array of <code>deterministic</code> predictions. </p><p>For more information, run <code>info(Accuracy)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/measures/meta_utilities.jl#LL83-L99">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.AreaUnderCurve" href="#MLJBase.AreaUnderCurve"><code>MLJBase.AreaUnderCurve</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MLJBase.AreaUnderCurve</code></pre><p>A measure type for area under the ROC, which includes the instance(s): <code>area_under_curve</code>, <code>auc</code>.</p><pre><code class="nohighlight hljs">AreaUnderCurve()(ŷ, y)</code></pre><p>Evaluate the area under the ROC on predictions <code>ŷ</code>, given ground truth observations <code>y</code>. </p><p>Returns the area under the ROC (<a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic">receiver operator characteristic</a>)</p><p>If <code>missing</code> or <code>NaN</code> values are present, use <code>auc(skipinvalid(yhat, y)...)</code>.</p><p>This metric is invariant to class reordering.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>Union{AbstractArray{&lt;:Union{Missing, ScientificTypesBase.Multiclass{2}}}, AbstractArray{&lt;:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}}</code>; <code>ŷ</code> must be an array of <code>probabilistic</code> predictions. </p><p>For more information, run <code>info(AreaUnderCurve)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/measures/meta_utilities.jl#LL83-L102">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.BalancedAccuracy" href="#MLJBase.BalancedAccuracy"><code>MLJBase.BalancedAccuracy</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MLJBase.BalancedAccuracy</code></pre><p>A measure type for balanced accuracy, which includes the instance(s): <code>balanced_accuracy</code>, <code>bacc</code>, <code>bac</code>.</p><pre><code class="nohighlight hljs">BalancedAccuracy()(ŷ, y)
BalancedAccuracy()(ŷ, y, w)</code></pre><p>Evaluate the default instance of BalancedAccuracy on predictions <code>ŷ</code>, given ground truth observations <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p>Balanced accuracy compensates standard <a href="#MLJBase.Accuracy"><code>Accuracy</code></a> for class imbalance. See <a href="https://en.wikipedia.org/wiki/Precision_and_recall#Imbalanced_data">https://en.wikipedia.org/wiki/Precision<em>and</em>recall#Imbalanced_data</a>. </p><p>Setting <code>adjusted=true</code> rescales the score in the way prescribed in <a href="https://lib.dr.iastate.edu/etd/13537/">L. Mosley (2013): A balanced approach to the multi-class imbalance problem. PhD thesis, Iowa State University</a>. In the binary case, the adjusted balanced accuracy is also known as <em>Youden’s J statistic</em>, or <em>informedness</em>.</p><p>This metric is invariant to class reordering.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Union{Finite,Missing}</code> (multiclass classification); <code>ŷ</code> must be an array of <code>deterministic</code> predictions. </p><p>For more information, run <code>info(BalancedAccuracy)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/measures/meta_utilities.jl#LL83-L108">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.BrierLoss" href="#MLJBase.BrierLoss"><code>MLJBase.BrierLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MLJBase.BrierLoss</code></pre><p>A measure type for Brier loss (a.k.a. quadratic loss), which includes the instance(s): <code>brier_loss</code>.</p><pre><code class="nohighlight hljs">BrierLoss()(ŷ, y)
BrierLoss()(ŷ, y, w)</code></pre><p>Evaluate the Brier loss (a.k.a. quadratic loss) on predictions <code>ŷ</code>, given ground truth observations <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p>For details, see <a href="#MLJBase.BrierScore"><code>BrierScore</code></a>, which differs only by a sign.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbtractArray{&lt;:Union{Missing,T}</code> where <code>T</code> is <code>Continuous</code> or <code>Count</code> (for respectively continuous or discrete Distribution.jl objects in <code>ŷ</code>) or  <code>OrderedFactor</code> or <code>Multiclass</code> (for <code>UnivariateFinite</code> distributions in <code>ŷ</code>); <code>ŷ</code> must be an array of <code>probabilistic</code> predictions. </p><p>For more information, run <code>info(BrierLoss)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/measures/meta_utilities.jl#LL83-L98">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.BrierScore" href="#MLJBase.BrierScore"><code>MLJBase.BrierScore</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MLJBase.BrierScore</code></pre><p>A measure type for Brier score (a.k.a. quadratic score), which includes the instance(s): <code>brier_score</code>.</p><pre><code class="nohighlight hljs">BrierScore()(ŷ, y)
BrierScore()(ŷ, y, w)</code></pre><p>Evaluate the Brier score (a.k.a. quadratic score) on predictions <code>ŷ</code>, given ground truth observations <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p>Convention as in <a href="https://doi.org/10.1198/016214506000001437">Gneiting and Raftery (2007), &quot;StrictlyProper Scoring Rules, Prediction, and Estimation&quot;</a></p><p><em>Finite case.</em> If <code>p</code> is the predicted probability mass function for a <em>single</em> observation <code>η</code>, and <code>C</code> all possible classes, then the corresponding score for that observation is given by</p><p><span>$2p(η) - \left(\sum_{c ∈ C} p(c)^2\right) - 1$</span></p><p><em>Warning.</em> <code>BrierScore()</code> is a &quot;score&quot; in the sense that bigger is better (with <code>0</code> optimal, and all other values negative). In Brier&#39;s original 1950 paper, and many other places, it has the opposite sign, despite the name. Moreover, the present implementation does not treat the binary case as special, so that the score may differ in the binary case by a factor of two from usage elsewhere.</p><p><em>Infinite case.</em> Replacing the sum above with an integral does <em>not</em> lead to the formula adopted here in the case of <code>Continuous</code> or <code>Count</code> target <code>y</code>. Rather the convention in the paper cited above is adopted, which means returning a score of</p><p><span>$2p(η) - ∫ p(t)^2 dt$</span></p><p>in the <code>Continuous</code> case (<code>p</code> the probablity density function) or</p><p><span>$2p(η) - ∑_t p(t)^2$</span></p><p>in the <code>Count</code> cae (<code>p</code> the probablity mass function).</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbtractArray{&lt;:Union{Missing,T}</code> where <code>T</code> is <code>Continuous</code> or <code>Count</code> (for respectively continuous or discrete Distribution.jl objects in <code>ŷ</code>) or  <code>OrderedFactor</code> or <code>Multiclass</code> (for <code>UnivariateFinite</code> distributions in <code>ŷ</code>); <code>ŷ</code> must be an array of <code>probabilistic</code> predictions. </p><p>For more information, run <code>info(BrierScore)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/measures/meta_utilities.jl#LL83-L124">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.ConfusionMatrix" href="#MLJBase.ConfusionMatrix"><code>MLJBase.ConfusionMatrix</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MLJBase.ConfusionMatrix</code></pre><p>A measure type for confusion matrix, which includes the instance(s): <code>confusion_matrix</code>, <code>confmat</code>.</p><pre><code class="nohighlight hljs">ConfusionMatrix()(ŷ, y)</code></pre><p>Evaluate the default instance of ConfusionMatrix on predictions <code>ŷ</code>, given ground truth observations <code>y</code>. </p><p>If <code>r</code> is the return value, then the raw confusion matrix is <code>r.mat</code>, whose rows correspond to predictions, and columns to ground truth. The ordering follows that of <code>levels(y)</code>.</p><p>Use <code>ConfusionMatrix(perm=[2, 1])</code> to reverse the class order for binary data. For more than two classes, specify an appropriate permutation, as in <code>ConfusionMatrix(perm=[2, 3, 1])</code>.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Union{OrderedFactor{2},Missing}}</code> (binary classification where choice of &quot;true&quot; effects the measure); <code>ŷ</code> must be an array of <code>deterministic</code> predictions. </p><p>For more information, run <code>info(ConfusionMatrix)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/measures/meta_utilities.jl#LL83-L104">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.DWDMarginLoss" href="#MLJBase.DWDMarginLoss"><code>MLJBase.DWDMarginLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MLJBase.DWDMarginLoss</code></pre><p>A measure type for distance weighted discrimination loss, which includes the instance(s): <code>dwd_margin_loss</code>.</p><pre><code class="nohighlight hljs">DWDMarginLoss()(ŷ, y)
DWDMarginLoss()(ŷ, y, w)</code></pre><p>Evaluate the default instance of DWDMarginLoss on predictions <code>ŷ</code>, given ground truth observations <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p>For more detail, see the original LossFunctions.jl documentation <em>but note differences in the signature.</em></p><p>Losses from LossFunctions.jl do not support <code>missing</code> values. To use with <code>missing</code> values, replace <code>(ŷ, y)</code> with <code>skipinvalid(ŷ, y))</code>.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Union{Finite{2},Missing}}</code> (binary classification); <code>ŷ</code> must be an array of <code>probabilistic</code> predictions. </p><p>Constructor signature: <code>DWDMarginLoss(; q=1.0)</code></p><p>For more information, run <code>info(DWDMarginLoss)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/measures/meta_utilities.jl#LL83-L104">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.ExpLoss" href="#MLJBase.ExpLoss"><code>MLJBase.ExpLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MLJBase.ExpLoss</code></pre><p>A measure type for exp loss, which includes the instance(s): <code>exp_loss</code>.</p><pre><code class="nohighlight hljs">ExpLoss()(ŷ, y)
ExpLoss()(ŷ, y, w)</code></pre><p>Evaluate the default instance of ExpLoss on predictions <code>ŷ</code>, given ground truth observations <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p>For more detail, see the original LossFunctions.jl documentation <em>but note differences in the signature.</em></p><p>Losses from LossFunctions.jl do not support <code>missing</code> values. To use with <code>missing</code> values, replace <code>(ŷ, y)</code> with <code>skipinvalid(ŷ, y))</code>.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Union{Finite{2},Missing}}</code> (binary classification); <code>ŷ</code> must be an array of <code>probabilistic</code> predictions. </p><p>For more information, run <code>info(ExpLoss)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/measures/meta_utilities.jl#LL83-L102">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.FScore" href="#MLJBase.FScore"><code>MLJBase.FScore</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MLJBase.FScore</code></pre><p>A measure type for F-Score, which includes the instance(s): <code>f1score</code>.</p><pre><code class="nohighlight hljs">FScore()(ŷ, y)</code></pre><p>Evaluate the default instance of FScore on predictions <code>ŷ</code>, given ground truth observations <code>y</code>. </p><p>This is the one-parameter generalization, <span>$F_β$</span>, of the F-measure or balanced F-score.</p><p><a href="https://en.wikipedia.org/wiki/F1_score">https://en.wikipedia.org/wiki/F1_score</a></p><p>Constructor signature: <code>FScore(; β=1.0, rev=true)</code>.</p><p>By default, the second element of <code>levels(y)</code> is designated as <code>true</code>. To reverse roles, specify <code>rev=true</code>.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Union{OrderedFactor{2},Missing}}</code> (binary classification where choice of &quot;true&quot; effects the measure); <code>ŷ</code> must be an array of <code>deterministic</code> predictions. </p><p>Constructor signature: <code>FScore(β=1.0, rev=false)</code>. </p><p>For more information, run <code>info(FScore)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/measures/meta_utilities.jl#LL83-L107">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.FalseDiscoveryRate" href="#MLJBase.FalseDiscoveryRate"><code>MLJBase.FalseDiscoveryRate</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MLJBase.FalseDiscoveryRate</code></pre><p>A measure type for false discovery rate, which includes the instance(s): <code>false_discovery_rate</code>, <code>falsediscovery_rate</code>, <code>fdr</code>.</p><pre><code class="nohighlight hljs">FalseDiscoveryRate()(ŷ, y)</code></pre><p>Evaluate the default instance of FalseDiscoveryRate on predictions <code>ŷ</code>, given ground truth observations <code>y</code>. </p><p>Assigns <code>false</code> to first element of <code>levels(y)</code>. To reverse roles, use <code>FalseDiscoveryRate(rev=true)</code>.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Union{OrderedFactor{2},Missing}}</code> (binary classification where choice of &quot;true&quot; effects the measure); <code>ŷ</code> must be an array of <code>deterministic</code> predictions. </p><p>For more information, run <code>info(FalseDiscoveryRate)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/measures/meta_utilities.jl#LL83-L98">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.FalseNegative" href="#MLJBase.FalseNegative"><code>MLJBase.FalseNegative</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MLJBase.FalseNegative</code></pre><p>A measure type for number of false negatives, which includes the instance(s): <code>false_negative</code>, <code>falsenegative</code>.</p><pre><code class="nohighlight hljs">FalseNegative()(ŷ, y)</code></pre><p>Evaluate the default instance of FalseNegative on predictions <code>ŷ</code>, given ground truth observations <code>y</code>. </p><p>Assigns <code>false</code> to first element of <code>levels(y)</code>. To reverse roles, use <code>FalseNegative(rev=true)</code>.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Union{OrderedFactor{2},Missing}}</code> (binary classification where choice of &quot;true&quot; effects the measure); <code>ŷ</code> must be an array of <code>deterministic</code> predictions. </p><p>For more information, run <code>info(FalseNegative)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/measures/meta_utilities.jl#LL83-L98">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.FalseNegativeRate" href="#MLJBase.FalseNegativeRate"><code>MLJBase.FalseNegativeRate</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MLJBase.FalseNegativeRate</code></pre><p>A measure type for false negative rate, which includes the instance(s): <code>false_negative_rate</code>, <code>falsenegative_rate</code>, <code>fnr</code>, <code>miss_rate</code>.</p><pre><code class="nohighlight hljs">FalseNegativeRate()(ŷ, y)</code></pre><p>Evaluate the default instance of FalseNegativeRate on predictions <code>ŷ</code>, given ground truth observations <code>y</code>. </p><p>Assigns <code>false</code> to first element of <code>levels(y)</code>. To reverse roles, use <code>FalseNegativeRate(rev=true)</code>.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Union{OrderedFactor{2},Missing}}</code> (binary classification where choice of &quot;true&quot; effects the measure); <code>ŷ</code> must be an array of <code>deterministic</code> predictions. </p><p>For more information, run <code>info(FalseNegativeRate)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/measures/meta_utilities.jl#LL83-L98">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.FalsePositive" href="#MLJBase.FalsePositive"><code>MLJBase.FalsePositive</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MLJBase.FalsePositive</code></pre><p>A measure type for number of false positives, which includes the instance(s): <code>false_positive</code>, <code>falsepositive</code>.</p><pre><code class="nohighlight hljs">FalsePositive()(ŷ, y)</code></pre><p>Evaluate the default instance of FalsePositive on predictions <code>ŷ</code>, given ground truth observations <code>y</code>. </p><p>Assigns <code>false</code> to first element of <code>levels(y)</code>. To reverse roles, use <code>FalsePositive(rev=true)</code>.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Union{OrderedFactor{2},Missing}}</code> (binary classification where choice of &quot;true&quot; effects the measure); <code>ŷ</code> must be an array of <code>deterministic</code> predictions. </p><p>For more information, run <code>info(FalsePositive)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/measures/meta_utilities.jl#LL83-L98">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.FalsePositiveRate" href="#MLJBase.FalsePositiveRate"><code>MLJBase.FalsePositiveRate</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MLJBase.FalsePositiveRate</code></pre><p>A measure type for false positive rate, which includes the instance(s): <code>false_positive_rate</code>, <code>falsepositive_rate</code>, <code>fpr</code>, <code>fallout</code>.</p><pre><code class="nohighlight hljs">FalsePositiveRate()(ŷ, y)</code></pre><p>Evaluate the default instance of FalsePositiveRate on predictions <code>ŷ</code>, given ground truth observations <code>y</code>. </p><p>Assigns <code>false</code> to first element of <code>levels(y)</code>. To reverse roles, use <code>FalsePositiveRate(rev=true)</code>.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Union{OrderedFactor{2},Missing}}</code> (binary classification where choice of &quot;true&quot; effects the measure); <code>ŷ</code> must be an array of <code>deterministic</code> predictions. </p><p>For more information, run <code>info(FalsePositiveRate)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/measures/meta_utilities.jl#LL83-L98">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.HuberLoss" href="#MLJBase.HuberLoss"><code>MLJBase.HuberLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MLJBase.HuberLoss</code></pre><p>A measure type for huber loss, which includes the instance(s): <code>huber_loss</code>.</p><pre><code class="nohighlight hljs">HuberLoss()(ŷ, y)
HuberLoss()(ŷ, y, w)</code></pre><p>Evaluate the default instance of HuberLoss on predictions <code>ŷ</code>, given ground truth observations <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p>For more detail, see the original LossFunctions.jl documentation <em>but note differences in the signature.</em></p><p>Losses from LossFunctions.jl do not support <code>missing</code> values. To use with <code>missing</code> values, replace <code>(ŷ, y)</code> with <code>skipinvalid(ŷ, y))</code>.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>Union{AbstractVector{ScientificTypesBase.Continuous}, AbstractVector{ScientificTypesBase.Count}}</code>; <code>ŷ</code> must be an array of <code>deterministic</code> predictions. </p><p>Constructor signature: <code>HuberLoss(; d=1.0)</code></p><p>For more information, run <code>info(HuberLoss)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/measures/meta_utilities.jl#LL83-L104">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.Kappa" href="#MLJBase.Kappa"><code>MLJBase.Kappa</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MLJBase.Kappa</code></pre><p>A measure type for kappa, which includes the instance(s): <code>kappa</code>.</p><pre><code class="nohighlight hljs">Kappa()(ŷ, y)</code></pre><p>Evaluate the kappa on predictions <code>ŷ</code>, given ground truth observations <code>y</code>. </p><p>A metric to measure agreement between predicted labels and the ground truth.  See <a href="https://en.wikipedia.org/wiki/Cohen%27s_kappa">https://en.wikipedia.org/wiki/Cohen%27s_kappa</a></p><p>This metric is invariant to class reordering.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Union{Finite,Missing}</code> (multiclass classification); <code>ŷ</code> must be an array of <code>deterministic</code> predictions. </p><p>For more information, run <code>info(Kappa)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/measures/meta_utilities.jl#LL83-L100">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.L1EpsilonInsLoss" href="#MLJBase.L1EpsilonInsLoss"><code>MLJBase.L1EpsilonInsLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MLJBase.L1EpsilonInsLoss</code></pre><p>A measure type for l1 ϵ-insensitive loss, which includes the instance(s): <code>l1_epsilon_ins_loss</code>.</p><pre><code class="nohighlight hljs">L1EpsilonInsLoss()(ŷ, y)
L1EpsilonInsLoss()(ŷ, y, w)</code></pre><p>Evaluate the default instance of L1EpsilonInsLoss on predictions <code>ŷ</code>, given ground truth observations <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p>For more detail, see the original LossFunctions.jl documentation <em>but note differences in the signature.</em></p><p>Losses from LossFunctions.jl do not support <code>missing</code> values. To use with <code>missing</code> values, replace <code>(ŷ, y)</code> with <code>skipinvalid(ŷ, y))</code>.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>Union{AbstractVector{ScientificTypesBase.Continuous}, AbstractVector{ScientificTypesBase.Count}}</code>; <code>ŷ</code> must be an array of <code>deterministic</code> predictions. </p><p>Constructor signature: <code>L1EpsilonInsLoss(; ε=1.0)</code></p><p>For more information, run <code>info(L1EpsilonInsLoss)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/measures/meta_utilities.jl#LL83-L104">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.L1HingeLoss" href="#MLJBase.L1HingeLoss"><code>MLJBase.L1HingeLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MLJBase.L1HingeLoss</code></pre><p>A measure type for l1 hinge loss, which includes the instance(s): <code>l1_hinge_loss</code>.</p><pre><code class="nohighlight hljs">L1HingeLoss()(ŷ, y)
L1HingeLoss()(ŷ, y, w)</code></pre><p>Evaluate the default instance of L1HingeLoss on predictions <code>ŷ</code>, given ground truth observations <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p>For more detail, see the original LossFunctions.jl documentation <em>but note differences in the signature.</em></p><p>Losses from LossFunctions.jl do not support <code>missing</code> values. To use with <code>missing</code> values, replace <code>(ŷ, y)</code> with <code>skipinvalid(ŷ, y))</code>.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Union{Finite{2},Missing}}</code> (binary classification); <code>ŷ</code> must be an array of <code>probabilistic</code> predictions. </p><p>For more information, run <code>info(L1HingeLoss)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/measures/meta_utilities.jl#LL83-L102">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.L2EpsilonInsLoss" href="#MLJBase.L2EpsilonInsLoss"><code>MLJBase.L2EpsilonInsLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MLJBase.L2EpsilonInsLoss</code></pre><p>A measure type for l2 ϵ-insensitive loss, which includes the instance(s): <code>l2_epsilon_ins_loss</code>.</p><pre><code class="nohighlight hljs">L2EpsilonInsLoss()(ŷ, y)
L2EpsilonInsLoss()(ŷ, y, w)</code></pre><p>Evaluate the default instance of L2EpsilonInsLoss on predictions <code>ŷ</code>, given ground truth observations <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p>For more detail, see the original LossFunctions.jl documentation <em>but note differences in the signature.</em></p><p>Losses from LossFunctions.jl do not support <code>missing</code> values. To use with <code>missing</code> values, replace <code>(ŷ, y)</code> with <code>skipinvalid(ŷ, y))</code>.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>Union{AbstractVector{ScientificTypesBase.Continuous}, AbstractVector{ScientificTypesBase.Count}}</code>; <code>ŷ</code> must be an array of <code>deterministic</code> predictions. </p><p>Constructor signature: <code>L2EpsilonInsLoss(; ε=1.0)</code></p><p>For more information, run <code>info(L2EpsilonInsLoss)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/measures/meta_utilities.jl#LL83-L104">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.L2HingeLoss" href="#MLJBase.L2HingeLoss"><code>MLJBase.L2HingeLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MLJBase.L2HingeLoss</code></pre><p>A measure type for l2 hinge loss, which includes the instance(s): <code>l2_hinge_loss</code>.</p><pre><code class="nohighlight hljs">L2HingeLoss()(ŷ, y)
L2HingeLoss()(ŷ, y, w)</code></pre><p>Evaluate the default instance of L2HingeLoss on predictions <code>ŷ</code>, given ground truth observations <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p>For more detail, see the original LossFunctions.jl documentation <em>but note differences in the signature.</em></p><p>Losses from LossFunctions.jl do not support <code>missing</code> values. To use with <code>missing</code> values, replace <code>(ŷ, y)</code> with <code>skipinvalid(ŷ, y))</code>.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Union{Finite{2},Missing}}</code> (binary classification); <code>ŷ</code> must be an array of <code>probabilistic</code> predictions. </p><p>For more information, run <code>info(L2HingeLoss)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/measures/meta_utilities.jl#LL83-L102">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.L2MarginLoss" href="#MLJBase.L2MarginLoss"><code>MLJBase.L2MarginLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MLJBase.L2MarginLoss</code></pre><p>A measure type for l2 margin loss, which includes the instance(s): <code>l2_margin_loss</code>.</p><pre><code class="nohighlight hljs">L2MarginLoss()(ŷ, y)
L2MarginLoss()(ŷ, y, w)</code></pre><p>Evaluate the default instance of L2MarginLoss on predictions <code>ŷ</code>, given ground truth observations <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p>For more detail, see the original LossFunctions.jl documentation <em>but note differences in the signature.</em></p><p>Losses from LossFunctions.jl do not support <code>missing</code> values. To use with <code>missing</code> values, replace <code>(ŷ, y)</code> with <code>skipinvalid(ŷ, y))</code>.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Union{Finite{2},Missing}}</code> (binary classification); <code>ŷ</code> must be an array of <code>probabilistic</code> predictions. </p><p>For more information, run <code>info(L2MarginLoss)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/measures/meta_utilities.jl#LL83-L102">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.LPDistLoss" href="#MLJBase.LPDistLoss"><code>MLJBase.LPDistLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MLJBase.LPDistLoss</code></pre><p>A measure type for lp dist loss, which includes the instance(s): <code>lp_dist_loss</code>.</p><pre><code class="nohighlight hljs">LPDistLoss()(ŷ, y)
LPDistLoss()(ŷ, y, w)</code></pre><p>Evaluate the default instance of LPDistLoss on predictions <code>ŷ</code>, given ground truth observations <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p>For more detail, see the original LossFunctions.jl documentation <em>but note differences in the signature.</em></p><p>Losses from LossFunctions.jl do not support <code>missing</code> values. To use with <code>missing</code> values, replace <code>(ŷ, y)</code> with <code>skipinvalid(ŷ, y))</code>.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>Union{AbstractVector{ScientificTypesBase.Continuous}, AbstractVector{ScientificTypesBase.Count}}</code>; <code>ŷ</code> must be an array of <code>deterministic</code> predictions. </p><p>Constructor signature: <code>LPDistLoss(; P=2)</code></p><p>For more information, run <code>info(LPDistLoss)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/measures/meta_utilities.jl#LL83-L104">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.LPLoss" href="#MLJBase.LPLoss"><code>MLJBase.LPLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MLJBase.LPLoss</code></pre><p>A measure type for lp loss, which includes the instance(s): <code>l1</code>, <code>l2</code>.</p><pre><code class="nohighlight hljs">LPLoss()(ŷ, y)
LPLoss()(ŷ, y, w)</code></pre><p>Evaluate the default instance of LPLoss on predictions <code>ŷ</code>, given ground truth observations <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p>Constructor signature: <code>LPLoss(p=2)</code>. Reports <code>|ŷ[i] - y[i]|^p</code> for every index <code>i</code>.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Union{Infinite,Missing}}</code>; <code>ŷ</code> must be an array of <code>deterministic</code> predictions. </p><p>For more information, run <code>info(LPLoss)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/measures/meta_utilities.jl#LL83-L99">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.LogCoshLoss" href="#MLJBase.LogCoshLoss"><code>MLJBase.LogCoshLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MLJBase.LogCoshLoss</code></pre><p>A measure type for log cosh loss, which includes the instance(s): <code>log_cosh</code>, <code>log_cosh_loss</code>.</p><pre><code class="nohighlight hljs">LogCoshLoss()(ŷ, y)
LogCoshLoss()(ŷ, y, w)</code></pre><p>Evaluate the log cosh loss on predictions <code>ŷ</code>, given ground truth observations <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p>Reports <span>$\log(\cosh(ŷᵢ-yᵢ))$</span> for each index <code>i</code>. </p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Union{Infinite,Missing}}</code>; <code>ŷ</code> must be an array of <code>deterministic</code> predictions. </p><p>For more information, run <code>info(LogCoshLoss)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/measures/meta_utilities.jl#LL83-L97">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.LogLoss" href="#MLJBase.LogLoss"><code>MLJBase.LogLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MLJBase.LogLoss</code></pre><p>A measure type for log loss, which includes the instance(s): <code>log_loss</code>, <code>cross_entropy</code>.</p><pre><code class="nohighlight hljs">LogLoss()(ŷ, y)
LogLoss()(ŷ, y, w)</code></pre><p>Evaluate the default instance of LogLoss on predictions <code>ŷ</code>, given ground truth observations <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p>For details, see <a href="#MLJBase.LogScore"><code>LogScore</code></a>, which differs only by a sign.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbtractArray{&lt;:Union{Missing,T}</code> where <code>T</code> is <code>Continuous</code> or <code>Count</code> (for respectively continuous or discrete Distribution.jl objects in <code>ŷ</code>) or  <code>OrderedFactor</code> or <code>Multiclass</code> (for <code>UnivariateFinite</code> distributions in <code>ŷ</code>); <code>ŷ</code> must be an array of <code>probabilistic</code> predictions. </p><p>For more information, run <code>info(LogLoss)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/measures/meta_utilities.jl#LL83-L98">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.LogScore" href="#MLJBase.LogScore"><code>MLJBase.LogScore</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MLJBase.LogScore</code></pre><p>A measure type for log score, which includes the instance(s): <code>log_score</code>.</p><pre><code class="nohighlight hljs">LogScore()(ŷ, y)
LogScore()(ŷ, y, w)</code></pre><p>Evaluate the default instance of LogScore on predictions <code>ŷ</code>, given ground truth observations <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p>Since the score is undefined in the case that the true observation is predicted to occur with probability zero, probablities are clamped between <code>tol</code> and <code>1-tol</code>, where <code>tol</code> is a constructor key-word argument.</p><p>If <code>p</code> is the predicted probability mass or density function corresponding to a <em>single</em> ground truth observation <code>η</code>, then the score for that example is</p><pre><code class="nohighlight hljs">log(clamp(p(η), tol), 1 - tol)</code></pre><p>For example, for a binary target with &quot;yes&quot;/&quot;no&quot; labels, and predicted probability of &quot;yes&quot; equal to 0.8, an observation of &quot;no&quot; scores <code>log(0.2)</code>.</p><p>The predictions <code>ŷ</code> should be an array of <code>UnivariateFinite</code> distributions in the case of <code>Finite</code> target <code>y</code>, and otherwise a supported <code>Distributions.UnivariateDistribution</code> such as <code>Normal</code> or <code>Poisson</code>.</p><p>See also <a href="#MLJBase.LogLoss"><code>LogLoss</code></a>, which differs only in sign.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbtractArray{&lt;:Union{Missing,T}</code> where <code>T</code> is <code>Continuous</code> or <code>Count</code> (for respectively continuous or discrete Distribution.jl objects in <code>ŷ</code>) or  <code>OrderedFactor</code> or <code>Multiclass</code> (for <code>UnivariateFinite</code> distributions in <code>ŷ</code>); <code>ŷ</code> must be an array of <code>probabilistic</code> predictions. </p><p>For more information, run <code>info(LogScore)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/measures/meta_utilities.jl#LL83-L118">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.LogitDistLoss" href="#MLJBase.LogitDistLoss"><code>MLJBase.LogitDistLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MLJBase.LogitDistLoss</code></pre><p>A measure type for logit dist loss, which includes the instance(s): <code>logit_dist_loss</code>.</p><pre><code class="nohighlight hljs">LogitDistLoss()(ŷ, y)
LogitDistLoss()(ŷ, y, w)</code></pre><p>Evaluate the default instance of LogitDistLoss on predictions <code>ŷ</code>, given ground truth observations <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p>For more detail, see the original LossFunctions.jl documentation <em>but note differences in the signature.</em></p><p>Losses from LossFunctions.jl do not support <code>missing</code> values. To use with <code>missing</code> values, replace <code>(ŷ, y)</code> with <code>skipinvalid(ŷ, y))</code>.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>Union{AbstractVector{ScientificTypesBase.Continuous}, AbstractVector{ScientificTypesBase.Count}}</code>; <code>ŷ</code> must be an array of <code>deterministic</code> predictions. </p><p>For more information, run <code>info(LogitDistLoss)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/measures/meta_utilities.jl#LL83-L102">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.LogitMarginLoss" href="#MLJBase.LogitMarginLoss"><code>MLJBase.LogitMarginLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MLJBase.LogitMarginLoss</code></pre><p>A measure type for logit margin loss, which includes the instance(s): <code>logit_margin_loss</code>.</p><pre><code class="nohighlight hljs">LogitMarginLoss()(ŷ, y)
LogitMarginLoss()(ŷ, y, w)</code></pre><p>Evaluate the default instance of LogitMarginLoss on predictions <code>ŷ</code>, given ground truth observations <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p>For more detail, see the original LossFunctions.jl documentation <em>but note differences in the signature.</em></p><p>Losses from LossFunctions.jl do not support <code>missing</code> values. To use with <code>missing</code> values, replace <code>(ŷ, y)</code> with <code>skipinvalid(ŷ, y))</code>.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Union{Finite{2},Missing}}</code> (binary classification); <code>ŷ</code> must be an array of <code>probabilistic</code> predictions. </p><p>For more information, run <code>info(LogitMarginLoss)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/measures/meta_utilities.jl#LL83-L102">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.MatthewsCorrelation" href="#MLJBase.MatthewsCorrelation"><code>MLJBase.MatthewsCorrelation</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MLJBase.MatthewsCorrelation</code></pre><p>A measure type for matthews correlation, which includes the instance(s): <code>matthews_correlation</code>, <code>mcc</code>.</p><pre><code class="nohighlight hljs">MatthewsCorrelation()(ŷ, y)</code></pre><p>Evaluate the matthews correlation on predictions <code>ŷ</code>, given ground truth observations <code>y</code>. </p><p><a href="https://en.wikipedia.org/wiki/Matthews_correlation_coefficient">https://en.wikipedia.org/wiki/Matthews<em>correlation</em>coefficient</a> This metric is invariant to class reordering.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Union{Finite{2},Missing}}</code> (binary classification); <code>ŷ</code> must be an array of <code>deterministic</code> predictions. </p><p>For more information, run <code>info(MatthewsCorrelation)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/measures/meta_utilities.jl#LL83-L98">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.MeanAbsoluteError" href="#MLJBase.MeanAbsoluteError"><code>MLJBase.MeanAbsoluteError</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MLJBase.MeanAbsoluteError</code></pre><p>A measure type for mean absolute error, which includes the instance(s): <code>mae</code>, <code>mav</code>, <code>mean_absolute_error</code>, <code>mean_absolute_value</code>.</p><pre><code class="nohighlight hljs">MeanAbsoluteError()(ŷ, y)
MeanAbsoluteError()(ŷ, y, w)</code></pre><p>Evaluate the mean absolute error on predictions <code>ŷ</code>, given ground truth observations <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p><span>$\text{mean absolute error} =  n^{-1}∑ᵢ|yᵢ-ŷᵢ|$</span> or <span>$\text{mean absolute error} = n^{-1}∑ᵢwᵢ|yᵢ-ŷᵢ|$</span></p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Union{Infinite,Missing}}</code>; <code>ŷ</code> must be an array of <code>deterministic</code> predictions. </p><p>For more information, run <code>info(MeanAbsoluteError)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/measures/meta_utilities.jl#LL83-L99">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.MeanAbsoluteProportionalError" href="#MLJBase.MeanAbsoluteProportionalError"><code>MLJBase.MeanAbsoluteProportionalError</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MLJBase.MeanAbsoluteProportionalError</code></pre><p>A measure type for mean absolute proportional error, which includes the instance(s): <code>mape</code>.</p><pre><code class="nohighlight hljs">MeanAbsoluteProportionalError()(ŷ, y)
MeanAbsoluteProportionalError()(ŷ, y, w)</code></pre><p>Evaluate the default instance of MeanAbsoluteProportionalError on predictions <code>ŷ</code>, given ground truth observations <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p>Constructor key-word arguments: <code>tol</code> (default = <code>eps()</code>).</p><p><span>$\text{mean absolute proportional error} =  m^{-1}∑ᵢ|{(yᵢ-ŷᵢ) \over yᵢ}|$</span></p><p>where the sum is over indices such that <code>abs(yᵢ) &gt; tol</code> and <code>m</code> is the number of such indices.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Union{Infinite,Missing}}</code>; <code>ŷ</code> must be an array of <code>deterministic</code> predictions. </p><p>For more information, run <code>info(MeanAbsoluteProportionalError)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/measures/meta_utilities.jl#LL83-L103">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.MisclassificationRate" href="#MLJBase.MisclassificationRate"><code>MLJBase.MisclassificationRate</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MLJBase.MisclassificationRate</code></pre><p>A measure type for misclassification rate, which includes the instance(s): <code>misclassification_rate</code>, <code>mcr</code>.</p><pre><code class="nohighlight hljs">MisclassificationRate()(ŷ, y)
MisclassificationRate()(ŷ, y, w)</code></pre><p>Evaluate the misclassification rate on predictions <code>ŷ</code>, given ground truth observations <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p>A confusion matrix can also be passed as argument. This metric is invariant to class reordering.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Union{Finite,Missing}</code> (multiclass classification); <code>ŷ</code> must be an array of <code>deterministic</code> predictions. </p><p>For more information, run <code>info(MisclassificationRate)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/measures/meta_utilities.jl#LL83-L99">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.ModifiedHuberLoss" href="#MLJBase.ModifiedHuberLoss"><code>MLJBase.ModifiedHuberLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MLJBase.ModifiedHuberLoss</code></pre><p>A measure type for modified huber loss, which includes the instance(s): <code>modified_huber_loss</code>.</p><pre><code class="nohighlight hljs">ModifiedHuberLoss()(ŷ, y)
ModifiedHuberLoss()(ŷ, y, w)</code></pre><p>Evaluate the default instance of ModifiedHuberLoss on predictions <code>ŷ</code>, given ground truth observations <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p>For more detail, see the original LossFunctions.jl documentation <em>but note differences in the signature.</em></p><p>Losses from LossFunctions.jl do not support <code>missing</code> values. To use with <code>missing</code> values, replace <code>(ŷ, y)</code> with <code>skipinvalid(ŷ, y))</code>.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Union{Finite{2},Missing}}</code> (binary classification); <code>ŷ</code> must be an array of <code>probabilistic</code> predictions. </p><p>For more information, run <code>info(ModifiedHuberLoss)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/measures/meta_utilities.jl#LL83-L102">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.NegativePredictiveValue" href="#MLJBase.NegativePredictiveValue"><code>MLJBase.NegativePredictiveValue</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MLJBase.NegativePredictiveValue</code></pre><p>A measure type for negative predictive value, which includes the instance(s): <code>negative_predictive_value</code>, <code>negativepredictive_value</code>, <code>npv</code>.</p><pre><code class="nohighlight hljs">NegativePredictiveValue()(ŷ, y)</code></pre><p>Evaluate the default instance of NegativePredictiveValue on predictions <code>ŷ</code>, given ground truth observations <code>y</code>. </p><p>Assigns <code>false</code> to first element of <code>levels(y)</code>. To reverse roles, use <code>NegativePredictiveValue(rev=true)</code>.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Union{OrderedFactor{2},Missing}}</code> (binary classification where choice of &quot;true&quot; effects the measure); <code>ŷ</code> must be an array of <code>deterministic</code> predictions. </p><p>For more information, run <code>info(NegativePredictiveValue)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/measures/meta_utilities.jl#LL83-L98">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.PerceptronLoss" href="#MLJBase.PerceptronLoss"><code>MLJBase.PerceptronLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MLJBase.PerceptronLoss</code></pre><p>A measure type for perceptron loss, which includes the instance(s): <code>perceptron_loss</code>.</p><pre><code class="nohighlight hljs">PerceptronLoss()(ŷ, y)
PerceptronLoss()(ŷ, y, w)</code></pre><p>Evaluate the default instance of PerceptronLoss on predictions <code>ŷ</code>, given ground truth observations <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p>For more detail, see the original LossFunctions.jl documentation <em>but note differences in the signature.</em></p><p>Losses from LossFunctions.jl do not support <code>missing</code> values. To use with <code>missing</code> values, replace <code>(ŷ, y)</code> with <code>skipinvalid(ŷ, y))</code>.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Union{Finite{2},Missing}}</code> (binary classification); <code>ŷ</code> must be an array of <code>probabilistic</code> predictions. </p><p>For more information, run <code>info(PerceptronLoss)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/measures/meta_utilities.jl#LL83-L102">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.PeriodicLoss" href="#MLJBase.PeriodicLoss"><code>MLJBase.PeriodicLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MLJBase.PeriodicLoss</code></pre><p>A measure type for periodic loss, which includes the instance(s): <code>periodic_loss</code>.</p><pre><code class="nohighlight hljs">PeriodicLoss()(ŷ, y)
PeriodicLoss()(ŷ, y, w)</code></pre><p>Evaluate the default instance of PeriodicLoss on predictions <code>ŷ</code>, given ground truth observations <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p>For more detail, see the original LossFunctions.jl documentation <em>but note differences in the signature.</em></p><p>Losses from LossFunctions.jl do not support <code>missing</code> values. To use with <code>missing</code> values, replace <code>(ŷ, y)</code> with <code>skipinvalid(ŷ, y))</code>.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>Union{AbstractVector{ScientificTypesBase.Continuous}, AbstractVector{ScientificTypesBase.Count}}</code>; <code>ŷ</code> must be an array of <code>deterministic</code> predictions. </p><p>For more information, run <code>info(PeriodicLoss)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/measures/meta_utilities.jl#LL83-L102">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.Precision" href="#MLJBase.Precision"><code>MLJBase.Precision</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MLJBase.Precision</code></pre><p>A measure type for precision (a.k.a. positive predictive value), which includes the instance(s): <code>positive_predictive_value</code>, <code>ppv</code>, <code>positivepredictive_value</code>, <code>precision</code>.</p><pre><code class="nohighlight hljs">Precision()(ŷ, y)</code></pre><p>Evaluate the default instance of Precision on predictions <code>ŷ</code>, given ground truth observations <code>y</code>. </p><p>Assigns <code>false</code> to first element of <code>levels(y)</code>. To reverse roles, use <code>Precision(rev=true)</code>.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Union{OrderedFactor{2},Missing}}</code> (binary classification where choice of &quot;true&quot; effects the measure); <code>ŷ</code> must be an array of <code>deterministic</code> predictions. </p><p>For more information, run <code>info(Precision)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/measures/meta_utilities.jl#LL83-L98">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.QuantileLoss" href="#MLJBase.QuantileLoss"><code>MLJBase.QuantileLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MLJBase.QuantileLoss</code></pre><p>A measure type for quantile loss, which includes the instance(s): <code>quantile_loss</code>.</p><pre><code class="nohighlight hljs">QuantileLoss()(ŷ, y)
QuantileLoss()(ŷ, y, w)</code></pre><p>Evaluate the default instance of QuantileLoss on predictions <code>ŷ</code>, given ground truth observations <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p>For more detail, see the original LossFunctions.jl documentation <em>but note differences in the signature.</em></p><p>Losses from LossFunctions.jl do not support <code>missing</code> values. To use with <code>missing</code> values, replace <code>(ŷ, y)</code> with <code>skipinvalid(ŷ, y))</code>.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>Union{AbstractVector{ScientificTypesBase.Continuous}, AbstractVector{ScientificTypesBase.Count}}</code>; <code>ŷ</code> must be an array of <code>deterministic</code> predictions. </p><p>Constructor signature: <code>QuantileLoss(; τ=0.7)</code></p><p>For more information, run <code>info(QuantileLoss)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/measures/meta_utilities.jl#LL83-L104">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.RSquared" href="#MLJBase.RSquared"><code>MLJBase.RSquared</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MLJBase.RSquared</code></pre><p>A measure type for r squared, which includes the instance(s): <code>rsq</code>, <code>rsquared</code>.</p><pre><code class="nohighlight hljs">RSquared()(ŷ, y)</code></pre><p>Evaluate the r squared on predictions <code>ŷ</code>, given ground truth observations <code>y</code>. </p><p>The R² (also known as R-squared or coefficient of determination) is suitable for interpreting linear regression analysis (Chicco et al., <a href="https://doi.org/10.7717/peerj-cs.623">2021</a>).</p><p>Let <span>$\overline{y}$</span> denote the mean of <span>$y$</span>, then</p><p><span>$\text{R^2} = 1 - \frac{∑ (\hat{y} - y)^2}{∑ \overline{y} - y)^2}.$</span></p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Union{Infinite,Missing}}</code>; <code>ŷ</code> must be an array of <code>deterministic</code> predictions. </p><p>For more information, run <code>info(RSquared)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/measures/meta_utilities.jl#LL83-L102">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.RootMeanSquaredError" href="#MLJBase.RootMeanSquaredError"><code>MLJBase.RootMeanSquaredError</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MLJBase.RootMeanSquaredError</code></pre><p>A measure type for root mean squared error, which includes the instance(s): <code>rms</code>, <code>rmse</code>, <code>root_mean_squared_error</code>.</p><pre><code class="nohighlight hljs">RootMeanSquaredError()(ŷ, y)
RootMeanSquaredError()(ŷ, y, w)</code></pre><p>Evaluate the root mean squared error on predictions <code>ŷ</code>, given ground truth observations <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p><span>$\text{root mean squared error} = \sqrt{n^{-1}∑ᵢ|yᵢ-ŷᵢ|^2}$</span> or <span>$\text{root mean squared error} = \sqrt{\frac{∑ᵢwᵢ|yᵢ-ŷᵢ|^2}{∑ᵢwᵢ}}$</span></p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Union{Infinite,Missing}}</code>; <code>ŷ</code> must be an array of <code>deterministic</code> predictions. </p><p>For more information, run <code>info(RootMeanSquaredError)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/measures/meta_utilities.jl#LL83-L99">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.RootMeanSquaredLogError" href="#MLJBase.RootMeanSquaredLogError"><code>MLJBase.RootMeanSquaredLogError</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MLJBase.RootMeanSquaredLogError</code></pre><p>A measure type for root mean squared log error, which includes the instance(s): <code>rmsl</code>, <code>rmsle</code>, <code>root_mean_squared_log_error</code>.</p><pre><code class="nohighlight hljs">RootMeanSquaredLogError()(ŷ, y)
RootMeanSquaredLogError()(ŷ, y, w)</code></pre><p>Evaluate the root mean squared log error on predictions <code>ŷ</code>, given ground truth observations <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p><span>$\text{root mean squared log error} = \sqrt{n^{-1}∑ᵢ\log\left({yᵢ \over ŷᵢ}\right)^2}$</span></p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Union{Infinite,Missing}}</code>; <code>ŷ</code> must be an array of <code>deterministic</code> predictions. </p><p>See also <a href="@ref"><code>rmslp1</code></a>.</p><p>For more information, run <code>info(RootMeanSquaredLogError)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/measures/meta_utilities.jl#LL83-L101">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.RootMeanSquaredLogProportionalError" href="#MLJBase.RootMeanSquaredLogProportionalError"><code>MLJBase.RootMeanSquaredLogProportionalError</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MLJBase.RootMeanSquaredLogProportionalError</code></pre><p>A measure type for root mean squared log proportional error, which includes the instance(s): <code>rmslp1</code>.</p><pre><code class="nohighlight hljs">RootMeanSquaredLogProportionalError()(ŷ, y)
RootMeanSquaredLogProportionalError()(ŷ, y, w)</code></pre><p>Evaluate the default instance of RootMeanSquaredLogProportionalError on predictions <code>ŷ</code>, given ground truth observations <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p>Constructor signature: <code>RootMeanSquaredLogProportionalError(; offset = 1.0)</code>.</p><p><span>$\text{root mean squared log proportional error} = \sqrt{n^{-1}∑ᵢ\log\left({yᵢ + \text{offset} \over ŷᵢ + \text{offset}}\right)}$</span></p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Union{Infinite,Missing}}</code>; <code>ŷ</code> must be an array of <code>deterministic</code> predictions. </p><p>See also <a href="@ref"><code>rmsl</code></a>. </p><p>For more information, run <code>info(RootMeanSquaredLogProportionalError)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/measures/meta_utilities.jl#LL83-L103">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.RootMeanSquaredProportionalError" href="#MLJBase.RootMeanSquaredProportionalError"><code>MLJBase.RootMeanSquaredProportionalError</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MLJBase.RootMeanSquaredProportionalError</code></pre><p>A measure type for root mean squared proportional error, which includes the instance(s): <code>rmsp</code>.</p><pre><code class="nohighlight hljs">RootMeanSquaredProportionalError()(ŷ, y)
RootMeanSquaredProportionalError()(ŷ, y, w)</code></pre><p>Evaluate the default instance of RootMeanSquaredProportionalError on predictions <code>ŷ</code>, given ground truth observations <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p>Constructor keyword arguments: <code>tol</code> (default = <code>eps()</code>).</p><p><span>$\text{root mean squared proportional error} = \sqrt{m^{-1}∑ᵢ \left({yᵢ-ŷᵢ \over yᵢ}\right)^2}$</span></p><p>where the sum is over indices such that <code>abs(yᵢ) &gt; tol</code> and <code>m</code> is the number of such indices.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Union{Infinite,Missing}}</code>; <code>ŷ</code> must be an array of <code>deterministic</code> predictions. </p><p>For more information, run <code>info(RootMeanSquaredProportionalError)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/measures/meta_utilities.jl#LL83-L105">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.SigmoidLoss" href="#MLJBase.SigmoidLoss"><code>MLJBase.SigmoidLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MLJBase.SigmoidLoss</code></pre><p>A measure type for sigmoid loss, which includes the instance(s): <code>sigmoid_loss</code>.</p><pre><code class="nohighlight hljs">SigmoidLoss()(ŷ, y)
SigmoidLoss()(ŷ, y, w)</code></pre><p>Evaluate the default instance of SigmoidLoss on predictions <code>ŷ</code>, given ground truth observations <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p>For more detail, see the original LossFunctions.jl documentation <em>but note differences in the signature.</em></p><p>Losses from LossFunctions.jl do not support <code>missing</code> values. To use with <code>missing</code> values, replace <code>(ŷ, y)</code> with <code>skipinvalid(ŷ, y))</code>.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Union{Finite{2},Missing}}</code> (binary classification); <code>ŷ</code> must be an array of <code>probabilistic</code> predictions. </p><p>For more information, run <code>info(SigmoidLoss)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/measures/meta_utilities.jl#LL83-L102">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.SmoothedL1HingeLoss" href="#MLJBase.SmoothedL1HingeLoss"><code>MLJBase.SmoothedL1HingeLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MLJBase.SmoothedL1HingeLoss</code></pre><p>A measure type for smoothed l1 hinge loss, which includes the instance(s): <code>smoothed_l1_hinge_loss</code>.</p><pre><code class="nohighlight hljs">SmoothedL1HingeLoss()(ŷ, y)
SmoothedL1HingeLoss()(ŷ, y, w)</code></pre><p>Evaluate the default instance of SmoothedL1HingeLoss on predictions <code>ŷ</code>, given ground truth observations <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p>For more detail, see the original LossFunctions.jl documentation <em>but note differences in the signature.</em></p><p>Losses from LossFunctions.jl do not support <code>missing</code> values. To use with <code>missing</code> values, replace <code>(ŷ, y)</code> with <code>skipinvalid(ŷ, y))</code>.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Union{Finite{2},Missing}}</code> (binary classification); <code>ŷ</code> must be an array of <code>probabilistic</code> predictions. </p><p>Constructor signature: <code>SmoothedL1HingeLoss(; gamma=1.0)</code></p><p>For more information, run <code>info(SmoothedL1HingeLoss)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/measures/meta_utilities.jl#LL83-L104">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.SphericalScore" href="#MLJBase.SphericalScore"><code>MLJBase.SphericalScore</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MLJBase.SphericalScore</code></pre><p>A measure type for Spherical score, which includes the instance(s): <code>spherical_score</code>.</p><pre><code class="nohighlight hljs">SphericalScore()(ŷ, y)
SphericalScore()(ŷ, y, w)</code></pre><p>Evaluate the default instance of SphericalScore on predictions <code>ŷ</code>, given ground truth observations <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p>Convention as in <a href="https://doi.org/10.1198/016214506000001437">Gneiting and Raftery (2007), &quot;StrictlyProper Scoring Rules, Prediction, and Estimation&quot;</a>: If <code>η</code> takes on a finite number of classes <code>C</code> and `<code>p(η)</code> is the predicted probability for a <em>single</em> observation <code>η</code>, then the corresponding score for that observation is given by</p><p><span>$p(y)^α / \left(\sum_{η ∈ C} p(η)^α\right)^{1-α} - 1$</span></p><p>where <code>α</code> is the measure parameter <code>alpha</code>.</p><p>In the case the predictions <code>ŷ</code> are continuous probability distributions, such as <code>Distributions.Normal</code>, replace the above sum with an integral, and interpret <code>p</code> as the probablity density function. In case of discrete distributions over the integers, such as <code>Distributions.Poisson</code>, sum over all integers instead of <code>C</code>.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbtractArray{&lt;:Union{Missing,T}</code> where <code>T</code> is <code>Continuous</code> or <code>Count</code> (for respectively continuous or discrete Distribution.jl objects in <code>ŷ</code>) or  <code>OrderedFactor</code> or <code>Multiclass</code> (for <code>UnivariateFinite</code> distributions in <code>ŷ</code>); <code>ŷ</code> must be an array of <code>probabilistic</code> predictions. </p><p>For more information, run <code>info(SphericalScore)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/measures/meta_utilities.jl#LL83-L113">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.TrueNegative" href="#MLJBase.TrueNegative"><code>MLJBase.TrueNegative</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MLJBase.TrueNegative</code></pre><p>A measure type for number of true negatives, which includes the instance(s): <code>true_negative</code>, <code>truenegative</code>.</p><pre><code class="nohighlight hljs">TrueNegative()(ŷ, y)</code></pre><p>Evaluate the default instance of TrueNegative on predictions <code>ŷ</code>, given ground truth observations <code>y</code>. </p><p>Assigns <code>false</code> to first element of <code>levels(y)</code>. To reverse roles, use <code>TrueNegative(rev=true)</code>.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Union{OrderedFactor{2},Missing}}</code> (binary classification where choice of &quot;true&quot; effects the measure); <code>ŷ</code> must be an array of <code>deterministic</code> predictions. </p><p>For more information, run <code>info(TrueNegative)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/measures/meta_utilities.jl#LL83-L98">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.TrueNegativeRate" href="#MLJBase.TrueNegativeRate"><code>MLJBase.TrueNegativeRate</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MLJBase.TrueNegativeRate</code></pre><p>A measure type for true negative rate, which includes the instance(s): <code>true_negative_rate</code>, <code>truenegative_rate</code>, <code>tnr</code>, <code>specificity</code>, <code>selectivity</code>.</p><pre><code class="nohighlight hljs">TrueNegativeRate()(ŷ, y)</code></pre><p>Evaluate the default instance of TrueNegativeRate on predictions <code>ŷ</code>, given ground truth observations <code>y</code>. </p><p>Assigns <code>false</code> to first element of <code>levels(y)</code>. To reverse roles, use <code>TrueNegativeRate(rev=true)</code>.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Union{OrderedFactor{2},Missing}}</code> (binary classification where choice of &quot;true&quot; effects the measure); <code>ŷ</code> must be an array of <code>deterministic</code> predictions. </p><p>For more information, run <code>info(TrueNegativeRate)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/measures/meta_utilities.jl#LL83-L98">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.TruePositive" href="#MLJBase.TruePositive"><code>MLJBase.TruePositive</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MLJBase.TruePositive</code></pre><p>A measure type for number of true positives, which includes the instance(s): <code>true_positive</code>, <code>truepositive</code>.</p><pre><code class="nohighlight hljs">TruePositive()(ŷ, y)</code></pre><p>Evaluate the default instance of TruePositive on predictions <code>ŷ</code>, given ground truth observations <code>y</code>. </p><p>Assigns <code>false</code> to first element of <code>levels(y)</code>. To reverse roles, use <code>TruePositive(rev=true)</code>.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Union{OrderedFactor{2},Missing}}</code> (binary classification where choice of &quot;true&quot; effects the measure); <code>ŷ</code> must be an array of <code>deterministic</code> predictions. </p><p>For more information, run <code>info(TruePositive)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/measures/meta_utilities.jl#LL83-L98">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.TruePositiveRate" href="#MLJBase.TruePositiveRate"><code>MLJBase.TruePositiveRate</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MLJBase.TruePositiveRate</code></pre><p>A measure type for true positive rate (a.k.a recall), which includes the instance(s): <code>true_positive_rate</code>, <code>truepositive_rate</code>, <code>tpr</code>, <code>sensitivity</code>, <code>recall</code>, <code>hit_rate</code>.</p><pre><code class="nohighlight hljs">TruePositiveRate()(ŷ, y)</code></pre><p>Evaluate the default instance of TruePositiveRate on predictions <code>ŷ</code>, given ground truth observations <code>y</code>. </p><p>Assigns <code>false</code> to first element of <code>levels(y)</code>. To reverse roles, use <code>TruePositiveRate(rev=true)</code>.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Union{OrderedFactor{2},Missing}}</code> (binary classification where choice of &quot;true&quot; effects the measure); <code>ŷ</code> must be an array of <code>deterministic</code> predictions. </p><p>For more information, run <code>info(TruePositiveRate)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/measures/meta_utilities.jl#LL83-L98">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.ZeroOneLoss" href="#MLJBase.ZeroOneLoss"><code>MLJBase.ZeroOneLoss</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">MLJBase.ZeroOneLoss</code></pre><p>A measure type for zero one loss, which includes the instance(s): <code>zero_one_loss</code>.</p><pre><code class="nohighlight hljs">ZeroOneLoss()(ŷ, y)
ZeroOneLoss()(ŷ, y, w)</code></pre><p>Evaluate the default instance of ZeroOneLoss on predictions <code>ŷ</code>, given ground truth observations <code>y</code>. Optionally specify per-sample weights, <code>w</code>. </p><p>For more detail, see the original LossFunctions.jl documentation <em>but note differences in the signature.</em></p><p>Losses from LossFunctions.jl do not support <code>missing</code> values. To use with <code>missing</code> values, replace <code>(ŷ, y)</code> with <code>skipinvalid(ŷ, y))</code>.</p><p>Requires <code>scitype(y)</code> to be a subtype of <code>AbstractArray{&lt;:Union{Finite{2},Missing}}</code> (binary classification); <code>ŷ</code> must be an array of <code>probabilistic</code> predictions. </p><p>For more information, run <code>info(ZeroOneLoss)</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/measures/meta_utilities.jl#LL83-L102">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase._permute_rows-Tuple{AbstractVecOrMat, Vector{Int64}}" href="#MLJBase._permute_rows-Tuple{AbstractVecOrMat, Vector{Int64}}"><code>MLJBase._permute_rows</code></a> — <span class="docstring-category">Method</span></header><section><div><p><em>permute</em>rows(obj, perm)</p><p>Internal function to return a vector or matrix with permuted rows given the permutation <code>perm</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/utilities.jl#LL166-L171">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.available_name-Tuple{Any, Any}" href="#MLJBase.available_name-Tuple{Any, Any}"><code>MLJBase.available_name</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">available_name(modl::Module, name::Symbol)</code></pre><p>Function to replace, if necessary, a given <code>name</code> with a modified one that ensures it is not the name of any existing object in the global scope of <code>modl</code>. Modifications are created with numerical suffixes.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/utilities.jl#LL394-L401">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.check_dimensions-Tuple{Any, Any}" href="#MLJBase.check_dimensions-Tuple{Any, Any}"><code>MLJBase.check_dimensions</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">check_dimensions(X, Y)</code></pre><p>Internal function to check two arrays have the same shape.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/utilities.jl#LL138-L143">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.check_same_nrows-Tuple{Any, Any}" href="#MLJBase.check_same_nrows-Tuple{Any, Any}"><code>MLJBase.check_same_nrows</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">check_same_nrows(X, Y)</code></pre><p>Internal function to check two objects, each a vector or a matrix, have the same number of rows.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/utilities.jl#LL152-L158">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.chunks-Tuple{AbstractRange, Integer}" href="#MLJBase.chunks-Tuple{AbstractRange, Integer}"><code>MLJBase.chunks</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">chunks(range, n)</code></pre><p>Split an <code>AbstractRange</code>  into <code>n</code> subranges of approximately equal length.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">julia&gt; collect(chunks(1:5, 2))
2-element Array{UnitRange{Int64},1}:
 1:3
 4:5

**Private method**
</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/utilities.jl#LL351-L366">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.flat_values-Tuple{NamedTuple}" href="#MLJBase.flat_values-Tuple{NamedTuple}"><code>MLJBase.flat_values</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">flat_values(t::NamedTuple)</code></pre><p>View a nested named tuple <code>t</code> as a tree and return, as a tuple, the values at the leaves, in the order they appear in the original tuple.</p><pre><code class="language-julia-repl hljs">julia&gt; t = (X = (x = 1, y = 2), Y = 3)
julia&gt; flat_values(t)
(1, 2, 3)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/utilities.jl#LL10-L22">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.generate_name!-Tuple{DataType, Any}" href="#MLJBase.generate_name!-Tuple{DataType, Any}"><code>MLJBase.generate_name!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">generate_name!(M, existing_names; only=Union{Function,Type}, substitute=:f)</code></pre><p>Given a type <code>M</code> (e.g., <code>MyEvenInteger{N}</code>) return a symbolic, snake-case, representation of the type name (such as <code>my_even_integer</code>). The symbol is pushed to <code>existing_names</code>, which must be an <code>AbstractVector</code> to which a <code>Symbol</code> can be pushed.</p><p>If the snake-case representation already exists in <code>existing_names</code> a suitable integer is appended to the name.</p><p>If <code>only</code> is specified, then the operation is restricted to those <code>M</code> for which <code>M isa only</code>. In all other cases the symbolic name is generated using <code>substitute</code> as the base symbol.</p><pre><code class="nohighlight hljs">existing_names = []
julia&gt; generate_name!(Vector{Int}, existing_names)
:vector

julia&gt; generate_name!(Vector{Int}, existing_names)
:vector2

julia&gt; generate_name!(AbstractFloat, existing_names)
:abstract_float

julia&gt; generate_name!(Int, existing_names, only=Array, substitute=:not_array)
:not_array

julia&gt; generate_name!(Int, existing_names, only=Array, substitute=:not_array)
:not_array2</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/utilities.jl#LL412-L445">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.init_rng-Tuple{Any}" href="#MLJBase.init_rng-Tuple{Any}"><code>MLJBase.init_rng</code></a> — <span class="docstring-category">Method</span></header><section><div><p>init_rng(rng)</p><p>Create an <code>AbstractRNG</code> from <code>rng</code>. If <code>rng</code> is a non-negative <code>Integer</code>, it returns a <code>MersenneTwister</code> random number generator seeded with <code>rng</code>; If <code>rng</code> is an <code>AbstractRNG</code> object it returns <code>rng</code>, otherwise it throws an error.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/utilities.jl#LL198-L204">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.measures_for_export-Tuple{}" href="#MLJBase.measures_for_export-Tuple{}"><code>MLJBase.measures_for_export</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">measures_for_export()</code></pre><p>Return a list of the symbolic representation of all:</p><ul><li><p>measure types (subtypes of <code>Aggregated</code> and <code>Unaggregated</code>) measure</p></li><li><p>type aliases (as defined by the constant <code>MLJBase.MEASURE_TYPE_ALIASES</code>)</p></li><li><p>all built-in measure instances (as declared by <code>instances</code> trait)</p></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/measures/meta_utilities.jl#LL208-L221">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.metadata_measure-Tuple{Any}" href="#MLJBase.metadata_measure-Tuple{Any}"><code>MLJBase.metadata_measure</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">metadata_measure(T; kw...)</code></pre><p>Helper function to write the metadata (trait definitions) for a single measure.</p><p><strong>Compulsory keyword arguments</strong></p><ul><li><p><code>target_scitype</code>: The allowed scientific type of <code>y</code> in <code>measure(ŷ, y, ...)</code>. This is typically some abstract array. E.g, in single target variable regression this is typically <code>AbstractArray{&lt;:Union{Missing,Continuous}}</code>. For a binary classification metric insensitive to class order, this would typically be <code>Union{AbstractArray{&lt;:Union{Missing,Multiclass{2}}}, AbstractArray{&lt;:Union{Missing,OrderedFactor{2}}}}</code>, which has the alias <code>FiniteArrMissing</code>.</p></li><li><p><code>orientation</code>: Orientation of the measure.  Use <code>:loss</code> when lower is   better and <code>:score</code> when higher is better.  For example, set   <code>:loss</code> for root mean square and <code>:score</code> for area under the ROC   curve.</p></li><li><p><code>prediction_type</code>: Refers to <code>ŷ</code> in <code>measure(ŷ, y, ...)</code> and should be one of: <code>:deterministic</code> (<code>ŷ</code> has same type as <code>y</code>), <code>:probabilistic</code> or <code>:interval</code>.</p></li></ul><p><strong>Optional keyword arguments</strong></p><p>The following have meaningful defaults but may still require overloading:</p><ul><li><p><code>instances</code>: A vector of strings naming the built-in instances of the measurement type provided by the implementation, which are usually just common aliases for the default instance. E.g., for <code>RSquared</code> has the <code>instances = [&quot;rsq&quot;, &quot;rsquared&quot;]</code> which are both defined as <code>RSquared()</code> in the implementation. <code>MulticlassFScore</code> has the <code>instances = [&quot;macro_f1score&quot;, &quot;micro_f1score&quot;, &quot;multiclass_f1score&quot;]</code>, where <code>micro_f1score = MulticlassFScore(average=micro_avg)</code>, etc.  Default is <code>String[]</code>.</p></li><li><p><code>aggregation</code>: Aggregation method for measurements, typically       <code>Mean()</code> (for, e.g., mean absolute error) or <code>Sum()</code> (for number   of true positives). Default is <code>Mean()</code>. Must subtype   <code>StatisticalTraits.AggregationMode</code>. It is used to:</p><ul><li><p>aggregate measurements in resampling (e.g., cross-validation)</p></li><li><p>aggregating per-observation measurements returned by <code>single</code> in the fallback definition of <code>call</code> for <code>Unaggregated</code> measures</p></li></ul><p>(such as area under the ROC curve).</p></li><li><p><code>supports_weights</code>: Whether the measure can be called with per-observation weights <code>w</code>, as in <code>l2(ŷ, y, w)</code>. Default is <code>true</code>.</p></li><li><p><code>supports_class_weights</code>: Whether the measure can be called with a class weight dictionary <code>w</code>, as in <code>micro_f1score(ŷ, y, w)</code>. Default is <code>true</code>. Default is <code>false</code>.</p></li><li><p><code>human_name</code>: Ordinary name of measure. Used in the full auto-generated docstring, which begins &quot;A measure type for human_name ...&quot;. Eg, the <code>human_name</code> for <code>TruePositive</code> is <code>number of true positives. Default is snake-case version of type name, with underscores replaced by spaces; so</code>MeanAbsoluteError` becomes &quot;mean absolute error&quot;.</p></li><li><p><code>docstring</code>: An abbreviated docstring, displayed by <code>info(measure)</code>. Fallback uses <code>human_name</code> and lists the <code>instances</code>.</p></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/measures/meta_utilities.jl#LL90-L160">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.prepend-Tuple{Symbol, Nothing}" href="#MLJBase.prepend-Tuple{Symbol, Nothing}"><code>MLJBase.prepend</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">MLJBase.prepend(::Symbol, ::Union{Symbol,Expr,Nothing})</code></pre><p>For prepending symbols in expressions like <code>:(y.w)</code> and <code>:(x1.x2.x3)</code>.</p><p>julia&gt; prepend(:x, :y) :(x.y)</p><p>julia&gt; prepend(:x, :(y.z)) :(x.y.z)</p><p>julia&gt; prepend(:w, ans) :(w.x.y.z)</p><p>If the second argument is <code>nothing</code>, then <code>nothing</code> is returned.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/utilities.jl#LL49-L65">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.recursive_getproperty-Tuple{Any, Symbol}" href="#MLJBase.recursive_getproperty-Tuple{Any, Symbol}"><code>MLJBase.recursive_getproperty</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">recursive_getproperty(object, nested_name::Expr)</code></pre><p>Call getproperty recursively on <code>object</code> to extract the value of some nested property, as in the following example:</p><pre><code class="nohighlight hljs">julia&gt; object = (X = (x = 1, y = 2), Y = 3)
julia&gt; recursive_getproperty(object, :(X.y))
2</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/utilities.jl#LL71-L81">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.recursive_setproperty!-Tuple{Any, Symbol, Any}" href="#MLJBase.recursive_setproperty!-Tuple{Any, Symbol, Any}"><code>MLJBase.recursive_setproperty!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">recursively_setproperty!(object, nested_name::Expr, value)</code></pre><p>Set a nested property of an <code>object</code> to <code>value</code>, as in the following example:</p><pre><code class="nohighlight hljs">julia&gt; mutable struct Foo
           X
           Y
       end

julia&gt; mutable struct Bar
           x
           y
       end

julia&gt; object = Foo(Bar(1, 2), 3)
Foo(Bar(1, 2), 3)

julia&gt; recursively_setproperty!(object, :(X.y), 42)
42

julia&gt; object
Foo(Bar(1, 42), 3)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/utilities.jl#LL103-L129">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.sequence_string-Union{Tuple{Itr}, Tuple{Itr, Any}} where Itr" href="#MLJBase.sequence_string-Union{Tuple{Itr}, Tuple{Itr, Any}} where Itr"><code>MLJBase.sequence_string</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">sequence_string(itr, n=3)</code></pre><p>Return a &quot;sequence&quot; string from the first <code>n</code> elements generated by <code>itr</code>.</p><pre><code class="nohighlight hljs">julia&gt; MLJBase.sequence_string(1:10, 4)
&quot;1, 2, 3, 4, ...&quot;</code></pre><p><strong>Private method.</strong></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/utilities.jl#LL260-L271">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.shuffle_rows-Tuple{AbstractVecOrMat, AbstractVecOrMat}" href="#MLJBase.shuffle_rows-Tuple{AbstractVecOrMat, AbstractVecOrMat}"><code>MLJBase.shuffle_rows</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">shuffle_rows(X::AbstractVecOrMat,
             Y::AbstractVecOrMat;
             rng::AbstractRNG=Random.GLOBAL_RNG)</code></pre><p>Return row-shuffled vectors or matrices using a random permutation of <code>X</code> and <code>Y</code>. An optional random number generator can be specified using the <code>rng</code> argument.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/utilities.jl#LL178-L187">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MLJBase.unwind-Tuple" href="#MLJBase.unwind-Tuple"><code>MLJBase.unwind</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">unwind(iterators...)</code></pre><p>Represent all possible combinations of values generated by <code>iterators</code> as rows of a matrix <code>A</code>. In more detail, <code>A</code> has one column for each iterator in <code>iterators</code> and one row for each distinct possible combination of values taken on by the iterators. Elements in the first column cycle fastest, those in the last clolumn slowest.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">julia&gt; iterators = ([1, 2], [&quot;a&quot;,&quot;b&quot;], [&quot;x&quot;, &quot;y&quot;, &quot;z&quot;]);
julia&gt; MLJTuning.unwind(iterators...)
12×3 Array{Any,2}:
 1  &quot;a&quot;  &quot;x&quot;
 2  &quot;a&quot;  &quot;x&quot;
 1  &quot;b&quot;  &quot;x&quot;
 2  &quot;b&quot;  &quot;x&quot;
 1  &quot;a&quot;  &quot;y&quot;
 2  &quot;a&quot;  &quot;y&quot;
 1  &quot;b&quot;  &quot;y&quot;
 2  &quot;b&quot;  &quot;y&quot;
 1  &quot;a&quot;  &quot;z&quot;
 2  &quot;a&quot;  &quot;z&quot;
 1  &quot;b&quot;  &quot;z&quot;
 2  &quot;b&quot;  &quot;z&quot;</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaAI/MLJBase.jl/blob/e151fe1d1d8ef14cb73324e2c18383bc3d68813c/src/utilities.jl#LL299-L328">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../distributions/">« Distributions</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.24 on <span class="colophon-date" title="Wednesday 28 June 2023 22:00">Wednesday 28 June 2023</span>. Using Julia version 1.9.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
