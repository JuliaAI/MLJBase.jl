var documenterSearchIndex = {"docs":
[{"location":"openml/#OpenML-1","page":"OpenML","title":"OpenML","text":"","category":"section"},{"location":"openml/#","page":"OpenML","title":"OpenML","text":"Modules = [MLJBase, OpenML]\nPages   = [\"openml.jl\"]","category":"page"},{"location":"openml/#MLJBase.OpenML.convert_ARFF_to_rowtable-Tuple{Any}","page":"OpenML","title":"MLJBase.OpenML.convert_ARFF_to_rowtable","text":"Returns a Vector of NamedTuples. Receives an HTTP.Message.response that has an ARFF file format in the body of the Message.\n\n\n\n\n\n","category":"method"},{"location":"openml/#MLJBase.OpenML.load-Tuple{Int64}","page":"OpenML","title":"MLJBase.OpenML.load","text":"OpenML.load(id)\n\nLoad the OpenML dataset with specified id, from those listed on the OpenML site.\n\nReturns a \"row table\", i.e., a Vector of identically typed NamedTuples. A row table is compatible with the Tables.jl interface and can therefore be readily converted to other compatible formats. For example:\n\nusing DataFrames\nrowtable = OpenML.load(61);\ndf = DataFrame(rowtable);\ndf2 = coerce(df, :class=>Multiclass)\n\n\n\n\n\n","category":"method"},{"location":"openml/#MLJBase.OpenML.load_Data_Features-Tuple{Int64}","page":"OpenML","title":"MLJBase.OpenML.load_Data_Features","text":"Returns a list of all data qualities in the system.\n\n271 - Unknown dataset. Data set with the given data ID was not found (or is not shared with you).\n272 - No features found. The dataset did not contain any features, or we could not extract them.\n273 - Dataset not processed yet. The dataset was not processed yet, features are not yet available. Please wait for a few minutes.\n274 - Dataset processed with error. The feature extractor has run into an error while processing the dataset. Please check whether it is a valid supported file. If so, please contact the API admins.\n\n\n\n\n\n","category":"method"},{"location":"openml/#MLJBase.OpenML.load_Data_Qualities-Tuple{Int64}","page":"OpenML","title":"MLJBase.OpenML.load_Data_Qualities","text":"Returns the qualities of a dataset.\n\n360 - Please provide data set ID\n361 - Unknown dataset. The data set with the given ID was not found in the database, or is not shared with you.\n362 - No qualities found. The registered dataset did not contain any calculated qualities.\n363 - Dataset not processed yet. The dataset was not processed yet, no qualities are available. Please wait for a few minutes.\n364 - Dataset processed with error. The quality calculator has run into an error while processing the dataset. Please check whether it is a valid supported file. If so, contact the support team.\n365 - Interval start or end illegal. There was a problem with the interval start or end.\n\n\n\n\n\n","category":"method"},{"location":"openml/#MLJBase.OpenML.load_Data_Qualities_List-Tuple{}","page":"OpenML","title":"MLJBase.OpenML.load_Data_Qualities_List","text":"Returns a list of all data qualities in the system.\n\n412 - Precondition failed. An error code and message are returned\n370 - No data qualities available. There are no data qualities in the system.\n\n\n\n\n\n","category":"method"},{"location":"openml/#MLJBase.OpenML.load_Dataset_Description-Tuple{Int64}","page":"OpenML","title":"MLJBase.OpenML.load_Dataset_Description","text":"Returns information about a dataset. The information includes the name, information about the creator, URL to download it and more.\n\n110 - Please provide data_id.\n111 - Unknown dataset. Data set description with data_id was not found in the database.\n112 - No access granted. This dataset is not shared with you.\n\n\n\n\n\n","category":"method"},{"location":"openml/#MLJBase.OpenML.load_List_And_Filter-Tuple{String}","page":"OpenML","title":"MLJBase.OpenML.load_List_And_Filter","text":"List datasets, possibly filtered by a range of properties. Any number of properties can be combined by listing them one after the other in the form '/data/list/{filter}/{value}/{filter}/{value}/...' Returns an array with all datasets that match the constraints.\n\nAny combination of these filters /limit/{limit}/offset/{offset} - returns only {limit} results starting from result number {offset}. Useful for paginating results. With /limit/5/offset/10,     results 11..15 will be returned.\n\nBoth limit and offset need to be specified. /status/{status} - returns only datasets with a given status, either 'active', 'deactivated', or 'inpreparation'. /tag/{tag} - returns only datasets tagged with the given tag. /{dataquality}/{range} - returns only tasks for which the underlying datasets have certain qualities. {dataquality} can be dataid, dataname, dataversion, numberinstances, numberfeatures, numberclasses, numbermissingvalues. {range} can be a specific value or a range in the form 'low..high'. Multiple qualities can be combined, as in 'numberinstances/0..50/number_features/0..10'.\n\n370 - Illegal filter specified.\n371 - Filter values/ranges not properly specified.\n372 - No results. There where no matches for the given constraints.\n373 - Can not specify an offset without a limit.\n\n\n\n\n\n","category":"method"},{"location":"measures/#Measures-1","page":"Measures","title":"Measures","text":"","category":"section"},{"location":"measures/#Helper-functions-1","page":"Measures","title":"Helper functions","text":"","category":"section"},{"location":"measures/#","page":"Measures","title":"Measures","text":"Modules = [MLJBase]\nPages   = [\"measures/registry.jl\", \"measures/measures.jl\"]","category":"page"},{"location":"measures/#Continuous-loss-functions-1","page":"Measures","title":"Continuous loss functions","text":"","category":"section"},{"location":"measures/#","page":"Measures","title":"Measures","text":"Modules = [MLJBase]\nPages   = [\"measures/continuous.jl\"]","category":"page"},{"location":"measures/#Confusion-matrix-1","page":"Measures","title":"Confusion matrix","text":"","category":"section"},{"location":"measures/#","page":"Measures","title":"Measures","text":"Modules = [MLJBase]\nPages   = [\"measures/confusion_matrix.jl\"]","category":"page"},{"location":"measures/#MLJBase.ConfusionMatrixObject","page":"Measures","title":"MLJBase.ConfusionMatrixObject","text":"ConfusionMatrixObject{C}\n\nConfusion matrix with C ≥ 2 classes. Rows correspond to predicted values and columns to the ground truth.\n\n\n\n\n\n","category":"type"},{"location":"measures/#MLJBase.ConfusionMatrixObject-Tuple{Array{Int64,2},Array{String,1}}","page":"Measures","title":"MLJBase.ConfusionMatrixObject","text":"ConfusionMatrixObject(m, labels)\n\nInstantiates a confusion matrix out of a square integer matrix m. Rows are the predicted class, columns the ground truth. See also the wikipedia article.\n\n\n\n\n\n","category":"method"},{"location":"measures/#MLJBase._confmat-Tuple{AbstractArray{var\"#s568\",1} where var\"#s568\"<:CategoricalArrays.CategoricalValue,AbstractArray{var\"#s567\",1} where var\"#s567\"<:CategoricalArrays.CategoricalValue}","page":"Measures","title":"MLJBase._confmat","text":"_confmat(ŷ, y; rev=false)\n\nA private method. General users should use confmat or other instance of the measure type ConfusionMatrix.\n\nComputes the confusion matrix given a predicted ŷ with categorical elements and the actual y. Rows are the predicted class, columns the ground truth. The ordering follows that of levels(y).\n\nKeywords\n\nrev=false: in the binary case, this keyword allows to swap the ordering of              classes.\nperm=[]:   in the general case, this keyword allows to specify a permutation              re-ordering the classes.\nwarn=true: whether to show a warning in case y does not have scientific              type OrderedFactor{2} (see note below).\n\nNote\n\nTo decrease the risk of unexpected errors, if y does not have scientific type OrderedFactor{2} (and so does not have a \"natural ordering\" negative-positive), a warning is shown indicating the current order unless the user explicitly specifies either rev or perm in which case it's assumed the user is aware of the class ordering.\n\nThe confusion_matrix is a measure (although neither a score nor a loss) and so may be specified as such in calls to evaluate, evaluate!, although not in TunedModels.  In this case, however, there no way to specify an ordering different from levels(y), where y is the target.\n\n\n\n\n\n","category":"method"},{"location":"measures/#Finite-loss-functions-1","page":"Measures","title":"Finite loss functions","text":"","category":"section"},{"location":"measures/#","page":"Measures","title":"Measures","text":"Modules = [MLJBase]\nPages   = [\"measures/finite.jl\"]","category":"page"},{"location":"measures/#MLJBase.MulticlassFScore","page":"Measures","title":"MLJBase.MulticlassFScore","text":"MulticlassFScore(; β=1.0, average=macro_avg, return_type=LittleDict)\n\nOne-parameter generalization, F_β, of the F-measure or balanced F-score for multiclass observations.\n\nMulticlassFScore()(ŷ, y)\nMulticlassFScore()(ŷ, y, class_w)\n\nEvaluate the default score on multiclass observations, ŷ, given ground truth values, y. Options for average are: no_avg, macro_avg (default) and micro_avg. Options for return_type, applying in the no_avg case, are: LittleDict (default) or Vector.  An optional AbstractDict, denoted class_w above, keyed on levels(y), specifies class weights. It applies if average=macro_avg or average=no_avg.\n\nFor more information, run info(MulticlassFScore).\n\n\n\n\n\n","category":"type"},{"location":"measures/#MLJBase.MulticlassFalseDiscoveryRate","page":"Measures","title":"MLJBase.MulticlassFalseDiscoveryRate","text":"MulticlassFalseDiscoveryRate(; average=macro_avg, return_type=LittleDict)\n\nmulticlass false discovery rate; aliases: multiclass_false_discovery_rate, multiclass_falsediscovery_rate, multiclass_fdr.\n\nMulticlassFalseDiscoveryRate()(ŷ, y)\nMulticlassFalseDiscoveryRate()(ŷ, y, class_w)\n\nFalse discovery rate for multiclass observations ŷ and ground truth y, using default averaging and return type.  Options for average are: no_avg, macro_avg (default) and micro_avg. Options for return_type, applying in the no_avg case, are: LittleDict (default) or Vector.  An optional AbstractDict, denoted class_w above, keyed on levels(y), specifies class weights. It applies if average=macro_avg or average=no_avg.\n\nFor more information, run info(MulticlassFalseDiscoveryRate).\n\n\n\n\n\n","category":"type"},{"location":"measures/#MLJBase.MulticlassFalseNegativeRate","page":"Measures","title":"MLJBase.MulticlassFalseNegativeRate","text":"MulticlassFalseNegativeRate(; average=macro_avg, return_type=LittleDict)\n\nmulticlass false negative rate; aliases: multiclass_false_negative_rate, multiclass_fnr, multiclass_miss_rate, multiclass_falsenegative_rate.\n\nMulticlassFalseNegativeRate()(ŷ, y)\nMulticlassFalseNegativeRate()(ŷ, y, class_w)\n\nFalse negative rate for multiclass observations ŷ and ground truth y, using default averaging and return type.  Options for average are: no_avg, macro_avg (default) and micro_avg. Options for return_type, applying in the no_avg case, are: LittleDict (default) or Vector.  An optional AbstractDict, denoted class_w above, keyed on levels(y), specifies class weights. It applies if average=macro_avg or average=no_avg.\n\nFor more information, run info(MulticlassFalseNegativeRate).\n\n\n\n\n\n","category":"type"},{"location":"measures/#MLJBase.MulticlassFalsePositiveRate","page":"Measures","title":"MLJBase.MulticlassFalsePositiveRate","text":"MulticlassFalsePositiveRate(; average=macro_avg, return_type=LittleDict)\n\nmulticlass false positive rate; aliases: multiclass_false_positive_rate, multiclass_fpr multiclass_fallout, multiclass_falsepositive_rate.\n\nMulticlassFalsePositiveRate()(ŷ, y)\nMulticlassFalsePositiveRate()(ŷ, y, class_w)\n\nFalse positive rate for multiclass observations ŷ and ground truth y, using default averaging and return type.  Options for average are: no_avg, macro_avg (default) and micro_avg. Options for return_type, applying in the no_avg case, are: LittleDict (default) or Vector.  An optional AbstractDict, denoted class_w above, keyed on levels(y), specifies class weights. It applies if average=macro_avg or average=no_avg.\n\nFor more information, run info(MulticlassFalsePositiveRate).\n\n\n\n\n\n","category":"type"},{"location":"measures/#MLJBase.MulticlassNegativePredictiveValue","page":"Measures","title":"MLJBase.MulticlassNegativePredictiveValue","text":"MulticlassNegativePredictiveValue(; average=macro_avg, return_type=LittleDict)\n\nmulticlass negative predictive value; aliases: multiclass_negative_predictive_value, multiclass_negativepredictive_value, multiclass_npv.\n\nMulticlassNegativePredictiveValue()(ŷ, y)\nMulticlassNegativePredictiveValue()(ŷ, y, class_w)\n\nNegative predictive value for multiclass observations ŷ and ground truth y, using default averaging and return type. Options for average are: no_avg, macro_avg (default) and micro_avg. Options for return_type, applying in the no_avg case, are: LittleDict (default) or Vector.  An optional AbstractDict, denoted class_w above, keyed on levels(y), specifies class weights. It applies if average=macro_avg or average=no_avg.\n\nFor more information, run info(MulticlassNegativePredictiveValue).\n\n\n\n\n\n","category":"type"},{"location":"measures/#MLJBase.MulticlassPrecision","page":"Measures","title":"MLJBase.MulticlassPrecision","text":"MulticlassPrecision(; average=macro_avg, return_type=LittleDict)\n\nmulticlass positive predictive value (aka precision); aliases: multiclass_positive_predictive_value, multiclass_ppv, multiclass_positivepredictive_value, multiclass_recall.\n\nMulticlassPrecision()(ŷ, y)\nMulticlassPrecision()(ŷ, y, class_w)\n\nPrecision for multiclass observations ŷ and ground truth y, using default averaging and return type. Options for average are: no_avg, macro_avg (default) and micro_avg. Options for return_type, applying in the no_avg case, are: LittleDict (default) or Vector.  An optional AbstractDict, denoted class_w above, keyed on levels(y), specifies class weights. It applies if average=macro_avg or average=no_avg.\n\nFor more information, run info(MulticlassPrecision).\n\n\n\n\n\n","category":"type"},{"location":"measures/#MLJBase.MulticlassTrueNegative","page":"Measures","title":"MLJBase.MulticlassTrueNegative","text":"MulticlassTrueNegative(; return_type=LittleDict)\n\nNumber of true negatives; aliases: multiclass_true_negative, multiclass_truenegative.\n\nMulticlassTrueNegative()(ŷ, y)\n\nNumber of true negatives for multiclass observations ŷ and ground truth y, using default return type. Options for return_type are: LittleDict(default) or Vector. \n\nFor more information, run info(MulticlassTrueNegative).\n\n\n\n\n\n","category":"type"},{"location":"measures/#MLJBase.MulticlassTrueNegativeRate","page":"Measures","title":"MLJBase.MulticlassTrueNegativeRate","text":"MulticlassTrueNegativeRate(; average=macro_avg, return_type=LittleDict)\n\nmulticlass true negative rate; aliases: multiclass_true_negative_rate, multiclass_tnr  multiclass_specificity, multiclass_selectivity, multiclass_truenegative_rate.\n\nMulticlassTrueNegativeRate()(ŷ, y)\nMulticlassTrueNegativeRate()(ŷ, y, class_w)\n\nTrue negative rate for multiclass observations ŷ and ground truth y, using default averaging and return type. Options for average are: no_avg, macro_avg (default) and micro_avg. Options for return_type, applying in the no_avg case, are: LittleDict (default) or Vector.  An optional AbstractDict, denoted class_w above, keyed on levels(y), specifies class weights. It applies if average=macro_avg or average=no_avg.\n\nFor more information, run info(MulticlassTrueNegativeRate).\n\n\n\n\n\n","category":"type"},{"location":"measures/#MLJBase.MulticlassTruePositive","page":"Measures","title":"MLJBase.MulticlassTruePositive","text":"MulticlassTruePositive(; return_type=LittleDict)\n\nNumber of true positives; aliases: multiclass_true_positive, multiclass_truepositive.\n\nMulticlassTruePositive()(ŷ, y)\n\nNumber of true positives for multiclass observations ŷ and ground truth y, using default return type. Options for return_type are: LittleDict(default) or Vector. \n\nFor more information, run info(MulticlassTruePositive).\n\n\n\n\n\n","category":"type"},{"location":"measures/#MLJBase.MulticlassTruePositiveRate","page":"Measures","title":"MLJBase.MulticlassTruePositiveRate","text":"MulticlassTruePositiveRate(; average=macro_avg, return_type=LittleDict)\n\nmulticlass true positive rate; aliases: multiclass_true_positive_rate, multiclass_tpr, multiclass_sensitivity, multiclass_recall, multiclass_hit_rate, multiclass_truepositive_rate, \n\nMulticlassTruePositiveRate(ŷ, y)\nMulticlassTruePositiveRate(ŷ, y, class_w)\n\nTrue positive rate (a.k.a. sensitivity, recall, hit rate) for multiclass observations ŷ and ground truth y, using default averaging and return type. Options for average are: no_avg, macro_avg (default) and micro_avg. Options for return_type, applying in the no_avg case, are: LittleDict (default) or Vector.  An optional AbstractDict, denoted class_w above, keyed on levels(y), specifies class weights. It applies if average=macro_avg or average=no_avg.\n\nFor more information, run info(MulticlassTruePositiveRate).\n\n\n\n\n\n","category":"type"},{"location":"measures/#MLJBase.roc_curve-Tuple{AbstractArray{var\"#s582\",1} where var\"#s582\"<:UnivariateFinite,AbstractArray{var\"#s581\",1} where var\"#s581\"<:CategoricalArrays.CategoricalValue}","page":"Measures","title":"MLJBase.roc_curve","text":"fprs, tprs, ts = roc_curve(ŷ, y) = roc(ŷ, y)\n\nReturn the ROC curve for a two-class probabilistic prediction ŷ given the ground  truth y. The true positive rates, false positive rates over a range of thresholds ts are returned. Note that if there are k unique scores, there are correspondingly  k thresholds and k+1 \"bins\" over which the FPR and TPR are constant:\n\n[0.0 - thresh[1]]\n[thresh[1] - thresh[2]]\n...\n[thresh[k] - 1]\n\nconsequently, tprs and fprs are of length k+1 if ts is of length k.\n\nTo draw the curve using your favorite plotting backend, do plot(fprs, tprs).\n\n\n\n\n\n","category":"method"},{"location":"measures/#Base.instances-Tuple{Type{var\"#s583\"} where var\"#s583\"<:FalseDiscoveryRate}","page":"Measures","title":"Base.instances","text":".\n\n\n\n\n\n","category":"method"},{"location":"measures/#Base.instances-Tuple{Type{var\"#s583\"} where var\"#s583\"<:FalseNegativeRate}","page":"Measures","title":"Base.instances","text":".\n\n\n\n\n\n","category":"method"},{"location":"measures/#MLJBase.MulticlassNegative","page":"Measures","title":"MLJBase.MulticlassNegative","text":"MulticlassFalseNegative(; return_type=LittleDict)\n\nNumber of false negatives; aliases: multiclass_false_negative, multiclass_falsenegative.\n\nMulticlassFalseNegative()(ŷ, y)\n\nNumber of false negatives for multiclass observations ŷ and ground truth y, using default return type. Options for return_type are: LittleDict(default) or Vector. \n\nFor more information, run info(MulticlassFalseNegative).\n\n\n\n\n\n","category":"function"},{"location":"measures/#MLJBase.MulticlassPositive","page":"Measures","title":"MLJBase.MulticlassPositive","text":"MulticlassFalsePositive(; return_type=LittleDict)\n\nNumber of false positives; aliases: multiclass_false_positive, multiclass_falsepositive.\n\nMulticlassFalsePositive()(ŷ, y)\n\nNumber of false positives for multiclass observations ŷ and ground truth y, using default return type. Options for return_type are: LittleDict(default) or Vector. \n\nFor more information, run info(MulticlassFalsePositive).\n\n\n\n\n\n","category":"function"},{"location":"measures/#MLJBase._idx_unique_sorted-Tuple{AbstractArray{var\"#s583\",1} where var\"#s583\"<:Real}","page":"Measures","title":"MLJBase._idx_unique_sorted","text":"_idx_unique_sorted(v)\n\nInternal function to return the index of unique elements in v under the assumption that the vector v is sorted in decreasing order.\n\n\n\n\n\n","category":"method"},{"location":"distributions/#Distributions-1","page":"Distributions","title":"Distributions","text":"","category":"section"},{"location":"distributions/#Univariate-Finite-Distribution-1","page":"Distributions","title":"Univariate Finite Distribution","text":"","category":"section"},{"location":"distributions/#","page":"Distributions","title":"Distributions","text":"Modules = [MLJBase]\nPages   = [\"interface/univariate_finite.jl\"]","category":"page"},{"location":"distributions/#hyperparameters-1","page":"Distributions","title":"hyperparameters","text":"","category":"section"},{"location":"distributions/#","page":"Distributions","title":"Distributions","text":"Modules = [MLJBase]\nPages   = [\"hyperparam/one_dimensional_range_methods.jl\", \"hyperparam/one_dimensional_ranges.jl\"]","category":"page"},{"location":"distributions/#Distributions.sampler-Union{Tuple{T}, Tuple{NumericRange{T,B,D} where D where B<:MLJBase.Boundedness,Distributions.Distribution{Distributions.Univariate,S} where S<:Distributions.ValueSupport}} where T","page":"Distributions","title":"Distributions.sampler","text":"sampler(r::NominalRange, probs::AbstractVector{<:Real})\nsampler(r::NominalRange)\nsampler(r::NumericRange{T}, d)\n\nConstruct an object s which can be used to generate random samples from a ParamRange object r (a one-dimensional range) using one of the following calls:\n\nrand(s)             # for one sample\nrand(s, n)          # for n samples\nrand(rng, s [, n])  # to specify an RNG\n\nThe argument probs can be any probability vector with the same length as r.values. The second sampler method above calls the first with a uniform probs vector.\n\nThe argument d can be either an arbitrary instance of UnivariateDistribution from the Distributions.jl package, or one of a Distributions.jl types for which fit(d, ::NumericRange) is defined. These include: Arcsine, Uniform, Biweight, Cosine, Epanechnikov, SymTriangularDist, Triweight, Normal, Gamma, InverseGaussian, Logistic, LogNormal, Cauchy, Gumbel, Laplace, and Poisson; but see the doc-string for Distributions.fit for an up-to-date list.\n\nIf d is an instance, then sampling is from a truncated form of the supplied distribution d, the truncation bounds being r.lower and r.upper (the attributes r.origin and r.unit attributes are ignored). For discrete numeric ranges (T <: Integer) the samples are rounded.\n\nIf d is a type then a suitably truncated distribution is automatically generated using Distributions.fit(d, r).\n\nImportant. Values are generated with no regard to r.scale, except in the special case r.scale is a callable object f. In that case, f is applied to all values generated by rand as described above (prior to rounding, in the case of discrete numeric ranges).\n\nExamples\n\nr = range(Char, :letter, values=collect(\"abc\"))\ns = sampler(r, [0.1, 0.2, 0.7])\nsamples =  rand(s, 1000);\nStatsBase.countmap(samples)\nDict{Char,Int64} with 3 entries:\n  'a' => 107\n  'b' => 205\n  'c' => 688\n\nr = range(Int, :k, lower=2, upper=6) # numeric but discrete\ns = sampler(r, Normal)\nsamples = rand(s, 1000);\nUnicodePlots.histogram(samples)\n           ┌                                        ┐\n[2.0, 2.5) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 119\n[2.5, 3.0) ┤ 0\n[3.0, 3.5) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 296\n[3.5, 4.0) ┤ 0\n[4.0, 4.5) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 275\n[4.5, 5.0) ┤ 0\n[5.0, 5.5) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 221\n[5.5, 6.0) ┤ 0\n[6.0, 6.5) ┤▇▇▇▇▇▇▇▇▇▇▇ 89\n           └                                        ┘\n\n\n\n\n\n","category":"method"},{"location":"distributions/#MLJBase.iterator-Tuple{Random.AbstractRNG,ParamRange,Vararg{Any,N} where N}","page":"Distributions","title":"MLJBase.iterator","text":"iterator([rng, ], r::NominalRange, [,n])\niterator([rng, ], r::NumericRange, n)\n\nReturn an iterator (currently a vector) for a ParamRange object r. In the first case iteration is over all values stored in the range (or just the first n, if n is specified). In the second case, the iteration is over approximately n ordered values, generated as follows:\n\n(i) First, exactly n values are generated between U and L, with a spacing determined by r.scale (uniform if scale=:linear) where U and L are given by the following table:\n\nr.lower r.upper L U\nfinite finite r.lower r.upper\n-Inf finite r.upper - 2r.unit r.upper\nfinite Inf r.lower r.lower + 2r.unit\n-Inf Inf r.origin - r.unit r.origin + r.unit\n\n(ii) If a callable f is provided as scale, then a uniform spacing is always applied in (i) but f is broadcast over the results. (Unlike ordinary scales, this alters the effective range of values generated, instead of just altering the spacing.)\n\n(iii) If r is a discrete numeric range (r isa NumericRange{<:Integer}) then the values are additionally rounded, with any duplicate values removed. Otherwise all the values are used (and there are exacltly n of them).\n\n(iv) Finally, if a random number generator rng is specified, then the values are returned in random order (sampling without replacement), and otherwise they are returned in numeric order, or in the order provided to the range constructor, in the case of a NominalRange.\n\n\n\n\n\n","category":"method"},{"location":"distributions/#MLJBase.scale-Tuple{NominalRange}","page":"Distributions","title":"MLJBase.scale","text":"scale(r::ParamRange)\n\nReturn the scale associated with a ParamRange object r. The possible return values are: :none (for a NominalRange), :linear, :log, :log10, :log2, or :custom (if r.scale is a callable object).\n\n\n\n\n\n","category":"method"},{"location":"distributions/#StatsBase.fit-Union{Tuple{D}, Tuple{Type{D},NumericRange}} where D<:Distributions.Distribution","page":"Distributions","title":"StatsBase.fit","text":"Distributions.fit(D, r::MLJBase.NumericRange)\n\nFit and return a distribution d of type D to the one-dimensional range r.\n\nOnly types D in the table below are supported.\n\nThe distribution d is constructed in two stages. First, a distributon d0, characterized by the conditions in the second column of the table, is fit to r. Then d0 is truncated between r.lower and r.upper to obtain d.\n\nDistribution type D Characterization of d0\nArcsine, Uniform, Biweight, Cosine, Epanechnikov, SymTriangularDist, Triweight minimum(d) = r.lower, maximum(d) = r.upper\nNormal, Gamma, InverseGaussian, Logistic, LogNormal mean(d) = r.origin, std(d) = r.unit\nCauchy, Gumbel, Laplace, (Normal) Dist.location(d) = r.origin, Dist.scale(d)  = r.unit\nPoisson Dist.mean(d) = r.unit\n\nHere Dist = Distributions.\n\n\n\n\n\n","category":"method"},{"location":"distributions/#Base.range-Union{Tuple{D}, Tuple{Union{Model, Type},Union{Expr, Symbol}}} where D","page":"Distributions","title":"Base.range","text":"r = range(model, :hyper; values=nothing)\n\nDefine a one-dimensional NominalRange object for a field hyper of model. Note that r is not directly iterable but iterator(r) is.\n\nA nested hyperparameter is specified using dot notation. For example, :(atom.max_depth) specifies the max_depth hyperparameter of the submodel model.atom.\n\nr = range(model, :hyper; upper=nothing, lower=nothing,\n          scale=nothing, values=nothing)\n\nAssuming values is not specified, define a one-dimensional NumericRange object for a Real field hyper of model.  Note that r is not directly iteratable but iterator(r, n)is an iterator of length n. To generate random elements from r, instead apply rand methods to sampler(r). The supported scales are :linear,:log, :logminus, :log10, :log2, or a callable object.\n\nNote that r is not directly iterable, but iterator(r, n) is, for given resolution (length) n.\n\nBy default, the behaviour of the constructed object depends on the type of the value of the hyperparameter :hyper at model at the time of construction. To override this behaviour (for instance if model is not available) specify a type in place of model so the behaviour is determined by the value of the specified type.\n\nA nested hyperparameter is specified using dot notation (see above).\n\nIf scale is unspecified, it is set to :linear, :log, :logminus, or :linear, according to whether the interval (lower, upper) is bounded, right-unbounded, left-unbounded, or doubly unbounded, respectively.  Note upper=Inf and lower=-Inf are allowed.\n\nIf values is specified, the other keyword arguments are ignored and a NominalRange object is returned (see above).\n\nSee also: iterator, sampler\n\n\n\n\n\n","category":"method"},{"location":"distributions/#Utility-functions-1","page":"Distributions","title":"Utility functions","text":"","category":"section"},{"location":"distributions/#","page":"Distributions","title":"Distributions","text":"Modules = [MLJBase]\nPages   = [\"distributions.jl\"]","category":"page"},{"location":"utilities/#Utilities-1","page":"Utilities","title":"Utilities","text":"","category":"section"},{"location":"utilities/#Machines-1","page":"Utilities","title":"Machines","text":"","category":"section"},{"location":"utilities/#","page":"Utilities","title":"Utilities","text":"Modules = [MLJBase]\nPages   = [\"machines.jl\"]","category":"page"},{"location":"utilities/#MLJBase.fit_only!-Union{Tuple{Machine{var\"#s13\",cache_data} where var\"#s13\"<:Model}, Tuple{cache_data}} where cache_data","page":"Utilities","title":"MLJBase.fit_only!","text":"MLJBase.fit_only!(mach::Machine; rows=nothing, verbosity=1, force=false)\n\nWithout mutating any other machine on which it may depend, perform one of the following actions to the machine mach, using the data and model bound to it, and restricting the data to rows if specified:\n\nAb initio training. Ignoring any previous learned parameters and cache, compute and store new learned parameters. Increment mach.state.\nTraining update. Making use of previous learned parameters and/or  cache, replace or mutate existing learned parameters. The effect is  the same (or nearly the same) as in ab initio training, but may be  faster or use less memory, assuming the model supports an update  option (implements MLJBase.update). Increment mach.state.\nNo-operation. Leave existing learned parameters untouched. Do not  increment mach.state.\n\nTraining action logic\n\nFor the action to be a no-operation, either mach.frozen == true or or none of the following apply:\n\n(i) mach has never been trained (mach.state == 0).\n(ii) force == true.\n(iii) The state of some other machine on which mach depends has changed since the last time mach was trained (ie, the last time mach.state was last incremented).\n(iv) The specified rows have changed since the last retraining and mach.model does not have Static type.\n(v) mach.model has changed since the last retraining.\n\nIn any of the cases (i) - (iv), mach is trained ab initio. If only (v) fails, then a training update is applied.\n\nTo freeze or unfreeze mach, use freeze!(mach) or thaw!(mach).\n\nImplementation detail\n\nThe data to which a machine is bound is stored in mach.args. Each element of args is either a Node object, or, in the case that concrete data was bound to the machine, it is concrete data wrapped in a Source node. In all cases, to obtain concrete data for actual training, each argument N is called, as in N() or N(rows=rows), and either MLJBase.fit (ab initio training) or MLJBase.update (training update) is dispatched on mach.model and this data. See the \"Adding models for general use\" section of the MLJ documentation for more on these lower-level training methods.\n\n\n\n\n\n","category":"method"},{"location":"utilities/#MLJBase.freeze!-Tuple{Machine}","page":"Utilities","title":"MLJBase.freeze!","text":"freeze!(mach)\n\nFreeze the machine mach so that it will never be retrained (unless thawed).\n\nSee also thaw!.\n\n\n\n\n\n","category":"method"},{"location":"utilities/#MLJBase.glb-Tuple{Machine{var\"#s255\",C} where C where var\"#s255\"<:Union{DeterministicSurrogate, IntervalSurrogate, JointProbabilisticSurrogate, ProbabilisticSurrogate, StaticSurrogate, UnsupervisedSurrogate, DeterministicComposite, IntervalComposite, JointProbabilisticComposite, ProbabilisticComposite, StaticComposite, UnsupervisedComposite}}","page":"Utilities","title":"MLJBase.glb","text":"N = glb(mach::Machine{<:Surrogate})\n\nA greatest lower bound for the nodes appearing in the signature of mach.\n\nPrivate method.\n\n\n\n\n\n","category":"method"},{"location":"utilities/#MLJBase.machine","page":"Utilities","title":"MLJBase.machine","text":"machine(model, args...; cache=true)\n\nConstruct a Machine object binding a model, storing hyper-parameters of some machine learning algorithm, to some data, args. When building a learning network, Node objects can be substituted for concrete data. Specify cache=false to prioritize memory managment over speed, and to guarantee data anonymity when serializing composite models.\n\nmachine(Xs; oper1=node1, oper2=node2)\nmachine(Xs, ys; oper1=node1, oper2=node2)\nmachine(Xs, ys, extras...; oper1=node1, oper2=node2, ...)\n\nConstruct a special machine called a learning network machine, that \"wraps\" a learning network, usually in preparation to export the network as a stand-alone composite model type. The keyword arguments declare what nodes are called when operations, such as predict and transform, are called on the machine.\n\nIn addition to the operations named in the constructor, the methods fit!, report, and fitted_params can be applied as usual to the machine constructed.\n\nmachine(Probablistic(), args...; kwargs...)\nmachine(Deterministic(), args...; kwargs...)\nmachine(Unsupervised(), args...; kwargs...)\nmachine(Static(), args...; kwargs...)\n\nSame as above, but specifying explicitly the kind of model the learning network is to meant to represent.\n\nLearning network machines are not to be confused with an ordinary machine that happens to be bound to a stand-alone composite model (i.e., an exported learning network).\n\nExamples\n\nSupposing a supervised learning network's final predictions are obtained by calling a node yhat, then the code\n\nmach = machine(Deterministic(), Xs, ys; predict=yhat)\nfit!(mach; rows=train)\npredictions = predict(mach, Xnew) # `Xnew` concrete data\n\nis  equivalent to\n\nfit!(yhat, rows=train)\npredictions = yhat(Xnew)\n\nHere Xs and ys are the source nodes receiving, respectively, the input and target data.\n\nIn a unsupervised learning network for clustering, with single source node Xs for inputs, and in which the node Xout delivers the output of dimension reduction, and yhat the class labels, one can write\n\nmach = machine(Unsupervised(), Xs; transform=Xout, predict=yhat)\nfit!(mach)\ntransformed = transform(mach, Xnew) # `Xnew` concrete data\npredictions = predict(mach, Xnew)\n\nwhich is equivalent to\n\nfit!(Xout)\nfit!(yhat)\ntransformed = Xout(Xnew)\npredictions = yhat(Xnew)\n\n\n\n\n\n","category":"function"},{"location":"utilities/#MLJBase.report-Tuple{Machine}","page":"Utilities","title":"MLJBase.report","text":"report(mach)\n\nReturn the report for a machine mach that has been fit!, for example the coefficients in a linear model.\n\nThis is a named tuple and human-readable if possible.\n\nIf mach is a machine for a composite model, such as a model constructed using @pipeline, then the returned named tuple has the composite type's field names as keys. The corresponding value is the report for the machine in the underlying learning network bound to that model. (If multiple machines share the same model, then the value is a vector.)\n\nusing MLJ\n@load LinearBinaryClassifier pkg=GLM\nX, y = @load_crabs;\npipe = @pipeline Standardizer LinearBinaryClassifier\nmach = machine(pipe, X, y) |> fit!\n\njulia> report(mach).linear_binary_classifier\n(deviance = 3.8893386087844543e-7,\n dof_residual = 195.0,\n stderror = [18954.83496713119, 6502.845740757159, 48484.240246060406, 34971.131004997274, 20654.82322484894, 2111.1294584763386],\n vcov = [3.592857686311793e8 9.122732393971942e6 … -8.454645589364915e7 5.38856837634321e6; 9.122732393971942e6 4.228700272808351e7 … -4.978433790526467e7 -8.442545425533723e6; … ; -8.454645589364915e7 -4.978433790526467e7 … 4.2662172244975924e8 2.1799125705781363e7; 5.38856837634321e6 -8.442545425533723e6 … 2.1799125705781363e7 4.456867590446599e6],)\n\n\nAdditional keys, machines and report_given_machine, give a list of all machines in the underlying network, and a dictionary of reports keyed on those machines.\n\n```\n\n\n\n\n\n","category":"method"},{"location":"utilities/#MLJBase.return!-Tuple{Machine{var\"#s255\",C} where C where var\"#s255\"<:Union{DeterministicSurrogate, IntervalSurrogate, JointProbabilisticSurrogate, ProbabilisticSurrogate, StaticSurrogate, UnsupervisedSurrogate},Union{Nothing, Model},Any}","page":"Utilities","title":"MLJBase.return!","text":"return!(mach::Machine{<:Surrogate}, model, verbosity)\n\nThe last call in custom code defining the MLJBase.fit method for a new composite model type. Here model is the instance of the new type appearing in the MLJBase.fit signature, while mach is a learning network machine constructed using model. Not relevant when defining composite models using @pipeline or @from_network.\n\nFor usage, see the example given below. Specificlly, the call does the following:\n\nDetermines which fields of model point to model instances in the learning network wrapped by mach, for recording in an object called cache, for passing onto the MLJ logic that handles smart updating (namely, an MLJBase.update fallback for composite models).\nCalls fit!(mach, verbosity=verbosity).\nMoves any data in source nodes of the learning network into cache (for data-anonymization purposes).\nRecords a copy of model in cache.\nReturns cache and outcomes of training in an appropriate form (specifically, (mach.fitresult, cache, mach.report); see Adding Models for General Use for technical details.)\n\nExample\n\nThe following code defines, \"by hand\", a new model type MyComposite for composing standardization (whitening) with a deterministic regressor:\n\nmutable struct MyComposite <: DeterministicComposite\n    regressor\nend\n\nfunction MLJBase.fit(model::MyComposite, verbosity, X, y)\n    Xs = source(X)\n    ys = source(y)\n\n    mach1 = machine(Standardizer(), Xs)\n    Xwhite = transform(mach1, Xs)\n\n    mach2 = machine(model.regressor, Xwhite, ys)\n    yhat = predict(mach2, Xwhite)\n\n    mach = machine(Deterministic(), Xs, ys; predict=yhat)\n    return!(mach, model, verbosity)\nend\n\n\n\n\n\n","category":"method"},{"location":"utilities/#MLJBase.thaw!-Tuple{Machine}","page":"Utilities","title":"MLJBase.thaw!","text":"thaw!(mach)\n\nUnfreeze the machine mach so that it can be retrained.\n\nSee also freeze!.\n\n\n\n\n\n","category":"method"},{"location":"utilities/#MLJModelInterface.fitted_params-Tuple{Machine}","page":"Utilities","title":"MLJModelInterface.fitted_params","text":"fitted_params(mach)\n\nReturn the learned parameters for a machine mach that has been fit!, for example the coefficients in a linear model.\n\nThis is a named tuple and human-readable if possible.\n\nIf mach is a machine for a composite model, such as a model constructed using @pipeline, then the returned named tuple has the composite type's field names as keys. The corresponding value is the fitted parameters for the machine in the underlying learning network bound to that model. (If multiple machines share the same model, then the value is a vector.)\n\nusing MLJ\n@load LogisticClassifier pkg=MLJLinearModels\nX, y = @load_crabs;\npipe = @pipeline Standardizer LogisticClassifier\nmach = machine(pipe, X, y) |> fit!\n\njulia> fitted_params(mach).logistic_classifier\n(classes = CategoricalArrays.CategoricalValue{String,UInt32}[\"B\", \"O\"],\n coefs = Pair{Symbol,Float64}[:FL => 3.7095037897680405, :RW => 0.1135739140854546, :CL => -1.6036892745322038, :CW => -4.415667573486482, :BD => 3.238476051092471],\n intercept = 0.0883301599726305,)\n\nAdditional keys, machines and fitted_params_given_machine, give a list of all machines in the underlying network, and a dictionary of fitted parameters keyed on those machines.\n\n```\n\n\n\n\n\n","category":"method"},{"location":"utilities/#StatsBase.fit!-Tuple{Machine{var\"#s252\",C} where C where var\"#s252\"<:Union{DeterministicSurrogate, IntervalSurrogate, JointProbabilisticSurrogate, ProbabilisticSurrogate, StaticSurrogate, UnsupervisedSurrogate}}","page":"Utilities","title":"StatsBase.fit!","text":"fit!(mach::Machine{<:Surrogate};\n     rows=nothing,\n     acceleration=CPU1(),\n     verbosity=1,\n     force=false))\n\nTrain the complete learning network wrapped by the machine mach.\n\nMore precisely, if s is the learning network signature used to construct mach, then call fit!(N), where N = glb(values(s)...) is a greatest lower bound on the nodes appearing in the signature. For example, if s = (predict=yhat, transform=W), then call fit!(glb(yhat, W)). Here glb is tuple overloaded for nodes.\n\nSee also machine\n\n\n\n\n\n","category":"method"},{"location":"utilities/#StatsBase.fit!-Tuple{Machine}","page":"Utilities","title":"StatsBase.fit!","text":"fit!(mach::Machine, rows=nothing, verbosity=1, force=false)\n\nFit the machine mach. In the case that mach has Node arguments, first train all other machines on which mach depends.\n\nTo attempt to fit a machine without touching any other machine, use fit_only!. For more on the internal logic of fitting see fit_only!\n\n\n\n\n\n","category":"method"},{"location":"utilities/#Base.replace-Tuple{Machine{var\"#s251\",C} where C where var\"#s251\"<:Union{DeterministicSurrogate, IntervalSurrogate, JointProbabilisticSurrogate, ProbabilisticSurrogate, StaticSurrogate, UnsupervisedSurrogate},Vararg{Pair,N} where N}","page":"Utilities","title":"Base.replace","text":"replace(mach, a1=>b1, a2=>b2, ...; empty_unspecified_sources=false)\n\nCreate a deep copy of a learning network machine mach but replacing any specified sources and models a1, a2, ... of the original underlying network with b1, b2, ....\n\nIf empty_unspecified_sources=true then any source nodes not specified are replaced with empty source nodes.\n\n\n\n\n\n","category":"method"},{"location":"utilities/#MLJBase.ancestors-Tuple{Machine}","page":"Utilities","title":"MLJBase.ancestors","text":"ancestors(mach::Machine; self=false)\n\nAll ancestors of mach, including mach if self=true.\n\n\n\n\n\n","category":"method"},{"location":"utilities/#MLJBase.model_supertype-Tuple{Any}","page":"Utilities","title":"MLJBase.model_supertype","text":"model_supertype(signature)\n\nReturn, if this can be deduced, which of Deterministic, Probabilistic and Unsupervised is the appropriate supertype for a composite model obtained by exporting a learning network with the specified signature.\n\nA learning network signature is a named tuple, such as (predict=yhat, transfrom=W), specifying what nodes of the network are called to produce output of each operation represented by the keys, in an exported version of the network.\n\nIf a supertype cannot be deduced, nothing is returned.\n\nIf the network with given signature is not exportable, this method will not error but it will not a give meaningful return value either.\n\nPrivate method.\n\n\n\n\n\n","category":"method"},{"location":"utilities/#MLJModelInterface.save-Tuple{Union{IO, String},Machine}","page":"Utilities","title":"MLJModelInterface.save","text":"MLJ.save(filename, mach::Machine; kwargs...)\nMLJ.save(io, mach::Machine; kwargs...)\n\nMLJBase.save(filename, mach::Machine; kwargs...)\nMLJBase.save(io, mach::Machine; kwargs...)\n\nSerialize the machine mach to a file with path filename, or to an input/output stream io (at least IOBuffer instances are supported).\n\nThe format is JLSO (a wrapper for julia native or BSON serialization). For some model types, a custom serialization will be additionally performed.\n\nKeyword arguments\n\nThese keyword arguments are passed to the JLSO serializer:\n\nkeyword values default\nformat :julia_serialize, :BSON :julia_serialize\ncompression :gzip, :none :none\n\nSee https://github.com/invenia/JLSO.jl for details.\n\nAny additional keyword arguments are passed to model-specific serializers.\n\nMachines are de-serialized using the machine constructor as shown in the example below. Data (or nodes) may be optionally passed to the constructor for retraining on new data using the saved model.\n\nExample\n\nusing MLJ\ntree = @load DecisionTreeClassifier\nX, y = @load_iris\nmach = fit!(machine(tree, X, y))\n\nMLJ.save(\"tree.jlso\", mach, compression=:none)\nmach_predict_only = machine(\"tree.jlso\")\npredict(mach_predict_only, X)\n\nmach2 = machine(\"tree.jlso\", selectrows(X, 1:100), y[1:100])\npredict(mach2, X) # same as above\n\nfit!(mach2) # saved learned parameters are over-written\npredict(mach2, X) # not same as above\n\n# using a buffer:\nio = IOBuffer()\nMLJ.save(io, mach)\nseekstart(io)\npredict_only_mach = machine(io)\npredict(predict_only_mach, X)\n\nwarning: Only load files from trusted sources\nMaliciously constructed JLSO files, like pickles, and most other general purpose serialization formats, can allow for arbitrary code execution during loading. This means it is possible for someone to use a JLSO file that looks like a serialized MLJ machine as a Trojan horse.\n\n\n\n\n\n","category":"method"},{"location":"utilities/#Parameter-Inspection-1","page":"Utilities","title":"Parameter Inspection","text":"","category":"section"},{"location":"utilities/#","page":"Utilities","title":"Utilities","text":"Modules = [MLJBase]\nPages   = [\"parameter_inspection.jl\"]","category":"page"},{"location":"utilities/#Show-1","page":"Utilities","title":"Show","text":"","category":"section"},{"location":"utilities/#","page":"Utilities","title":"Utilities","text":"Modules = [MLJBase]\nPages   = [\"show.jl\"]","category":"page"},{"location":"utilities/#MLJBase.color_off-Tuple{}","page":"Utilities","title":"MLJBase.color_off","text":"color_off()\n\nSuppress color and bold output at the REPL for displaying MLJ objects.\n\n\n\n\n\n","category":"method"},{"location":"utilities/#MLJBase.color_on-Tuple{}","page":"Utilities","title":"MLJBase.color_on","text":"color_on()\n\nEnable color and bold output at the REPL, for enhanced display of MLJ objects.\n\n\n\n\n\n","category":"method"},{"location":"utilities/#MLJBase.@constant-Tuple{Any}","page":"Utilities","title":"MLJBase.@constant","text":"@constant x = value\n\nEquivalent to const x = value but registers the binding thus:\n\nMLJBase.HANDLE_GIVEN_ID[objectid(value)] = :x\n\nRegistered objects get displayed using the variable name to which it was bound in calls to show(x), etc.\n\nWARNING: As with any const declaration, binding x to new value of the same type is not prevented and the registration will not be updated.\n\n\n\n\n\n","category":"macro"},{"location":"utilities/#MLJBase.@more-Tuple{}","page":"Utilities","title":"MLJBase.@more","text":"@more\n\nEntered at the REPL, equivalent to show(ans, 100). Use to get a recursive description of all fields of the last REPL value.\n\n\n\n\n\n","category":"macro"},{"location":"utilities/#MLJBase._recursive_show-Tuple{IO,MLJType,Any,Any}","page":"Utilities","title":"MLJBase._recursive_show","text":"_recursive_show(stream, object, current_depth, depth)\n\nGenerate a table of the field values of the MLJType object, dislaying each value by calling the method _show on it. The behaviour of _show(stream, f) is as follows:\n\nIf f is itself a MLJType object, then its short form is shown\n\nand _recursive_show generates as separate table for each of its field values (and so on, up to a depth of argument depth).\n\nOtherwise f is displayed as \"(omitted T)\" where T = typeof(f),\n\nunless istoobig(f) is false (the istoobig fall-back for arbitrary types being true). In the latter case, the long (ie, MIME\"plain/text\") form of f is shown. To override this behaviour, overload the _show method for the type in question.\n\n\n\n\n\n","category":"method"},{"location":"utilities/#MLJBase.abbreviated-Tuple{Any}","page":"Utilities","title":"MLJBase.abbreviated","text":"to display abbreviated versions of integers\n\n\n\n\n\n","category":"method"},{"location":"utilities/#MLJBase.handle-Tuple{Any}","page":"Utilities","title":"MLJBase.handle","text":"return abbreviated object id (as string) or it's registered handle (as string) if this exists\n\n\n\n\n\n","category":"method"},{"location":"utilities/#Utility-functions-1","page":"Utilities","title":"Utility functions","text":"","category":"section"},{"location":"utilities/#","page":"Utilities","title":"Utilities","text":"Modules = [MLJBase]\nPages   = [\"utilities.jl\"]","category":"page"},{"location":"utilities/#LossFunctions.DWDMarginLoss","page":"Utilities","title":"LossFunctions.DWDMarginLoss","text":"LossFunctions.DWDMarginLoss\n\nA measure type for distance weighted discrimination loss, which includes the instance(s), dwd_margin_loss.\n\nDWDMarginLoss()(ŷ, y)\nDWDMarginLoss()(ŷ, y, w)\n\nEvaluate the default instance of DWDMarginLoss on observations ŷ, given ground truth values y. Optionally specify per-sample weights, w. \n\nSee above for original LossFunctions.jl documentation. \n\nRequires scitype(y) to be a subtype of AbstractArray{<:Finite{2}} (binary classification); ŷ must be a probabilistic prediction. \n\nConstructor signature: DWDMarginLoss(; q=1.0)\n\nFor more information, run info(DWDMarginLoss). \n\n\n\n\n\n","category":"type"},{"location":"utilities/#LossFunctions.ExpLoss","page":"Utilities","title":"LossFunctions.ExpLoss","text":"LossFunctions.ExpLoss\n\nA measure type for exp loss, which includes the instance(s), exp_loss.\n\nExpLoss()(ŷ, y)\nExpLoss()(ŷ, y, w)\n\nEvaluate the exp loss on observations ŷ, given ground truth values y. Optionally specify per-sample weights, w. \n\nSee above for original LossFunctions.jl documentation. \n\nRequires scitype(y) to be a subtype of AbstractArray{<:Finite{2}} (binary classification); ŷ must be a probabilistic prediction. \n\nFor more information, run info(ExpLoss). \n\n\n\n\n\n","category":"type"},{"location":"utilities/#LossFunctions.HuberLoss","page":"Utilities","title":"LossFunctions.HuberLoss","text":"LossFunctions.HuberLoss\n\nA measure type for huber loss, which includes the instance(s), huber_loss.\n\nHuberLoss()(ŷ, y)\nHuberLoss()(ŷ, y, w)\n\nEvaluate the default instance of HuberLoss on observations ŷ, given ground truth values y. Optionally specify per-sample weights, w. \n\nSee above for original LossFunctions.jl documentation. \n\nRequires scitype(y) to be a subtype of Union{AbstractArray{ScientificTypes.Continuous,1}, AbstractArray{ScientificTypes.Count,1}}; ŷ must be a deterministic prediction. \n\nFor more information, run info(HuberLoss). \n\n\n\n\n\n","category":"type"},{"location":"utilities/#LossFunctions.L1EpsilonInsLoss","page":"Utilities","title":"LossFunctions.L1EpsilonInsLoss","text":"LossFunctions.L1EpsilonInsLoss\n\nA measure type for l1 ϵ-insensitive loss, which includes the instance(s), l1_epsilon_ins_loss.\n\nL1EpsilonInsLoss()(ŷ, y)\nL1EpsilonInsLoss()(ŷ, y, w)\n\nEvaluate the default instance of L1EpsilonInsLoss on observations ŷ, given ground truth values y. Optionally specify per-sample weights, w. \n\nSee above for original LossFunctions.jl documentation. \n\nRequires scitype(y) to be a subtype of Union{AbstractArray{ScientificTypes.Continuous,1}, AbstractArray{ScientificTypes.Count,1}}; ŷ must be a deterministic prediction. \n\nConstructor signature: L1EpsilonInsLoss(; ϵ=1.0)\n\nFor more information, run info(L1EpsilonInsLoss). \n\n\n\n\n\n","category":"type"},{"location":"utilities/#LossFunctions.L1HingeLoss","page":"Utilities","title":"LossFunctions.L1HingeLoss","text":"LossFunctions.L1HingeLoss\n\nA measure type for l1 hinge loss, which includes the instance(s), l1_hinge_loss.\n\nL1HingeLoss()(ŷ, y)\nL1HingeLoss()(ŷ, y, w)\n\nEvaluate the l1 hinge loss on observations ŷ, given ground truth values y. Optionally specify per-sample weights, w. \n\nSee above for original LossFunctions.jl documentation. \n\nRequires scitype(y) to be a subtype of AbstractArray{<:Finite{2}} (binary classification); ŷ must be a probabilistic prediction. \n\nFor more information, run info(L1HingeLoss). \n\n\n\n\n\n","category":"type"},{"location":"utilities/#LossFunctions.L2EpsilonInsLoss","page":"Utilities","title":"LossFunctions.L2EpsilonInsLoss","text":"LossFunctions.L2EpsilonInsLoss\n\nA measure type for l2 ϵ-insensitive loss, which includes the instance(s), l2_epsilon_ins_loss.\n\nL2EpsilonInsLoss()(ŷ, y)\nL2EpsilonInsLoss()(ŷ, y, w)\n\nEvaluate the default instance of L2EpsilonInsLoss on observations ŷ, given ground truth values y. Optionally specify per-sample weights, w. \n\nSee above for original LossFunctions.jl documentation. \n\nRequires scitype(y) to be a subtype of Union{AbstractArray{ScientificTypes.Continuous,1}, AbstractArray{ScientificTypes.Count,1}}; ŷ must be a deterministic prediction. \n\nConstructor signature: L2EpsilonInsLoss(; ϵ=1.0)\n\nFor more information, run info(L2EpsilonInsLoss). \n\n\n\n\n\n","category":"type"},{"location":"utilities/#LossFunctions.L2HingeLoss","page":"Utilities","title":"LossFunctions.L2HingeLoss","text":"LossFunctions.L2HingeLoss\n\nA measure type for l2 hinge loss, which includes the instance(s), l2_hinge_loss.\n\nL2HingeLoss()(ŷ, y)\nL2HingeLoss()(ŷ, y, w)\n\nEvaluate the l2 hinge loss on observations ŷ, given ground truth values y. Optionally specify per-sample weights, w. \n\nSee above for original LossFunctions.jl documentation. \n\nRequires scitype(y) to be a subtype of AbstractArray{<:Finite{2}} (binary classification); ŷ must be a probabilistic prediction. \n\nFor more information, run info(L2HingeLoss). \n\n\n\n\n\n","category":"type"},{"location":"utilities/#LossFunctions.L2MarginLoss","page":"Utilities","title":"LossFunctions.L2MarginLoss","text":"LossFunctions.L2MarginLoss\n\nA measure type for l2 margin loss, which includes the instance(s), l2_margin_loss.\n\nL2MarginLoss()(ŷ, y)\nL2MarginLoss()(ŷ, y, w)\n\nEvaluate the l2 margin loss on observations ŷ, given ground truth values y. Optionally specify per-sample weights, w. \n\nSee above for original LossFunctions.jl documentation. \n\nRequires scitype(y) to be a subtype of AbstractArray{<:Finite{2}} (binary classification); ŷ must be a probabilistic prediction. \n\nFor more information, run info(L2MarginLoss). \n\n\n\n\n\n","category":"type"},{"location":"utilities/#LossFunctions.LPDistLoss","page":"Utilities","title":"LossFunctions.LPDistLoss","text":"LossFunctions.LPDistLoss\n\nA measure type for lp dist loss, which includes the instance(s), lp_dist_loss.\n\nLPDistLoss()(ŷ, y)\nLPDistLoss()(ŷ, y, w)\n\nEvaluate the lp dist loss on observations ŷ, given ground truth values y. Optionally specify per-sample weights, w. \n\nSee above for original LossFunctions.jl documentation. \n\nRequires scitype(y) to be a subtype of Union{AbstractArray{ScientificTypes.Continuous,1}, AbstractArray{ScientificTypes.Count,1}}; ŷ must be a deterministic prediction. \n\nConstructor signature: LPDistLoss(; P=2)\n\nFor more information, run info(LPDistLoss). \n\n\n\n\n\n","category":"type"},{"location":"utilities/#LossFunctions.LogitDistLoss","page":"Utilities","title":"LossFunctions.LogitDistLoss","text":"LossFunctions.LogitDistLoss\n\nA measure type for logit dist loss, which includes the instance(s), logit_dist_loss.\n\nLogitDistLoss()(ŷ, y)\nLogitDistLoss()(ŷ, y, w)\n\nEvaluate the logit dist loss on observations ŷ, given ground truth values y. Optionally specify per-sample weights, w. \n\nSee above for original LossFunctions.jl documentation. \n\nRequires scitype(y) to be a subtype of Union{AbstractArray{ScientificTypes.Continuous,1}, AbstractArray{ScientificTypes.Count,1}}; ŷ must be a deterministic prediction. \n\nFor more information, run info(LogitDistLoss). \n\n\n\n\n\n","category":"type"},{"location":"utilities/#LossFunctions.LogitMarginLoss","page":"Utilities","title":"LossFunctions.LogitMarginLoss","text":"LossFunctions.LogitMarginLoss\n\nA measure type for logit margin loss, which includes the instance(s), logit_margin_loss.\n\nLogitMarginLoss()(ŷ, y)\nLogitMarginLoss()(ŷ, y, w)\n\nEvaluate the logit margin loss on observations ŷ, given ground truth values y. Optionally specify per-sample weights, w. \n\nSee above for original LossFunctions.jl documentation. \n\nRequires scitype(y) to be a subtype of AbstractArray{<:Finite{2}} (binary classification); ŷ must be a probabilistic prediction. \n\nFor more information, run info(LogitMarginLoss). \n\n\n\n\n\n","category":"type"},{"location":"utilities/#LossFunctions.ModifiedHuberLoss","page":"Utilities","title":"LossFunctions.ModifiedHuberLoss","text":"LossFunctions.ModifiedHuberLoss\n\nA measure type for modified huber loss, which includes the instance(s), modified_huber_loss.\n\nModifiedHuberLoss()(ŷ, y)\nModifiedHuberLoss()(ŷ, y, w)\n\nEvaluate the modified huber loss on observations ŷ, given ground truth values y. Optionally specify per-sample weights, w. \n\nSee above for original LossFunctions.jl documentation. \n\nRequires scitype(y) to be a subtype of AbstractArray{<:Finite{2}} (binary classification); ŷ must be a probabilistic prediction. \n\nFor more information, run info(ModifiedHuberLoss). \n\n\n\n\n\n","category":"type"},{"location":"utilities/#LossFunctions.PerceptronLoss","page":"Utilities","title":"LossFunctions.PerceptronLoss","text":"LossFunctions.PerceptronLoss\n\nA measure type for perceptron loss, which includes the instance(s), perceptron_loss.\n\nPerceptronLoss()(ŷ, y)\nPerceptronLoss()(ŷ, y, w)\n\nEvaluate the perceptron loss on observations ŷ, given ground truth values y. Optionally specify per-sample weights, w. \n\nSee above for original LossFunctions.jl documentation. \n\nRequires scitype(y) to be a subtype of AbstractArray{<:Finite{2}} (binary classification); ŷ must be a probabilistic prediction. \n\nFor more information, run info(PerceptronLoss). \n\n\n\n\n\n","category":"type"},{"location":"utilities/#LossFunctions.PeriodicLoss","page":"Utilities","title":"LossFunctions.PeriodicLoss","text":"LossFunctions.PeriodicLoss\n\nA measure type for periodic loss, which includes the instance(s), periodic_loss.\n\nPeriodicLoss()(ŷ, y)\nPeriodicLoss()(ŷ, y, w)\n\nEvaluate the default instance of PeriodicLoss on observations ŷ, given ground truth values y. Optionally specify per-sample weights, w. \n\nSee above for original LossFunctions.jl documentation. \n\nRequires scitype(y) to be a subtype of Union{AbstractArray{ScientificTypes.Continuous,1}, AbstractArray{ScientificTypes.Count,1}}; ŷ must be a deterministic prediction. \n\nFor more information, run info(PeriodicLoss). \n\n\n\n\n\n","category":"type"},{"location":"utilities/#LossFunctions.QuantileLoss","page":"Utilities","title":"LossFunctions.QuantileLoss","text":"LossFunctions.QuantileLoss\n\nA measure type for quantile loss, which includes the instance(s), quantile_loss.\n\nQuantileLoss()(ŷ, y)\nQuantileLoss()(ŷ, y, w)\n\nEvaluate the default instance of QuantileLoss on observations ŷ, given ground truth values y. Optionally specify per-sample weights, w. \n\nSee above for original LossFunctions.jl documentation. \n\nRequires scitype(y) to be a subtype of Union{AbstractArray{ScientificTypes.Continuous,1}, AbstractArray{ScientificTypes.Count,1}}; ŷ must be a deterministic prediction. \n\nConstructor signature: QuantileLoss(; τ=0.7)\n\nFor more information, run info(QuantileLoss). \n\n\n\n\n\n","category":"type"},{"location":"utilities/#LossFunctions.SigmoidLoss","page":"Utilities","title":"LossFunctions.SigmoidLoss","text":"LossFunctions.SigmoidLoss\n\nA measure type for sigmoid loss, which includes the instance(s), sigmoid_loss.\n\nSigmoidLoss()(ŷ, y)\nSigmoidLoss()(ŷ, y, w)\n\nEvaluate the sigmoid loss on observations ŷ, given ground truth values y. Optionally specify per-sample weights, w. \n\nSee above for original LossFunctions.jl documentation. \n\nRequires scitype(y) to be a subtype of AbstractArray{<:Finite{2}} (binary classification); ŷ must be a probabilistic prediction. \n\nFor more information, run info(SigmoidLoss). \n\n\n\n\n\n","category":"type"},{"location":"utilities/#LossFunctions.SmoothedL1HingeLoss","page":"Utilities","title":"LossFunctions.SmoothedL1HingeLoss","text":"LossFunctions.SmoothedL1HingeLoss\n\nA measure type for smoothed l1 hinge loss, which includes the instance(s), smoothed_l1_hinge_loss.\n\nSmoothedL1HingeLoss()(ŷ, y)\nSmoothedL1HingeLoss()(ŷ, y, w)\n\nEvaluate the default instance of SmoothedL1HingeLoss on observations ŷ, given ground truth values y. Optionally specify per-sample weights, w. \n\nSee above for original LossFunctions.jl documentation. \n\nRequires scitype(y) to be a subtype of AbstractArray{<:Finite{2}} (binary classification); ŷ must be a probabilistic prediction. \n\nConstructor signature: SmoothedL1HingeLoss(; γ=1.0)\n\nFor more information, run info(SmoothedL1HingeLoss). \n\n\n\n\n\n","category":"type"},{"location":"utilities/#LossFunctions.ZeroOneLoss","page":"Utilities","title":"LossFunctions.ZeroOneLoss","text":"LossFunctions.ZeroOneLoss\n\nA measure type for zero one loss, which includes the instance(s), zero_one_loss.\n\nZeroOneLoss()(ŷ, y)\nZeroOneLoss()(ŷ, y, w)\n\nEvaluate the zero one loss on observations ŷ, given ground truth values y. Optionally specify per-sample weights, w. \n\nSee above for original LossFunctions.jl documentation. \n\nRequires scitype(y) to be a subtype of AbstractArray{<:Finite{2}} (binary classification); ŷ must be a probabilistic prediction. \n\nFor more information, run info(ZeroOneLoss). \n\n\n\n\n\n","category":"type"},{"location":"utilities/#MLJBase.Accuracy","page":"Utilities","title":"MLJBase.Accuracy","text":"MLJBase.Accuracy\n\nA measure type for accuracy, which includes the instance(s), accuracy.\n\nAccuracy()(ŷ, y)\nAccuracy()(ŷ, y, w)\n\nEvaluate the accuracy on observations ŷ, given ground truth values y. Optionally specify per-sample weights, w. \n\nAccuracy is proportion of correct predictions ŷ[i] that match the ground truth y[i] observations. This metric is invariant to class reordering.\n\nRequires scitype(y) to be a subtype of AbstractArray{<:Finite} (multiclass classification); ŷ must be a deterministic prediction. \n\nFor more information, run info(Accuracy). \n\n\n\n\n\n","category":"type"},{"location":"utilities/#MLJBase.AreaUnderCurve","page":"Utilities","title":"MLJBase.AreaUnderCurve","text":"MLJBase.AreaUnderCurve\n\nA measure type for area under the ROC, which includes the instance(s), area_under_curve, auc.\n\nAreaUnderCurve()(ŷ, y)\n\nEvaluate the area under the ROC on observations ŷ, given ground truth values y. \n\nReturns the area under the ROC (receiver operator characteristic) This metric is invariant to class reordering.\n\nRequires scitype(y) to be a subtype of AbstractArray{var\"#s584\",1} where var\"#s584\"<:ScientificTypes.Finite{2}; ŷ must be a probabilistic prediction. \n\nFor more information, run info(AreaUnderCurve). \n\n\n\n\n\n","category":"type"},{"location":"utilities/#MLJBase.BalancedAccuracy","page":"Utilities","title":"MLJBase.BalancedAccuracy","text":"MLJBase.BalancedAccuracy\n\nA measure type for balanced accuracy, which includes the instance(s), balanced_accuracy, bacc, bac.\n\nBalancedAccuracy()(ŷ, y)\nBalancedAccuracy()(ŷ, y, w)\n\nEvaluate the balanced accuracy on observations ŷ, given ground truth values y. Optionally specify per-sample weights, w. \n\nBalanced accuracy compensates standard Accuracy for class imbalance. See https://en.wikipedia.org/wiki/Precisionandrecall#Imbalanced_data. This metric is invariant to class reordering.\n\nRequires scitype(y) to be a subtype of AbstractArray{<:Finite} (multiclass classification); ŷ must be a deterministic prediction. \n\nFor more information, run info(BalancedAccuracy). \n\n\n\n\n\n","category":"type"},{"location":"utilities/#MLJBase.BrierLoss","page":"Utilities","title":"MLJBase.BrierLoss","text":"MLJBase.BrierLoss\n\nA measure type for Brier loss (a.k.a. quadratic loss), which includes the instance(s), brier_loss.\n\nBrierLoss()(ŷ, y)\nBrierLoss()(ŷ, y, w)\n\nEvaluate the Brier loss (a.k.a. quadratic loss) on observations ŷ, given ground truth values y. Optionally specify per-sample weights, w. \n\nIf p(y) is the predicted probability for a single observation y, and C all possible classes, then the corresponding Brier score for that observation is given by\n\nleft(sum_η  C p(η)^2right) - 2p(y) + 1\n\nWarning. In Brier's original 1950 paper, what is implemented here is called a \"loss\". It is, however, a \"score\" in the contemporary use of that term: smaller is better (with 0 optimal, and all other values positive).  Note also the present implementation does not treat the binary case as special, so that the loss may differ, in that case, by a factor of two from usage elsewhere.\n\nRequires scitype(y) to be a subtype of AbstractArray{<:Finite} (multiclass classification); ŷ must be a probabilistic prediction. \n\nFor more information, run info(BrierLoss). \n\n\n\n\n\n","category":"type"},{"location":"utilities/#MLJBase.BrierScore","page":"Utilities","title":"MLJBase.BrierScore","text":"MLJBase.BrierScore\n\nA measure type for Brier score (a.k.a. quadratic score), which includes the instance(s), brier_score.\n\nBrierScore()(ŷ, y)\nBrierScore()(ŷ, y, w)\n\nEvaluate the Brier score (a.k.a. quadratic score) on observations ŷ, given ground truth values y. Optionally specify per-sample weights, w. \n\nIf p(y) is the predicted probability for a single observation y, and C all possible classes, then the corresponding Brier score for that observation is given by\n\n2p(y) - left(sum_η  C p(η)^2right) - 1\n\nWarning. BrierScore() is a \"score\" in the sense that bigger is better (with 0 optimal, and all other values negative). In Brier's original 1950 paper, and many other places, it has the opposite sign, despite the name. Moreover, the present implementation does not treat the binary case as special, so that the score may differ, in that case, by a factor of two from usage elsewhere.\n\nRequires scitype(y) to be a subtype of AbstractArray{<:Finite} (multiclass classification); ŷ must be a probabilistic prediction. \n\nFor more information, run info(BrierScore). \n\n\n\n\n\n","category":"type"},{"location":"utilities/#MLJBase.ConfusionMatrix","page":"Utilities","title":"MLJBase.ConfusionMatrix","text":"MLJBase.ConfusionMatrix\n\nA measure type for confusion matrix, which includes the instance(s), confusion_matrix, confmat.\n\nConfusionMatrix()(ŷ, y)\n\nEvaluate the default instance of ConfusionMatrix on observations ŷ, given ground truth values y. \n\nIf r is the return value, then the raw confusion matrix is r.mat, whose rows correspond to predictions, and columns to ground truth. The ordering follows that of levels(y).\n\nUse ConfusionMatrix(perm=[2, 1]) to reverse the class order for binary data. For more than two classes, specify an appropriate permutation, as in ConfusionMatrix(perm=[2, 3, 1]).\n\nRequires scitype(y) to be a subtype of AbstractArray{<:OrderedFactor{2}} (binary classification where choice of \"true\" effects the measure); ŷ must be a deterministic prediction. \n\nFor more information, run info(ConfusionMatrix). \n\n\n\n\n\n","category":"type"},{"location":"utilities/#MLJBase.FScore","page":"Utilities","title":"MLJBase.FScore","text":"MLJBase.FScore\n\nA measure type for F-Score, which includes the instance(s), f1score.\n\nFScore()(ŷ, y)\n\nEvaluate the default instance of FScore on observations ŷ, given ground truth values y. \n\nThis is the one-parameter generalization, F_β, of the F-measure or balanced F-score.\n\nhttps://en.wikipedia.org/wiki/F1_score\n\nConstructor signature: FScore(; β=1.0, rev=true).\n\nBy default, the second element of levels(y) is designated as true. To reverse roles, specify rev=true.\n\nRequires scitype(y) to be a subtype of AbstractArray{<:OrderedFactor{2}} (binary classification where choice of \"true\" effects the measure); ŷ must be a deterministic prediction. \n\nConstructor signature: FScore(β=1.0, rev=false). \n\nFor more information, run info(FScore). \n\n\n\n\n\n","category":"type"},{"location":"utilities/#MLJBase.FalseDiscoveryRate","page":"Utilities","title":"MLJBase.FalseDiscoveryRate","text":"MLJBase.FalseDiscoveryRate\n\nA measure type for false discovery rate, which includes the instance(s), false_discovery_rate, falsediscovery_rate, fdr.\n\nFalseDiscoveryRate()(ŷ, y)\n\nEvaluate the default instance of FalseDiscoveryRate on observations ŷ, given ground truth values y. \n\nAssigns false to first element of levels(y). To reverse roles, use FalseDiscoveryRate(rev=true).\n\nRequires scitype(y) to be a subtype of AbstractArray{<:OrderedFactor{2}} (binary classification where choice of \"true\" effects the measure); ŷ must be a deterministic prediction. \n\nFor more information, run info(FalseDiscoveryRate). \n\n\n\n\n\n","category":"type"},{"location":"utilities/#MLJBase.FalseNegative","page":"Utilities","title":"MLJBase.FalseNegative","text":"MLJBase.FalseNegative\n\nA measure type for number of false negatives, which includes the instance(s), false_negative, falsenegative.\n\nFalseNegative()(ŷ, y)\n\nEvaluate the default instance of FalseNegative on observations ŷ, given ground truth values y. \n\nAssigns false to first element of levels(y). To reverse roles, use FalseNegative(rev=true).\n\nRequires scitype(y) to be a subtype of AbstractArray{<:OrderedFactor{2}} (binary classification where choice of \"true\" effects the measure); ŷ must be a deterministic prediction. \n\nFor more information, run info(FalseNegative). \n\n\n\n\n\n","category":"type"},{"location":"utilities/#MLJBase.FalseNegativeRate","page":"Utilities","title":"MLJBase.FalseNegativeRate","text":"MLJBase.FalseNegativeRate\n\nA measure type for false negative rate, which includes the instance(s), false_negative_rate, falsenegative_rate, fnr, miss_rate.\n\nFalseNegativeRate()(ŷ, y)\n\nEvaluate the default instance of FalseNegativeRate on observations ŷ, given ground truth values y. \n\nAssigns false to first element of levels(y). To reverse roles, use FalseNegativeRate(rev=true).\n\nRequires scitype(y) to be a subtype of AbstractArray{<:OrderedFactor{2}} (binary classification where choice of \"true\" effects the measure); ŷ must be a deterministic prediction. \n\nFor more information, run info(FalseNegativeRate). \n\n\n\n\n\n","category":"type"},{"location":"utilities/#MLJBase.FalsePositive","page":"Utilities","title":"MLJBase.FalsePositive","text":"MLJBase.FalsePositive\n\nA measure type for number of false positives, which includes the instance(s), false_positive, falsepositive.\n\nFalsePositive()(ŷ, y)\n\nEvaluate the default instance of FalsePositive on observations ŷ, given ground truth values y. \n\nAssigns false to first element of levels(y). To reverse roles, use FalsePositive(rev=true).\n\nRequires scitype(y) to be a subtype of AbstractArray{<:OrderedFactor{2}} (binary classification where choice of \"true\" effects the measure); ŷ must be a deterministic prediction. \n\nFor more information, run info(FalsePositive). \n\n\n\n\n\n","category":"type"},{"location":"utilities/#MLJBase.FalsePositiveRate","page":"Utilities","title":"MLJBase.FalsePositiveRate","text":"MLJBase.FalsePositiveRate\n\nA measure type for false positive rate, which includes the instance(s), false_positive_rate, falsepositive_rate, fpr, fallout.\n\nFalsePositiveRate()(ŷ, y)\n\nEvaluate the default instance of FalsePositiveRate on observations ŷ, given ground truth values y. \n\nAssigns false to first element of levels(y). To reverse roles, use FalsePositiveRate(rev=true).\n\nRequires scitype(y) to be a subtype of AbstractArray{<:OrderedFactor{2}} (binary classification where choice of \"true\" effects the measure); ŷ must be a deterministic prediction. \n\nFor more information, run info(FalsePositiveRate). \n\n\n\n\n\n","category":"type"},{"location":"utilities/#MLJBase.LPLoss","page":"Utilities","title":"MLJBase.LPLoss","text":"MLJBase.LPLoss\n\nA measure type for lp loss, which includes the instance(s), l1, l2.\n\nLPLoss()(ŷ, y)\nLPLoss()(ŷ, y, w)\n\nEvaluate the default instance of LPLoss on observations ŷ, given ground truth values y. Optionally specify per-sample weights, w. \n\nConstructor signature: LPLoss(p=2). Reports |ŷ[i] - y[i]|^p for every index i.\n\nRequires scitype(y) to be a subtype of Union{AbstractArray{ScientificTypes.Continuous,1}, AbstractArray{ScientificTypes.Count,1}}; ŷ must be a deterministic prediction. \n\nFor more information, run info(LPLoss). \n\n\n\n\n\n","category":"type"},{"location":"utilities/#MLJBase.LogCoshLoss","page":"Utilities","title":"MLJBase.LogCoshLoss","text":"MLJBase.LogCoshLoss\n\nA measure type for log cosh loss, which includes the instance(s), log_cosh, log_cosh_loss.\n\nLogCoshLoss()(ŷ, y)\n\nEvaluate the log cosh loss on observations ŷ, given ground truth values y. \n\nReports log(cosh(yᵢ-yᵢ)) for each index i. \n\nRequires scitype(y) to be a subtype of Union{AbstractArray{ScientificTypes.Continuous,1}, AbstractArray{ScientificTypes.Count,1}}; ŷ must be a deterministic prediction. \n\nFor more information, run info(LogCoshLoss). \n\n\n\n\n\n","category":"type"},{"location":"utilities/#MLJBase.LogLoss","page":"Utilities","title":"MLJBase.LogLoss","text":"MLJBase.LogLoss\n\nA measure type for log loss, which includes the instance(s), log_loss, cross_entropy.\n\nLogLoss()(ŷ, y)\n\nEvaluate the default instance of LogLoss on observations ŷ, given ground truth values y. \n\nSince the score is undefined in the case that the true observation is predicted to occur with probability zero, probablities are clipped between tol and 1-tol, where tol is a constructor key-word argument.\n\nIf sᵢ is the predicted probability for the true class yᵢ then the score for that example is given by\n\n-log(clamp(sᵢ, tol), 1 - tol)\n\nA score is reported for every observation.\n\nRequires scitype(y) to be a subtype of AbstractArray{<:Finite} (multiclass classification); ŷ must be a probabilistic prediction. \n\nFor more information, run info(LogLoss). \n\n\n\n\n\n","category":"type"},{"location":"utilities/#MLJBase.MatthewsCorrelation","page":"Utilities","title":"MLJBase.MatthewsCorrelation","text":"MLJBase.MatthewsCorrelation\n\nA measure type for matthews correlation, which includes the instance(s), matthews_correlation, mcc.\n\nMatthewsCorrelation()(ŷ, y)\n\nEvaluate the matthews correlation on observations ŷ, given ground truth values y. \n\nhttps://en.wikipedia.org/wiki/Matthewscorrelationcoefficient This metric is invariant to class reordering.\n\nRequires scitype(y) to be a subtype of AbstractArray{<:Finite{2}} (binary classification); ŷ must be a deterministic prediction. \n\nFor more information, run info(MatthewsCorrelation). \n\n\n\n\n\n","category":"type"},{"location":"utilities/#MLJBase.MeanAbsoluteError","page":"Utilities","title":"MLJBase.MeanAbsoluteError","text":"MLJBase.MeanAbsoluteError\n\nA measure type for mean absolute error, which includes the instance(s), mae, mav, mean_absolute_error, mean_absolute_value.\n\nMeanAbsoluteError()(ŷ, y)\nMeanAbsoluteError()(ŷ, y, w)\n\nEvaluate the mean absolute error on observations ŷ, given ground truth values y. Optionally specify per-sample weights, w. \n\ntextmean absolute error =  n^-1ᵢyᵢ-ŷᵢ or textmean absolute error = n^-1ᵢwᵢyᵢ-ŷᵢ\n\nRequires scitype(y) to be a subtype of Union{AbstractArray{ScientificTypes.Continuous,1}, AbstractArray{ScientificTypes.Count,1}}; ŷ must be a deterministic prediction. \n\nFor more information, run info(MeanAbsoluteError). \n\n\n\n\n\n","category":"type"},{"location":"utilities/#MLJBase.MeanAbsoluteProportionalError","page":"Utilities","title":"MLJBase.MeanAbsoluteProportionalError","text":"MLJBase.MeanAbsoluteProportionalError\n\nA measure type for mean absolute proportional error, which includes the instance(s), mape.\n\nMeanAbsoluteProportionalError()(ŷ, y)\n\nEvaluate the default instance of MeanAbsoluteProportionalError on observations ŷ, given ground truth values y. \n\nConstructor key-word arguments: tol (default = eps()).\n\ntextmean absolute proportional error =  m^-1ᵢ(yᵢ-yᵢ) over yᵢ\n\nwhere the sum is over indices such that abs(yᵢ) > tol and m is the number of such indices.\n\nRequires scitype(y) to be a subtype of Union{AbstractArray{ScientificTypes.Continuous,1}, AbstractArray{ScientificTypes.Count,1}}; ŷ must be a deterministic prediction. \n\nFor more information, run info(MeanAbsoluteProportionalError). \n\n\n\n\n\n","category":"type"},{"location":"utilities/#MLJBase.MisclassificationRate","page":"Utilities","title":"MLJBase.MisclassificationRate","text":"MLJBase.MisclassificationRate\n\nA measure type for misclassification rate, which includes the instance(s), misclassification_rate, mcr.\n\nMisclassificationRate()(ŷ, y)\nMisclassificationRate()(ŷ, y, w)\n\nEvaluate the misclassification rate on observations ŷ, given ground truth values y. Optionally specify per-sample weights, w. \n\nA confusion matrix can also be passed as argument. This metric is invariant to class reordering.\n\nRequires scitype(y) to be a subtype of AbstractArray{<:Finite} (multiclass classification); ŷ must be a deterministic prediction. \n\nFor more information, run info(MisclassificationRate). \n\n\n\n\n\n","category":"type"},{"location":"utilities/#MLJBase.NegativePredictiveValue","page":"Utilities","title":"MLJBase.NegativePredictiveValue","text":"MLJBase.NegativePredictiveValue\n\nA measure type for negative predictive value, which includes the instance(s), negative_predictive_value, negativepredictive_value, npv.\n\nNegativePredictiveValue()(ŷ, y)\n\nEvaluate the default instance of NegativePredictiveValue on observations ŷ, given ground truth values y. \n\nAssigns false to first element of levels(y). To reverse roles, use NegativePredictiveValue(rev=true).\n\nRequires scitype(y) to be a subtype of AbstractArray{<:OrderedFactor{2}} (binary classification where choice of \"true\" effects the measure); ŷ must be a deterministic prediction. \n\nFor more information, run info(NegativePredictiveValue). \n\n\n\n\n\n","category":"type"},{"location":"utilities/#MLJBase.Precision","page":"Utilities","title":"MLJBase.Precision","text":"MLJBase.Precision\n\nA measure type for precision (a.k.a. positive predictive value), which includes the instance(s), positive_predictive_value, ppv, positivepredictive_value, precision.\n\nPrecision()(ŷ, y)\n\nEvaluate the default instance of Precision on observations ŷ, given ground truth values y. \n\nAssigns false to first element of levels(y). To reverse roles, use Precision(rev=true).\n\nRequires scitype(y) to be a subtype of AbstractArray{<:OrderedFactor{2}} (binary classification where choice of \"true\" effects the measure); ŷ must be a deterministic prediction. \n\nFor more information, run info(Precision). \n\n\n\n\n\n","category":"type"},{"location":"utilities/#MLJBase.RootMeanSquaredError","page":"Utilities","title":"MLJBase.RootMeanSquaredError","text":"MLJBase.RootMeanSquaredError\n\nA measure type for root mean squared error, which includes the instance(s), rms, rmse, root_mean_squared_error.\n\nRootMeanSquaredError()(ŷ, y)\nRootMeanSquaredError()(ŷ, y, w)\n\nEvaluate the root mean squared error on observations ŷ, given ground truth values y. Optionally specify per-sample weights, w. \n\ntextroot mean squared error = sqrtn^-1ᵢyᵢ-yᵢ^2 or textroot mean squared error = sqrtfracᵢwᵢyᵢ-yᵢ^2ᵢwᵢ\n\nRequires scitype(y) to be a subtype of Union{AbstractArray{ScientificTypes.Continuous,1}, AbstractArray{ScientificTypes.Count,1}}; ŷ must be a deterministic prediction. \n\nFor more information, run info(RootMeanSquaredError). \n\n\n\n\n\n","category":"type"},{"location":"utilities/#MLJBase.RootMeanSquaredLogError","page":"Utilities","title":"MLJBase.RootMeanSquaredLogError","text":"MLJBase.RootMeanSquaredLogError\n\nA measure type for root mean squared log error, which includes the instance(s), rmsl, rmsle, root_mean_squared_log_error.\n\nRootMeanSquaredLogError()(ŷ, y)\n\nEvaluate the root mean squared log error on observations ŷ, given ground truth values y. \n\ntextroot mean squared log error = n^-1ᵢlogleft(yᵢ over yᵢright)\n\nRequires scitype(y) to be a subtype of Union{AbstractArray{ScientificTypes.Continuous,1}, AbstractArray{ScientificTypes.Count,1}}; ŷ must be a deterministic prediction. \n\nSee also rmslp1.\n\nFor more information, run info(RootMeanSquaredLogError). \n\n\n\n\n\n","category":"type"},{"location":"utilities/#MLJBase.RootMeanSquaredLogProportionalError","page":"Utilities","title":"MLJBase.RootMeanSquaredLogProportionalError","text":"MLJBase.RootMeanSquaredLogProportionalError\n\nA measure type for root mean squared log proportional error, which includes the instance(s), rmslp1.\n\nRootMeanSquaredLogProportionalError()(ŷ, y)\n\nEvaluate the default instance of RootMeanSquaredLogProportionalError on observations ŷ, given ground truth values y. \n\nConstructor signature: RootMeanSquaredLogProportionalError(; offset = 1.0).\n\ntextroot mean squared log proportional error = n^-1ᵢlogleft(yᵢ + textoffset over yᵢ + textoffsetright)\n\nRequires scitype(y) to be a subtype of Union{AbstractArray{ScientificTypes.Continuous,1}, AbstractArray{ScientificTypes.Count,1}}; ŷ must be a deterministic prediction. \n\nSee also rmsl. \n\nFor more information, run info(RootMeanSquaredLogProportionalError). \n\n\n\n\n\n","category":"type"},{"location":"utilities/#MLJBase.RootMeanSquaredProportionalError","page":"Utilities","title":"MLJBase.RootMeanSquaredProportionalError","text":"MLJBase.RootMeanSquaredProportionalError\n\nA measure type for root mean squared proportional error, which includes the instance(s), rmsp.\n\nRootMeanSquaredProportionalError()(ŷ, y)\n\nEvaluate the default instance of RootMeanSquaredProportionalError on observations ŷ, given ground truth values y. \n\nConstructor keyword arguments: tol (default = eps()).\n\ntextroot mean squared proportional error = m^-1ᵢ left(yᵢ-yᵢ over yᵢright)^2\n\nwhere the sum is over indices such that abs(yᵢ) > tol and m is the number of such indices.\n\nRequires scitype(y) to be a subtype of Union{AbstractArray{ScientificTypes.Continuous,1}, AbstractArray{ScientificTypes.Count,1}}; ŷ must be a deterministic prediction. \n\nFor more information, run info(RootMeanSquaredProportionalError). \n\n\n\n\n\n","category":"type"},{"location":"utilities/#MLJBase.TrueNegative","page":"Utilities","title":"MLJBase.TrueNegative","text":"MLJBase.TrueNegative\n\nA measure type for number of true negatives, which includes the instance(s), true_negative, truenegative.\n\nTrueNegative()(ŷ, y)\n\nEvaluate the default instance of TrueNegative on observations ŷ, given ground truth values y. \n\nAssigns false to first element of levels(y). To reverse roles, use TrueNegative(rev=true).\n\nRequires scitype(y) to be a subtype of AbstractArray{<:OrderedFactor{2}} (binary classification where choice of \"true\" effects the measure); ŷ must be a deterministic prediction. \n\nFor more information, run info(TrueNegative). \n\n\n\n\n\n","category":"type"},{"location":"utilities/#MLJBase.TrueNegativeRate","page":"Utilities","title":"MLJBase.TrueNegativeRate","text":"MLJBase.TrueNegativeRate\n\nA measure type for true negative rate, which includes the instance(s), true_negative_rate, truenegative_rate, tnr, specificity, selectivity.\n\nTrueNegativeRate()(ŷ, y)\n\nEvaluate the default instance of TrueNegativeRate on observations ŷ, given ground truth values y. \n\nAssigns false to first element of levels(y). To reverse roles, use TrueNegativeRate(rev=true).\n\nRequires scitype(y) to be a subtype of AbstractArray{<:OrderedFactor{2}} (binary classification where choice of \"true\" effects the measure); ŷ must be a deterministic prediction. \n\nFor more information, run info(TrueNegativeRate). \n\n\n\n\n\n","category":"type"},{"location":"utilities/#MLJBase.TruePositive","page":"Utilities","title":"MLJBase.TruePositive","text":"MLJBase.TruePositive\n\nA measure type for number of true positives, which includes the instance(s), true_positive, truepositive.\n\nTruePositive()(ŷ, y)\n\nEvaluate the default instance of TruePositive on observations ŷ, given ground truth values y. \n\nAssigns false to first element of levels(y). To reverse roles, use TruePositive(rev=true).\n\nRequires scitype(y) to be a subtype of AbstractArray{<:OrderedFactor{2}} (binary classification where choice of \"true\" effects the measure); ŷ must be a deterministic prediction. \n\nFor more information, run info(TruePositive). \n\n\n\n\n\n","category":"type"},{"location":"utilities/#MLJBase.TruePositiveRate","page":"Utilities","title":"MLJBase.TruePositiveRate","text":"MLJBase.TruePositiveRate\n\nA measure type for true positive rate (a.k.a recall), which includes the instance(s), true_positive_rate, truepositive_rate, tpr, sensitivity, recall, hit_rate.\n\nTruePositiveRate()(ŷ, y)\n\nEvaluate the default instance of TruePositiveRate on observations ŷ, given ground truth values y. \n\nAssigns false to first element of levels(y). To reverse roles, use TruePositiveRate(rev=true).\n\nRequires scitype(y) to be a subtype of AbstractArray{<:OrderedFactor{2}} (binary classification where choice of \"true\" effects the measure); ŷ must be a deterministic prediction. \n\nFor more information, run info(TruePositiveRate). \n\n\n\n\n\n","category":"type"},{"location":"utilities/#MLJBase.flat_values-Tuple{NamedTuple}","page":"Utilities","title":"MLJBase.flat_values","text":"flat_values(t::NamedTuple)\n\nView a nested named tuple t as a tree and return, as a tuple, the values at the leaves, in the order they appear in the original tuple.\n\njulia> t = (X = (x = 1, y = 2), Y = 3)\njulia> flat_values(t)\n(1, 2, 3)\n\n\n\n\n\n","category":"method"},{"location":"utilities/#MLJBase.metadata_measure-Tuple{Any}","page":"Utilities","title":"MLJBase.metadata_measure","text":"metadata_measure(T; kw...)\n\nHelper function to write the metadata for a single measure.\n\n\n\n\n\n","category":"method"},{"location":"utilities/#MLJBase.recursive_getproperty-Tuple{Any,Symbol}","page":"Utilities","title":"MLJBase.recursive_getproperty","text":"recursive_getproperty(object, nested_name::Expr)\n\nCall getproperty recursively on object to extract the value of some nested property, as in the following example:\n\njulia> object = (X = (x = 1, y = 2), Y = 3)\njulia> recursive_getproperty(object, :(X.y))\n2\n\n\n\n\n\n","category":"method"},{"location":"utilities/#MLJBase.recursive_setproperty!-Tuple{Any,Symbol,Any}","page":"Utilities","title":"MLJBase.recursive_setproperty!","text":"recursively_setproperty!(object, nested_name::Expr, value)\n\nSet a nested property of an object to value, as in the following example:\n\njulia> mutable struct Foo\n           X\n           Y\n       end\n\njulia> mutable struct Bar\n           x\n           y\n       end\n\njulia> object = Foo(Bar(1, 2), 3)\nFoo(Bar(1, 2), 3)\n\njulia> recursively_setproperty!(object, :(X.y), 42)\n42\n\njulia> object\nFoo(Bar(1, 42), 3)\n\n\n\n\n\n","category":"method"},{"location":"utilities/#MLJBase.unwind-Tuple","page":"Utilities","title":"MLJBase.unwind","text":"unwind(iterators...)\n\nRepresent all possible combinations of values generated by iterators as rows of a matrix A. In more detail, A has one column for each iterator in iterators and one row for each distinct possible combination of values taken on by the iterators. Elements in the first column cycle fastest, those in the last clolumn slowest.\n\nExample\n\njulia> iterators = ([1, 2], [\"a\",\"b\"], [\"x\", \"y\", \"z\"]);\njulia> MLJTuning.unwind(iterators...)\n12×3 Array{Any,2}:\n 1  \"a\"  \"x\"\n 2  \"a\"  \"x\"\n 1  \"b\"  \"x\"\n 2  \"b\"  \"x\"\n 1  \"a\"  \"y\"\n 2  \"a\"  \"y\"\n 1  \"b\"  \"y\"\n 2  \"b\"  \"y\"\n 1  \"a\"  \"z\"\n 2  \"a\"  \"z\"\n 1  \"b\"  \"z\"\n 2  \"b\"  \"z\"\n\n\n\n\n\n","category":"method"},{"location":"utilities/#MLJBase._permute_rows-Tuple{Union{AbstractArray{T,1}, AbstractArray{T,2}} where T,Array{Int64,1}}","page":"Utilities","title":"MLJBase._permute_rows","text":"permuterows(obj, perm)\n\nInternal function to return a vector or matrix with permuted rows given the permutation perm.\n\n\n\n\n\n","category":"method"},{"location":"utilities/#MLJBase.available_name-Tuple{Any,Any}","page":"Utilities","title":"MLJBase.available_name","text":"available_name(modl::Module, name::Symbol)\n\nFunction to replace, if necessary, a given name with a modified one that ensures it is not the name of any existing object in the global scope of modl. Modifications are created with numerical suffixes.\n\n\n\n\n\n","category":"method"},{"location":"utilities/#MLJBase.check_dimensions-Tuple{Union{AbstractArray{T,1}, AbstractArray{T,2}} where T,Union{AbstractArray{T,1}, AbstractArray{T,2}} where T}","page":"Utilities","title":"MLJBase.check_dimensions","text":"check_dimension(X, Y)\n\nCheck that two vectors or matrices have matching dimensions\n\n\n\n\n\n","category":"method"},{"location":"utilities/#MLJBase.chunks-Tuple{AbstractRange,Integer}","page":"Utilities","title":"MLJBase.chunks","text":"chunks(range, n)\n\nSplit an AbstractRange  into n subranges of approximately equal length.\n\nExample\n\njulia> collect(chunks(1:5, 2))\n2-element Array{UnitRange{Int64},1}:\n 1:3\n 4:5\n\n**Private method**\n\n\n\n\n\n\n","category":"method"},{"location":"utilities/#MLJBase.shuffle_rows-Tuple{Union{AbstractArray{T,1}, AbstractArray{T,2}} where T,Union{AbstractArray{T,1}, AbstractArray{T,2}} where T}","page":"Utilities","title":"MLJBase.shuffle_rows","text":"shuffle_rows(X, Y, ...; rng=)\n\nReturn a shuffled view of a vector or  matrix X (or set of such) using a random permutation (which can be seeded specifying rng).\n\n\n\n\n\n","category":"method"},{"location":"resampling/#Resampling-1","page":"Resampling","title":"Resampling","text":"","category":"section"},{"location":"resampling/#","page":"Resampling","title":"Resampling","text":"Modules = [MLJBase]\nPages   = [\"resampling.jl\"]","category":"page"},{"location":"resampling/#MLJBase.CV","page":"Resampling","title":"MLJBase.CV","text":"cv = CV(; nfolds=6,  shuffle=nothing, rng=nothing)\n\nCross-validation resampling strategy, for use in evaluate!, evaluate and tuning.\n\ntrain_test_pairs(cv, rows)\n\nReturns an nfolds-length iterator of (train, test) pairs of vectors (row indices), where each train and test is a sub-vector of rows. The test vectors are mutually exclusive and exhaust rows. Each train vector is the complement of the corresponding test vector. With no row pre-shuffling, the order of rows is preserved, in the sense that rows coincides precisely with the concatenation of the test vectors, in the order they are generated. The first r test vectors have length n + 1, where n, r = divrem(length(rows), nfolds), and the remaining test vectors have length n.\n\nPre-shuffling of rows is controlled by rng and shuffle. If rng is an integer, then the CV keyword constructor resets it to MersenneTwister(rng). Otherwise some AbstractRNG object is expected.\n\nIf rng is left unspecified, rng is reset to Random.GLOBAL_RNG, in which case rows are only pre-shuffled if shuffle=true is explicitly specified.\n\n\n\n\n\n","category":"type"},{"location":"resampling/#MLJBase.Holdout","page":"Resampling","title":"MLJBase.Holdout","text":"holdout = Holdout(; fraction_train=0.7,\n                     shuffle=nothing,\n                     rng=nothing)\n\nHoldout resampling strategy, for use in evaluate!, evaluate and in tuning.\n\ntrain_test_pairs(holdout, rows)\n\nReturns the pair [(train, test)], where train and test are vectors such that rows=vcat(train, test) and length(train)/length(rows) is approximatey equal to fraction_train`.\n\nPre-shuffling of rows is controlled by rng and shuffle. If rng is an integer, then the Holdout keyword constructor resets it to MersenneTwister(rng). Otherwise some AbstractRNG object is expected.\n\nIf rng is left unspecified, rng is reset to Random.GLOBAL_RNG, in which case rows are only pre-shuffled if shuffle=true is specified.\n\n\n\n\n\n","category":"type"},{"location":"resampling/#MLJBase.Resampler","page":"Resampling","title":"MLJBase.Resampler","text":"resampler = Resampler(model=ConstantRegressor(),\n                      resampling=CV(),\n                      measure=nothing,\n                      weights=nothing,\n                      class_weights=nothing\n                      operation=predict,\n                      repeats = 1,\n                      acceleration=default_resource(),\n                      check_measure=true)\n\nResampling model wrapper, used internally by the fit method of TunedModel instances. See `evaluate! for options. Not intended for general use.\n\nGiven a machine mach = machine(resampler, args...) one obtains a performance evaluation of the specified model, performed according to the prescribed resampling strategy and other parameters, using data args..., by calling fit!(mach) followed by evaluate(mach). The advantage over using evaluate(model, X, y) is that the latter call always calls fit on the model but fit!(mach) only calls update after the first call.\n\nThe sample weights are passed to the specified performance measures that support weights for evaluation. These weights are not to be confused with any weights bound to a Resampler instance in a machine, used for training the wrapped model when supported.\n\nThe sample class_weights are passed to the specified performance measures that support per-class weights for evaluation. These weights are not to be confused with any weights bound to a Resampler instance in a machine, used for training the wrapped model when supported.\n\n\n\n\n\n","category":"type"},{"location":"resampling/#MLJBase.StratifiedCV","page":"Resampling","title":"MLJBase.StratifiedCV","text":"stratified_cv = StratifiedCV(; nfolds=6,\n                               shuffle=false,\n                               rng=Random.GLOBAL_RNG)\n\nStratified cross-validation resampling strategy, for use in evaluate!, evaluate and in tuning. Applies only to classification problems (OrderedFactor or Multiclass targets).\n\ntrain_test_pairs(stratified_cv, rows, y)\n\nReturns an nfolds-length iterator of (train, test) pairs of vectors (row indices) where each train and test is a sub-vector of rows. The test vectors are mutually exclusive and exhaust rows. Each train vector is the complement of the corresponding test vector.\n\nUnlike regular cross-validation, the distribution of the levels of the target y corresponding to each train and test is constrained, as far as possible, to replicate that of y[rows] as a whole.\n\nThe stratified train_test_pairs algorithm is invariant to label renaming. For example, if you run replace!(y, 'a' => 'b', 'b' => 'a') and then re-run train_test_pairs, the returned (train, test) pairs will be the same.\n\nPre-shuffling of rows is controlled by rng and shuffle. If rng is an integer, then the StratifedCV keyword constructor resets it to MersenneTwister(rng). Otherwise some AbstractRNG object is expected.\n\nIf rng is left unspecified, rng is reset to Random.GLOBAL_RNG, in which case rows are only pre-shuffled if shuffle=true is explicitly specified.\n\n\n\n\n\n","category":"type"},{"location":"resampling/#MLJBase.evaluate!-Tuple{Machine{var\"#s252\",C} where C where var\"#s252\"<:Supervised}","page":"Resampling","title":"MLJBase.evaluate!","text":"evaluate!(mach,\n          resampling=CV(),\n          measure=nothing,\n          rows=nothing,\n          weights=nothing,\n          class_weights=nothing,\n          operation=predict,\n          repeats=1,\n          acceleration=default_resource(),\n          force=false,\n          verbosity=1,\n          check_measure=true)\n\nEstimate the performance of a machine mach wrapping a supervised model in data, using the specified resampling strategy (defaulting to 6-fold cross-validation) and measure, which can be a single measure or vector.\n\nDo subtypes(MLJ.ResamplingStrategy) to obtain a list of available resampling strategies. If resampling is not an object of type MLJ.ResamplingStrategy, then a vector of pairs (of the form (train_rows, test_rows) is expected. For example, setting\n\nresampling = [(1:100), (101:200)),\n               (101:200), (1:100)]\n\ngives two-fold cross-validation using the first 200 rows of data.\n\nThe resampling strategy is applied repeatedly (Monte Carlo resampling) if repeats > 1. For example, if repeats = 10, then resampling = CV(nfolds=5, shuffle=true), generates a total of 50 (train, test) pairs for evaluation and subsequent aggregation.\n\nIf resampling isa MLJ.ResamplingStrategy then one may optionally restrict the data used in evaluation by specifying rows.\n\nAn optional weights vector may be passed for measures that support sample weights (MLJ.supports_weights(measure) == true), which is ignored by those that don't. These weights are not to be confused with any weights w bound to mach (as in mach = machine(model, X, y, w)). To pass these to the performance evaluation measures you must explictly specify weights=w in the evaluate! call.\n\nAdditionally, optional class_weights dictionary may be passed for measures that support class weights (MLJ.supports_class_weights(measure) == true), which is ignored by those that don't. These weights are not to be confused with any weights class_w bound to mach (as in mach = machine(model, X, y, class_w)). To pass these to the performance evaluation measures you must explictly specify class_weights=w in the evaluate! call.\n\nUser-defined measures are supported; see the manual for details.\n\nIf no measure is specified, then default_measure(mach.model) is used, unless this default is nothing and an error is thrown.\n\nThe acceleration keyword argument is used to specify the compute resource (a subtype of ComputationalResources.AbstractResource) that will be used to accelerate/parallelize the resampling operation.\n\nAlthough evaluate! is mutating, mach.model and mach.args are untouched.\n\nSummary of key-word arguments\n\nresampling - resampling strategy (default is CV(nfolds=6))\nmeasure/measures - measure or vector of measures (losses, scores, etc)\nrows - vector of observation indices from which both train and test folds are constructed (default is all observations)\nweights - per-sample weights for measures that support them (not to be confused with weights used in training)\nclass_weights - dictionary of per-class weights for use with measures that support these, in classification problems (not to be confused with per-sample weights or with class weights used in training)\noperation - predict, predict_mean, predict_mode or predict_median; predict is the default but cannot be used with a deterministic measure if model isa Probabilistic\nrepeats - default is 1; set to a higher value for repeated (Monte Carlo) resampling\nacceleration - parallelization option; currently supported  options are instances of CPU1 (single-threaded computation)  CPUThreads (multi-threaded computation) and CPUProcesses  (multi-process computation); default is default_resource().\nforce - default is false; set to true for force cold-restart of each training event\nverbosity level, an integer defaulting to 1.\ncheck_measure - default is true\n\nReturn value\n\nA property-accessible object of type PerformanceEvaluation with these properties:\n\nmeasure: the vector of specified measures\nmeasurements: the corresponding measurements, aggregated across the test folds using the aggregation method defined for each measure (do aggregation(measure) to inspect)\nper_fold: a vector of vectors of individual test fold evaluations (one vector per measure)\nper_observation: a vector of vectors of individual observation evaluations of those measures for which reports_each_observation(measure) is true, which is otherwise reported missing\n\n-fitted_params_per_fold: a vector containing fitted pamarms(mach) for each   machine mach trained during resampling.\n\nreport_per_fold: a vector containing report(mach) for each  machine mach training in resampling\n\n\n\n\n\n","category":"method"},{"location":"resampling/#MLJModelInterface.evaluate-Tuple{Supervised,Vararg{Any,N} where N}","page":"Resampling","title":"MLJModelInterface.evaluate","text":"evaluate(model, data...; cache=true, kw_options...)\n\nEquivalent to evaluate!(machine(model, data..., cache=cache); wk_options...).  See the machine version evaluate! for the complete list of options.\n\n\n\n\n\n","category":"method"},{"location":"composition/#Composition-1","page":"Composition","title":"Composition","text":"","category":"section"},{"location":"composition/#Composites-1","page":"Composition","title":"Composites","text":"","category":"section"},{"location":"composition/#","page":"Composition","title":"Composition","text":"Modules = [MLJBase]\nPages   = [\"composition/composites.jl\"]","category":"page"},{"location":"composition/#Networks-1","page":"Composition","title":"Networks","text":"","category":"section"},{"location":"composition/#","page":"Composition","title":"Composition","text":"Modules = [MLJBase]\nPages   = [\"composition/networks.jl\"]","category":"page"},{"location":"composition/#Pipelines-1","page":"Composition","title":"Pipelines","text":"","category":"section"},{"location":"composition/#","page":"Composition","title":"Composition","text":"Modules = [MLJBase]\nPages   = [\"composition/pipeline_static.jl\", \"composition/pipelines.jl\"]","category":"page"},{"location":"#MLJBase.jl-1","page":"Home","title":"MLJBase.jl","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"MLJ (Machine Learning in Julia) is a toolbox written in Julia providing a common interface and meta-algorithms for selecting, tuning, evaluating, composing and comparing machine learning models written in Julia and other languages.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"MLJ is released under the MIT licensed and sponsored by the Alan Turing Institute.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"This repository, MLJBase, provides core functionality for MLJ, including:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"completing the functionality for methods defined \"minimally\" in MLJ's light-weight model interface MLJModelInterface\ndefinition of machines and their associated methods, such as fit! and predict/transform\nMLJ's model composition interface, including learning networks and pipelines\nbasic utilities for manipulating data\nan extension to Distributions.jl called UnivariateFinite for randomly sampling labeled categorical data\na small interface for resampling strategies and implementations, including CV(), StratifiedCV and Holdout\nmethods for performance evaluation, based on those resampling strategies\none-dimensional hyperparameter range types, constructors and associated methods, for use with MLJTuning\na small interface for performance measures (losses and scores), enabling the integration of the LossFunctions.jl library, user-defined measures, as well as many natively defined measures.\nintegration with OpenML","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Previously MLJBase provided the model interface for integrating third party machine learning models into MLJ. That role has now shifted to the lightweight MLJModelInterface package.","category":"page"},{"location":"datasets/#Datasets-1","page":"Datasets","title":"Datasets","text":"","category":"section"},{"location":"datasets/#","page":"Datasets","title":"Datasets","text":"Pages   = [\"data/datasets_synthetic.jl\"]","category":"page"},{"location":"datasets/#Standard-datasets-1","page":"Datasets","title":"Standard datasets","text":"","category":"section"},{"location":"datasets/#","page":"Datasets","title":"Datasets","text":"To add a new dataset assuming it has a header and is, at path data/newdataset.csv","category":"page"},{"location":"datasets/#","page":"Datasets","title":"Datasets","text":"Start by loading it with CSV:","category":"page"},{"location":"datasets/#","page":"Datasets","title":"Datasets","text":"fpath = joinpath(\"datadir\", \"newdataset.csv\")\ndata = CSV.read(fpath, copycols=true,\n                categorical=true)","category":"page"},{"location":"datasets/#","page":"Datasets","title":"Datasets","text":"Load it with DelimitedFiles and Tables","category":"page"},{"location":"datasets/#","page":"Datasets","title":"Datasets","text":"data_raw, data_header = readdlm(fpath, ',', header=true)\ndata_table = Tables.table(data_raw; header=Symbol.(vec(data_header)))","category":"page"},{"location":"datasets/#","page":"Datasets","title":"Datasets","text":"Retrieve the conversions:","category":"page"},{"location":"datasets/#","page":"Datasets","title":"Datasets","text":"for (n, st) in zip(names(data), scitype_union.(eachcol(data)))\n    println(\":$n=>$st,\")\nend","category":"page"},{"location":"datasets/#","page":"Datasets","title":"Datasets","text":"Copy and paste the result in a coerce","category":"page"},{"location":"datasets/#","page":"Datasets","title":"Datasets","text":"data_table = coerce(data_table, ...)","category":"page"},{"location":"datasets/#","page":"Datasets","title":"Datasets","text":"Modules = [MLJBase]\nPages   = [\"data/datasets.jl\"]","category":"page"},{"location":"datasets/#MLJBase.@load_ames-Tuple{}","page":"Datasets","title":"MLJBase.@load_ames","text":"Load the full version of the well-known Ames Housing task.\n\n\n\n\n\n","category":"macro"},{"location":"datasets/#MLJBase.@load_boston-Tuple{}","page":"Datasets","title":"MLJBase.@load_boston","text":"Load a well-known public regression dataset with Continuous features.\n\n\n\n\n\n","category":"macro"},{"location":"datasets/#MLJBase.@load_crabs-Tuple{}","page":"Datasets","title":"MLJBase.@load_crabs","text":"Load a well-known crab classification dataset with nominal features.\n\n\n\n\n\n","category":"macro"},{"location":"datasets/#MLJBase.@load_iris-Tuple{}","page":"Datasets","title":"MLJBase.@load_iris","text":"Load a well-known public classification task with nominal features.\n\n\n\n\n\n","category":"macro"},{"location":"datasets/#MLJBase.@load_reduced_ames-Tuple{}","page":"Datasets","title":"MLJBase.@load_reduced_ames","text":"Load a reduced version of the well-known Ames Housing task\n\n\n\n\n\n","category":"macro"},{"location":"datasets/#MLJBase.@load_smarket-Tuple{}","page":"Datasets","title":"MLJBase.@load_smarket","text":"Load S&P Stock Market dataset, as used in (An Introduction to Statistical Learning with applications in R)https://rdrr.io/cran/ISLR/man/Smarket.html, by Witten et al (2013), Springer-Verlag, New York.\n\n\n\n\n\n","category":"macro"},{"location":"datasets/#MLJBase.load_dataset-Tuple{String,Tuple}","page":"Datasets","title":"MLJBase.load_dataset","text":"load_dataset(fpath, coercions)\n\nLoad one of standard dataset like Boston etc assuming the file is a comma separated file with a header.\n\n\n\n\n\n","category":"method"},{"location":"datasets/#Synthetic-datasets-1","page":"Datasets","title":"Synthetic datasets","text":"","category":"section"},{"location":"datasets/#","page":"Datasets","title":"Datasets","text":"Modules = [MLJBase]\nPages   = [\"data/datasets_synthetic.jl\"]","category":"page"},{"location":"datasets/#MLJBase.make_blobs","page":"Datasets","title":"MLJBase.make_blobs","text":"X, y = make_blobs(n=100, p=2; kwargs...)\n\nGenerate Gaussian blobs for clustering and classification problems.\n\nReturn value\n\nBy default, a table X with p columns (features) and n rows (observations), together with a corresponding vector of n Multiclass target observations y, indicating blob membership.\n\nKeyword arguments\n\nshuffle=true: whether to shuffle the resulting points,\ncenters=3: either a number of centers or a c x p matrix with c pre-determined centers,\ncluster_std=1.0: the standard deviation(s) of each blob,\ncenter_box=(-10. => 10.): the limits of the p-dimensional cube within which the cluster centers are drawn if they are not provided,\neltype=Float64: machine type of points (any subtype of  AbstractFloat).\nrng=nothing: any AbstractRNG object, or integer to seed a MersenneTwister (for reproducibility).\nas_table=true: whether to return the points as a table (true) or a matrix (false). If false the target y has integer element type. \n\nExample\n\nX, y = make_blobs(100, 3; centers=2, cluster_std=[1.0, 3.0])\n\n\n\n\n\n","category":"function"},{"location":"datasets/#MLJBase.make_circles","page":"Datasets","title":"MLJBase.make_circles","text":"X, y = make_circles(n=100; kwargs...)\n\nGenerate n labeled points close to two concentric circles for classification and clustering models.\n\nReturn value\n\nBy default, a table X with 2 columns and n rows (observations), together with a corresponding vector of n Multiclass target observations y. The target is either 0 or 1, corresponding to membership to the smaller or larger circle, respectively.\n\nKeyword arguments\n\nshuffle=true: whether to shuffle the resulting points,\nnoise=0: standard deviation of the Gaussian noise added to the data,\nfactor=0.8: ratio of the smaller radius over the larger one,\n\neltype=Float64: machine type of points (any subtype of  AbstractFloat).\nrng=nothing: any AbstractRNG object, or integer to seed a MersenneTwister (for reproducibility).\nas_table=true: whether to return the points as a table (true) or a matrix (false). If false the target y has integer element type. \n\nExample\n\nX, y = make_circles(100; noise=0.5, factor=0.3)\n\n\n\n\n\n","category":"function"},{"location":"datasets/#MLJBase.make_moons","page":"Datasets","title":"MLJBase.make_moons","text":"    make_moons(n::Int=100; kwargs...)\n\nGenerates labeled two-dimensional points lying close to two interleaved semi-circles, for use with classification and clustering models.\n\nReturn value\n\nBy default, a table X with 2 columns and n rows (observations), together with a corresponding vector of n Multiclass target observations y. The target is either 0 or 1, corresponding to membership to the left or right semi-circle.\n\nKeyword arguments\n\nshuffle=true: whether to shuffle the resulting points,\nnoise=0.1: standard deviation of the Gaussian noise added to the data,\nxshift=1.0: horizontal translation of the second center with respect to the first one.\nyshift=0.3: vertical translation of the second center with respect to the first one.  \neltype=Float64: machine type of points (any subtype of  AbstractFloat).\nrng=nothing: any AbstractRNG object, or integer to seed a MersenneTwister (for reproducibility).\nas_table=true: whether to return the points as a table (true) or a matrix (false). If false the target y has integer element type. \n\nExample\n\nX, y = make_moons(100; noise=0.5)\n\n\n\n\n\n","category":"function"},{"location":"datasets/#MLJBase.make_regression","page":"Datasets","title":"MLJBase.make_regression","text":"make_regression(n, p; kwargs...)\n\nGenerate Gaussian input features and a linear response with Gaussian noise, for use with regression models.\n\nReturn value\n\nBy default, a table X with p columns and n rows (observations), together with a corresponding vector of n Continuous target observations y.\n\nKeywords\n\n`intercept=true: whether to generate data from a model with intercept,\nsparse=0: portion of the generating weight vector that is sparse,\nnoise=0.1: standard deviation of the Gaussian noise added to the response,\noutliers=0: portion of the response vector to make as outliers by adding a random quantity with high variance. (Only applied if binary is false)\nbinary=false: whether the target should be binarized (via a sigmoid).\neltype=Float64: machine type of points (any subtype of  AbstractFloat).\nrng=nothing: any AbstractRNG object, or integer to seed a MersenneTwister (for reproducibility).\nas_table=true: whether to return the points as a table (true) or a matrix (false). \n\nExample\n\nX, y = make_regression(100, 5; noise=0.5, sparse=0.2, outliers=0.1)\n\n\n\n\n\n","category":"function"},{"location":"datasets/#MLJBase.augment_X-Tuple{Array{var\"#s583\",2} where var\"#s583\"<:Real,Bool}","page":"Datasets","title":"MLJBase.augment_X","text":"augment_X(X, fit_intercept)\n\nGiven a matrix X, append a column of ones if fit_intercept is true. See make_regression.\n\n\n\n\n\n","category":"method"},{"location":"datasets/#MLJBase.finalize_Xy-NTuple{6,Any}","page":"Datasets","title":"MLJBase.finalize_Xy","text":"finalize_Xy(X, y, shuffle, as_table, eltype, rng; clf)\n\nInternal function to  finalize the make_* functions.\n\n\n\n\n\n","category":"method"},{"location":"datasets/#MLJBase.outlify!-Tuple{Any,Any,Any}","page":"Datasets","title":"MLJBase.outlify!","text":"Add outliers to portion s of vector.\n\n\n\n\n\n","category":"method"},{"location":"datasets/#MLJBase.runif_ab-NTuple{5,Any}","page":"Datasets","title":"MLJBase.runif_ab","text":"runif_ab(rng, n, p, a, b)\n\nInternal function to generate n points in [a, b]ᵖ uniformly at random.\n\n\n\n\n\n","category":"method"},{"location":"datasets/#MLJBase.sigmoid-Tuple{Float64}","page":"Datasets","title":"MLJBase.sigmoid","text":"sigmoid(x)\n\nReturn the sigmoid computed in a numerically stable way:\n\nσ(x) = 1(1+exp(-x))\n\n\n\n\n\n","category":"method"},{"location":"datasets/#MLJBase.sparsify!-Tuple{Any,Any,Any}","page":"Datasets","title":"MLJBase.sparsify!","text":"sparsify!(rng, θ, s)\n\nMake portion s of vector θ exactly 0.\n\n\n\n\n\n","category":"method"},{"location":"datasets/#Utility-functions-1","page":"Datasets","title":"Utility functions","text":"","category":"section"},{"location":"datasets/#","page":"Datasets","title":"Datasets","text":"Modules = [MLJBase]\nPages   = [\"data/data.jl\"]","category":"page"},{"location":"datasets/#MLJBase.complement-Tuple{Any,Any}","page":"Datasets","title":"MLJBase.complement","text":"complement(folds, i)\n\nThe complement of the ith fold of folds in the concatenation of all elements of folds. Here folds is a vector or tuple of integer vectors, typically representing row indices or a vector, matrix or table.\n\ncomplement(([1,2], [3,], [4, 5]), 2) # [1 ,2, 4, 5]\n\n\n\n\n\n","category":"method"},{"location":"datasets/#MLJBase.corestrict-Union{Tuple{N}, Tuple{Tuple{Vararg{T,N}} where T,Any}} where N","page":"Datasets","title":"MLJBase.corestrict","text":"corestrict(X, folds, i)\n\nThe restriction of X, a vector, matrix or table, to the complement of the ith fold of folds, where folds is a tuple of vectors of row indices.\n\nThe method is curried, so that corestrict(folds, i) is the operator on data defined by corestrict(folds, i)(X) = corestrict(X, folds, i).\n\nExample\n\nfolds = ([1, 2], [3, 4, 5],  [6,])\ncorestrict([:x1, :x2, :x3, :x4, :x5, :x6], folds, 2) # [:x1, :x2, :x6]\n\n\n\n\n\n","category":"method"},{"location":"datasets/#MLJBase.partition-Tuple{AbstractArray{Int64,1},Vararg{Real,N} where N}","page":"Datasets","title":"MLJBase.partition","text":"partition(rows::AbstractVector{Int}, fractions...;\n          shuffle=nothing, rng=Random.GLOBAL_RNG, stratify=nothing)\n\nSplits the vector rows into a tuple of vectors whose lengths are given by the corresponding fractions of length(rows) where valid fractions are in (0,1) and sum up to less than 1. The last fraction is not provided, as it is inferred from the preceding ones. So, for example,\n\njulia> partition(1:1000, 0.8)\n([1,...,800], [801,...,1000])\n\njulia> partition(1:1000, 0.2, 0.7)\n([1,...,200], [201,...,900], [901,...,1000])\n\nKeywords\n\nshuffle=nothing:        if set to  true, shuffles the rows before taking fractions.\nrng=Random.GLOBAL_RNG:  specifies the random number generator to be used, can be an integer                           seed. If specified, and shuffle === nothing is interpreted as true.\nstratify=nothing:       if a vector is specified, the partition will match the stratification                           of the given vector. In that case, shuffle cannot be false.\n\n\n\n\n\n","category":"method"},{"location":"datasets/#MLJBase.restrict-Union{Tuple{N}, Tuple{Tuple{Vararg{T,N}} where T,Any}} where N","page":"Datasets","title":"MLJBase.restrict","text":"restrict(X, folds, i)\n\nThe restriction of X, a vector, matrix or table, to the ith fold of folds, where folds is a tuple of vectors of row indices.\n\nThe method is curried, so that restrict(folds, i) is the operator on data defined by restrict(folds, i)(X) = restrict(X, folds, i).\n\nExample\n\nfolds = ([1, 2], [3, 4, 5],  [6,])\nrestrict([:x1, :x2, :x3, :x4, :x5, :x6], folds, 2) # [:x3, :x4, :x5]\n\nSee also corestrict\n\n\n\n\n\n","category":"method"},{"location":"datasets/#MLJBase.unpack-Tuple{Any,Vararg{Any,N} where N}","page":"Datasets","title":"MLJBase.unpack","text":"t1, t2, ...., tk = unnpack(table, f1, f2, ... fk;\n                           wrap_singles=false,\n                           shuffle=false,\n                           rng::Union{AbstractRNG,Int,Nothing}=nothing)\n\nSplit any Tables.jl compatible table into smaller tables (or vectors) t1, t2, ..., tk by making selections without replacement from the column names defined by the filters f1, f2, ..., fk. A filter is any object f such that f(name) is true or false for each column name::Symbol of table.\n\nWhenever a returned table contains a single column, it is converted to a vector unless wrap_singles=true.\n\nScientific type conversions can be optionally specified (note semicolon):\n\nunpack(table, t...; wrap_singles=false, col1=>scitype1, col2=>scitype2, ... )\n\nIf shuffle=true then the rows of table are first shuffled, using the global RNG, unless rng is specified; if rng is an integer, it specifies the seed of an automatically generated Mersenne twister. If rng is specified then shuffle=true is implicit.\n\nExample\n\njulia> table = DataFrame(x=[1,2], y=['a', 'b'], z=[10.0, 20.0], w=[\"A\", \"B\"])\njulia> Z, XY = unpack(table, ==(:z), !=(:w);\n               :x=>Continuous, :y=>Multiclass)\njulia> XY\n2×2 DataFrame\n│ Row │ x       │ y            │\n│     │ Float64 │ Categorical… │\n├─────┼─────────┼──────────────┤\n│ 1   │ 1.0     │ 'a'          │\n│ 2   │ 2.0     │ 'b'          │\n\njulia> Z\n2-element Array{Float64,1}:\n 10.0\n 20.0\n\n\n\n\n\n","category":"method"},{"location":"datasets/#MLJModelInterface.transform-Tuple{Union{CategoricalArrays.CategoricalArray, CategoricalArrays.CategoricalValue},Any}","page":"Datasets","title":"MLJModelInterface.transform","text":"transform(e::Union{CategoricalElement,CategoricalArray,CategoricalPool},  X)\n\nTransform the specified object X into a categorical version, using the pool contained in e. Here X is a raw value (an element of levels(e)) or an AbstractArray of such values.\n\n```julia v = categorical([\"x\", \"y\", \"y\", \"x\", \"x\"]) julia> transform(v, \"x\") CategoricalValue{String,UInt32} \"x\"\n\njulia> transform(v[1], [\"x\" \"x\"; missing \"y\"]) 2×2 CategoricalArray{Union{Missing, Symbol},2,UInt32}:  \"x\"       \"x\"  missing   \"y\"\n\n\n\n\n\n","category":"method"}]
}
