var documenterSearchIndex = {"docs":
[{"location":"openml/#OpenML-1","page":"OpenML","title":"OpenML","text":"","category":"section"},{"location":"openml/#","page":"OpenML","title":"OpenML","text":"Modules = [MLJBase, OpenML]\nPages   = [\"openml.jl\"]","category":"page"},{"location":"openml/#MLJBase.OpenML.convert_ARFF_to_rowtable-Tuple{Any}","page":"OpenML","title":"MLJBase.OpenML.convert_ARFF_to_rowtable","text":"Returns a Vector of NamedTuples. Receives an HTTP.Message.response that has an ARFF file format in the body of the Message.\n\n\n\n\n\n","category":"method"},{"location":"openml/#MLJBase.OpenML.load-Tuple{Int64}","page":"OpenML","title":"MLJBase.OpenML.load","text":"OpenML.load(id)\n\nLoad the OpenML dataset with specified id, from those listed on the OpenML site.\n\nReturns a \"row table\", i.e., a Vector of identically typed NamedTuples. A row table is compatible with the Tables.jl interface and can therefore be readily converted to other compatible formats. For example:\n\nusing DataFrames\nrowtable = OpenML.load(61);\ndf = DataFrame(rowtable);\ndf2 = coerce(df, :class=>Multiclass)\n\n\n\n\n\n","category":"method"},{"location":"openml/#MLJBase.OpenML.load_Data_Features-Tuple{Int64}","page":"OpenML","title":"MLJBase.OpenML.load_Data_Features","text":"Returns a list of all data qualities in the system.\n\n271 - Unknown dataset. Data set with the given data ID was not found (or is not shared with you).\n272 - No features found. The dataset did not contain any features, or we could not extract them.\n273 - Dataset not processed yet. The dataset was not processed yet, features are not yet available. Please wait for a few minutes.\n274 - Dataset processed with error. The feature extractor has run into an error while processing the dataset. Please check whether it is a valid supported file. If so, please contact the API admins.\n\n\n\n\n\n","category":"method"},{"location":"openml/#MLJBase.OpenML.load_Data_Qualities-Tuple{Int64}","page":"OpenML","title":"MLJBase.OpenML.load_Data_Qualities","text":"Returns the qualities of a dataset.\n\n360 - Please provide data set ID\n361 - Unknown dataset. The data set with the given ID was not found in the database, or is not shared with you.\n362 - No qualities found. The registered dataset did not contain any calculated qualities.\n363 - Dataset not processed yet. The dataset was not processed yet, no qualities are available. Please wait for a few minutes.\n364 - Dataset processed with error. The quality calculator has run into an error while processing the dataset. Please check whether it is a valid supported file. If so, contact the support team.\n365 - Interval start or end illegal. There was a problem with the interval start or end.\n\n\n\n\n\n","category":"method"},{"location":"openml/#MLJBase.OpenML.load_Data_Qualities_List-Tuple{}","page":"OpenML","title":"MLJBase.OpenML.load_Data_Qualities_List","text":"Returns a list of all data qualities in the system.\n\n412 - Precondition failed. An error code and message are returned\n370 - No data qualities available. There are no data qualities in the system.\n\n\n\n\n\n","category":"method"},{"location":"openml/#MLJBase.OpenML.load_Dataset_Description-Tuple{Int64}","page":"OpenML","title":"MLJBase.OpenML.load_Dataset_Description","text":"Returns information about a dataset. The information includes the name, information about the creator, URL to download it and more.\n\n110 - Please provide data_id.\n111 - Unknown dataset. Data set description with data_id was not found in the database.\n112 - No access granted. This dataset is not shared with you.\n\n\n\n\n\n","category":"method"},{"location":"openml/#MLJBase.OpenML.load_List_And_Filter-Tuple{String}","page":"OpenML","title":"MLJBase.OpenML.load_List_And_Filter","text":"List datasets, possibly filtered by a range of properties. Any number of properties can be combined by listing them one after the other in the form '/data/list/{filter}/{value}/{filter}/{value}/...' Returns an array with all datasets that match the constraints.\n\nAny combination of these filters /limit/{limit}/offset/{offset} - returns only {limit} results starting from result number {offset}. Useful for paginating results. With /limit/5/offset/10,     results 11..15 will be returned.\n\nBoth limit and offset need to be specified. /status/{status} - returns only datasets with a given status, either 'active', 'deactivated', or 'inpreparation'. /tag/{tag} - returns only datasets tagged with the given tag. /{dataquality}/{range} - returns only tasks for which the underlying datasets have certain qualities. {dataquality} can be dataid, dataname, dataversion, numberinstances, numberfeatures, numberclasses, numbermissingvalues. {range} can be a specific value or a range in the form 'low..high'. Multiple qualities can be combined, as in 'numberinstances/0..50/number_features/0..10'.\n\n370 - Illegal filter specified.\n371 - Filter values/ranges not properly specified.\n372 - No results. There where no matches for the given constraints.\n373 - Can not specify an offset without a limit.\n\n\n\n\n\n","category":"method"},{"location":"measures/#Measures-1","page":"Measures","title":"Measures","text":"","category":"section"},{"location":"measures/#Helper-functions-1","page":"Measures","title":"Helper functions","text":"","category":"section"},{"location":"measures/#","page":"Measures","title":"Measures","text":"Modules = [MLJBase]\nPages   = [\"measures/registry.jl\", \"measures/measures.jl\"]","category":"page"},{"location":"measures/#MLJBase.measures-Tuple","page":"Measures","title":"MLJBase.measures","text":"measures()\n\nList all measures as named-tuples keyed on measure traits.\n\nmeasures(conditions...)\n\nList all measures satisifying the specified conditions. A condition is any Bool-valued function on the named-tuples.\n\nExample\n\nFind all classification measures supporting sample weights:\n\nmeasures(m -> m.target_scitype <: AbstractVector{<:Finite} &&\n              m.supports_weights)\n\n\n\n\n\n","category":"method"},{"location":"measures/#MLJBase.metadata_measure-Tuple{Any}","page":"Measures","title":"MLJBase.metadata_measure","text":"metadata_measure(T; kw...)\n\nHelper function to write the metadata for a single measure.\n\n\n\n\n\n","category":"method"},{"location":"measures/#Continuous-loss-functions-1","page":"Measures","title":"Continuous loss functions","text":"","category":"section"},{"location":"measures/#","page":"Measures","title":"Measures","text":"Modules = [MLJBase]\nPages   = [\"measures/continuous.jl\"]","category":"page"},{"location":"measures/#MLJBase.l1","page":"Measures","title":"MLJBase.l1","text":"l1(ŷ, y)\nl1(ŷ, y, w)\n\nL1 per-observation loss.\n\nFor more information, run info(l1).\n\n\n\n\n\n","category":"constant"},{"location":"measures/#MLJBase.l2","page":"Measures","title":"MLJBase.l2","text":"l2(ŷ, y)\nl2(ŷ, y, w)\n\nL2 per-observation loss.\n\nFor more information, run info(l2).\n\n\n\n\n\n","category":"constant"},{"location":"measures/#MLJBase.mae","page":"Measures","title":"MLJBase.mae","text":"mae(ŷ, y)\nmae(ŷ, y, w)\n\nMean absolute error.\n\ntextMAE =  n^-1ᵢyᵢ-ŷᵢ or textMAE = n^-1ᵢwᵢyᵢ-ŷᵢ\n\nFor more information, run info(mae).\n\n\n\n\n\n","category":"constant"},{"location":"measures/#MLJBase.rms","page":"Measures","title":"MLJBase.rms","text":"rms(ŷ, y)\nrms(ŷ, y, w)\n\nRoot mean squared error:\n\ntextRMS = sqrtn^-1ᵢyᵢ-yᵢ^2 or textRMS = sqrtfracᵢwᵢyᵢ-yᵢ^2ᵢwᵢ\n\nFor more information, run info(rms).\n\n\n\n\n\n","category":"constant"},{"location":"measures/#MLJBase.rmsl","page":"Measures","title":"MLJBase.rmsl","text":"rmsl(ŷ, y)\n\nRoot mean squared logarithmic error:\n\ntextRMSL = n^-1ᵢlogleft(yᵢ over yᵢright)\n\nFor more information, run info(rmsl).\n\nSee also rmslp1.\n\n\n\n\n\n","category":"constant"},{"location":"measures/#MLJBase.rmslp1","page":"Measures","title":"MLJBase.rmslp1","text":"rmslp1(ŷ, y)\n\nRoot mean squared logarithmic error with an offset of 1:\n\ntextRMSLP1 = n^-1ᵢlogleft(yᵢ + 1 over yᵢ + 1right)\n\nFor more information, run info(rmslp1).\n\nSee also rmsl.\n\n\n\n\n\n","category":"constant"},{"location":"measures/#MLJBase.rmsp","page":"Measures","title":"MLJBase.rmsp","text":"rmsp(ŷ, y)\n\nRoot mean squared percentage loss:\n\ntextRMSP = m^-1ᵢ left(yᵢ-yᵢ over yᵢright)^2\n\nwhere the sum is over indices such that yᵢ≂̸0 and m is the number of such indices.\n\nFor more information, run info(rmsp).\n\n\n\n\n\n","category":"constant"},{"location":"measures/#Confusion-matrix-1","page":"Measures","title":"Confusion matrix","text":"","category":"section"},{"location":"measures/#","page":"Measures","title":"Measures","text":"Modules = [MLJBase]\nPages   = [\"measures/confusion_matrix.jl\"]","category":"page"},{"location":"measures/#MLJBase.confusion_matrix-Tuple{AbstractArray{#s348,1} where #s348<:Union{CategoricalArrays.CategoricalString, CategoricalArrays.CategoricalValue},AbstractArray{#s347,1} where #s347<:Union{CategoricalArrays.CategoricalString, CategoricalArrays.CategoricalValue}}","page":"Measures","title":"MLJBase.confusion_matrix","text":"confusion_matrix(ŷ, y; rev=false)\n\nComputes the confusion matrix given a predicted ŷ with categorical elements and the actual y. Rows are the predicted class, columns the ground truth. The ordering follows that of levels(y).\n\nKeywords\n\nrev=false: in the binary case, this keyword allows to swap the ordering of              classes.\nperm=[]:   in the general case, this keyword allows to specify a permutation              re-ordering the classes.\nwarn=true: whether to show a warning in case y does not have scientific              type OrderedFactor{2} (see note below).\n\nNote\n\nTo decrease the risk of unexpected errors, if y does not have scientific type OrderedFactor{2} (and so does not have a \"natural ordering\" negative-positive), a warning is shown indicating the current order unless the user explicitly specifies either rev or perm in which case it's assumed the user is aware of the class ordering.\n\nThe confusion_matrix is a measure (although neither a score nor a loss) and so may be specified as such in calls to evaluate, evaluate!, although not in TunedModels.  In this case, however, there no way to specify an ordering different from levels(y), where y is the target. \n\n\n\n\n\n","category":"method"},{"location":"measures/#MLJBase.ConfusionMatrix","page":"Measures","title":"MLJBase.ConfusionMatrix","text":"ConfusionMatrix{C}\n\nConfusion matrix with C ≥ 2 classes. Rows correspond to predicted values and columns to the ground truth.\n\n\n\n\n\n","category":"type"},{"location":"measures/#MLJBase.ConfusionMatrix-Tuple{Array{Int64,2},Array{String,1}}","page":"Measures","title":"MLJBase.ConfusionMatrix","text":"ConfusionMatrix(m, labels)\n\nInstantiates a confusion matrix out of a square integer matrix m. Rows are the predicted class, columns the ground truth. See also the wikipedia article.\n\n\n\n\n\n","category":"method"},{"location":"measures/#Finite-loss-functions-1","page":"Measures","title":"Finite loss functions","text":"","category":"section"},{"location":"measures/#","page":"Measures","title":"Measures","text":"Modules = [MLJBase]\nPages   = [\"measures/finite.jl\"]","category":"page"},{"location":"measures/#MLJBase.accuracy","page":"Measures","title":"MLJBase.accuracy","text":"accuracy\n\nClassification accuracy; aliases: accuracy.\n\naccuracy(ŷ, y)\naccuracy(ŷ, y, w)\naccuracy(conf_mat)\n\nReturns the accuracy of the (point) predictions ŷ, given true observations y, optionally weighted by the weights w. All three arguments must be abstract vectors of the same length. This metric is invariant to class labelling and can be used for multiclass classification.\n\nFor more information, run info(accuracy).\n\n\n\n\n\n","category":"constant"},{"location":"measures/#MLJBase.area_under_curve","page":"Measures","title":"MLJBase.area_under_curve","text":"area_under_curve\n\nArea under the ROC curve; aliases: area_under_curve, auc\n\narea_under_curve(ŷ, y)\n\nReturn the area under the receiver operator characteristic (curve), for probabilistic predictions ŷ, given ground truth y. This metric is invariant to class labelling and can be used only for binary classification.\n\nFor more information, run info(area_under_curve).\n\n\n\n\n\n","category":"constant"},{"location":"measures/#MLJBase.balanced_accuracy","page":"Measures","title":"MLJBase.balanced_accuracy","text":"balanced_accuracy\n\nBalanced classification accuracy; aliases: balanced_accuracy, bacc, bac.\n\nbalanced_accuracy(ŷ, y [, w])\nbalanced_accuracy(conf_mat)\n\nReturn the balanced accuracy of the point prediction ŷ, given true observations y, optionally weighted by w. The balanced accuracy takes into consideration class imbalance. All  three arguments must have the same length. This metric is invariant to class labelling and can be used for multiclass classification.\n\nFor more information, run info(balanced_accuracy).\n\n\n\n\n\n","category":"constant"},{"location":"measures/#MLJBase.cross_entropy","page":"Measures","title":"MLJBase.cross_entropy","text":"cross_entropy\n\nCross entropy loss with probabilities clamped between eps and 1-eps; aliases: cross_entropy.\n\nCrossEntropy(; eps=eps())\nce(ŷ, y)\n\nGiven an abstract vector of distributions ŷ and an abstract vector of true observations y, return the corresponding Cross-Entropy loss (aka log loss) scores.\n\nSince the score is undefined in the case of the true observation has predicted probability zero, probablities are clipped between eps and 1-eps where eps can be specified.\n\nIf sᵢ is the predicted probability for the true class yᵢ then the score for that example is given by\n\n-log(clamp(sᵢ, eps, 1-eps))\n\nFor more information, run info(cross_entropy).\n\n\n\n\n\n","category":"constant"},{"location":"measures/#MLJBase.false_discovery_rate","page":"Measures","title":"MLJBase.false_discovery_rate","text":"false_discovery_rate\n\nfalse discovery rate; aliases: false_discovery_rate, falsediscovery_rate, fdr.\n\nfalse_discovery_rate(ŷ, y)\n\nFalse discovery rate for observations ŷ and ground truth y. Assigns false to first element of levels(y). To reverse roles, use FalseDiscoveryRate(rev=true) instead of false_discovery_rate.\n\nFor more information, run info(false_discovery_rate).\n\n\n\n\n\n","category":"constant"},{"location":"measures/#MLJBase.false_negative","page":"Measures","title":"MLJBase.false_negative","text":"false_negative\n\nNumber of false negatives; aliases: false_negative, falsenegative.\n\nfalse_negative(ŷ, y)\n\nNumber of false positives for observations ŷ and ground truth y. Assigns false to first element of levels(y). To reverse roles, use FalseNegative(rev=true) instead of false_negative.\n\nFor more information, run info(false_negative).\n\n\n\n\n\n","category":"constant"},{"location":"measures/#MLJBase.false_negative_rate","page":"Measures","title":"MLJBase.false_negative_rate","text":"false_negative_rate\n\nfalse negative rate; aliases: false_negative_rate, falsenegative_rate, fnr, miss_rate.\n\nfalse_negative_rate(ŷ, y)\n\nFalse negative rate for observations ŷ and ground truth y. Assigns false to first element of levels(y). To reverse roles, use FalseNegativeRate(rev=true) instead of false_negative_rate.\n\nFor more information, run info(false_negative_rate).\n\n\n\n\n\n","category":"constant"},{"location":"measures/#MLJBase.false_positive","page":"Measures","title":"MLJBase.false_positive","text":"false_positive\n\nNumber of false positives; aliases: false_positive, falsepositive.\n\nfalse_positive(ŷ, y)\n\nNumber of false positives for observations ŷ and ground truth y. Assigns false to first element of levels(y). To reverse roles, use FalsePositive(rev=true) instead of false_positive.\n\nFor more information, run info(false_positive).\n\n\n\n\n\n","category":"constant"},{"location":"measures/#MLJBase.false_positive_rate","page":"Measures","title":"MLJBase.false_positive_rate","text":"false_positive_rate\n\nfalse positive rate; aliases: false_positive_rate, falsepositive_rate, fpr, fallout.\n\nfalse_positive_rate(ŷ, y)\n\nFalse positive rate for observations ŷ and ground truth y. Assigns false to first element of levels(y). To reverse roles, use FalsePositiveRate(rev=true) instead of false_positive_rate.\n\nFor more information, run info(false_positive_rate).\n\n\n\n\n\n","category":"constant"},{"location":"measures/#MLJBase.matthews_correlation","page":"Measures","title":"MLJBase.matthews_correlation","text":"matthews_correlation\n\nMatthew's correlation; aliases: matthews_correlation, mcc\n\nmatthews_correlation(ŷ, y)\nmatthews_correlation(conf_mat)\n\nReturn Matthews' correlation coefficient corresponding to the point prediction ŷ, given true observations y. This metric is invariant to class labelling and can be used for multiclass classification.\n\nFor more information, run info(matthews_correlation).\n\n\n\n\n\n","category":"constant"},{"location":"measures/#MLJBase.misclassification_rate","page":"Measures","title":"MLJBase.misclassification_rate","text":"misclassification_rate\n\nmisclassification rate; aliases: misclassification_rate, mcr.\n\nmisclassification_rate(ŷ, y)\nmisclassification_rate(ŷ, y, w)\nmisclassification_rate(conf_mat)\n\nReturns the rate of misclassification of the (point) predictions ŷ, given true observations y, optionally weighted by the weights w. All three arguments must be abstract vectors of the same length. A confusion matrix can also be passed as argument. This metric is invariant to class labelling and can be used for multiclass classification.\n\nFor more information, run info(misclassification_rate).\n\n\n\n\n\n","category":"constant"},{"location":"measures/#MLJBase.negative_predictive_value","page":"Measures","title":"MLJBase.negative_predictive_value","text":"negative_predictive_value\n\nnegative predictive value; aliases: negative_predictive_value, negativepredictive_value, npv.\n\nnegative_predictive_value(ŷ, y)\n\nNegative predictive value for observations ŷ and ground truth y. Assigns false to first element of levels(y). To reverse roles, use NPV(rev=true) instead of negative_predictive_value.\n\nFor more information, run info(negative_predictive_value).\n\n\n\n\n\n","category":"constant"},{"location":"measures/#MLJBase.positive_predictive_value","page":"Measures","title":"MLJBase.positive_predictive_value","text":"positive_predictive_value\n\npositive predictive value (aka precision); aliases: positive_predictive_value, ppv, Precision(), positivepredictive_value. \n\npositive_predictive_value(ŷ, y)\n\nPositive predictive value for observations ŷ and ground truth y. Assigns false to first element of levels(y). To reverse roles, use Precision(rev=true) instead of positive_predictive_value.\n\nFor more information, run info(positive_predictive_value).\n\n\n\n\n\n","category":"constant"},{"location":"measures/#MLJBase.true_negative","page":"Measures","title":"MLJBase.true_negative","text":"true_negative\n\nNumber of true negatives; aliases: true_negative, truenegative.\n\ntrue_negative(ŷ, y)\n\nNumber of true negatives for observations ŷ and ground truth y. Assigns false to first element of levels(y). To reverse roles, use TrueNegative(rev=true) instead of true_negative.\n\nFor more information, run info(true_negative).\n\n\n\n\n\n","category":"constant"},{"location":"measures/#MLJBase.true_negative_rate","page":"Measures","title":"MLJBase.true_negative_rate","text":"true_negative_rate\n\ntrue negative rate; aliases: true_negative_rate, truenegative_rate, tnr, specificity, selectivity.\n\ntrue_negative_rate(ŷ, y)\n\nTrue negative rate for observations ŷ and ground truth y. Assigns false to first element of levels(y). To reverse roles, use TrueNegativeRate(rev=true) instead of true_negative_rate.\n\nFor more information, run info(true_negative_rate).\n\n\n\n\n\n","category":"constant"},{"location":"measures/#MLJBase.true_positive","page":"Measures","title":"MLJBase.true_positive","text":"true_positive\n\nNumber of true positives; aliases: true_positive, truepositive.\n\ntrue_positive(ŷ, y)\n\nNumber of true positives for observations ŷ and ground truth y. Assigns false to first element of levels(y). To reverse roles, use TruePositive(rev=true) instead of true_positive.\n\nFor more information, run info(true_positive).\n\n\n\n\n\n","category":"constant"},{"location":"measures/#MLJBase.true_positive_rate","page":"Measures","title":"MLJBase.true_positive_rate","text":"true_positive_rate\n\nTrue positive rate; aliases: true_positive_rate, truepositive_rate, tpr, sensitivity, recall, hit_rate.\n\ntrue_positive_rate(ŷ, y)\n\nTrue positive rate for observations ŷ and ground truth y. Assigns false to first element of levels(y). To reverse roles, use TruePositiveRate(rev=true) instead of true_positive_rate.\n\nFor more information, run info(true_positive_rate).\n\n\n\n\n\n","category":"constant"},{"location":"measures/#MLJBase.BrierScore-Tuple{}","page":"Measures","title":"MLJBase.BrierScore","text":"BrierScore(; distribution=UnivariateFinite)(ŷ, y)\n\nGiven an abstract vector of distributions ŷ of type distribution, and an abstract vector of true observations y, return the corresponding Brier (aka quadratic) scores.\n\nCurrently only distribution=UnivariateFinite is supported, which is applicable to superivised models with Finite target scitype. In this case, if p(y) is the predicted probability for a single observation y, and C all possible classes, then the corresponding Brier score for that observation is given by\n\n2p(y) - left(sum_η  C p(η)^2right) - 1\n\nNote that BrierScore()=BrierScore{UnivariateFinite} has the alias brier_score.\n\nWarning. Here BrierScore is a \"score\" in the sense that bigger is better (with 0 optimal, and all other values negative). In Brier's original 1950 paper, and many other places, it has the opposite sign, despite the name. Moreover, the present implementation does not treat the binary case as special, so that the score may differ, in that case, by a factor of two from usage elsewhere.\n\nFor more information, run info(BrierScore).\n\n\n\n\n\n","category":"method"},{"location":"measures/#MLJBase.FScore","page":"Measures","title":"MLJBase.FScore","text":"FScore{β}(rev=nothing)\n\nOne-parameter generalization, F_β, of the F-measure or balanced F-score.\n\nWikipedia entry\n\nFScore{β}(ŷ, y)\n\nEvaluate F_β score on observations ,ŷ, given ground truth values, y.\n\nBy default, the second element of levels(y) is designated as true. To reverse roles, use FScore{β}(rev=true) instead of FScore{β}.\n\nFor more information, run info(FScore).\n\n\n\n\n\n","category":"type"},{"location":"measures/#MLJBase.roc_curve-Tuple{AbstractArray{#s361,1} where #s361<:UnivariateFinite,AbstractArray{#s360,1} where #s360<:Union{CategoricalArrays.CategoricalString, CategoricalArrays.CategoricalValue}}","page":"Measures","title":"MLJBase.roc_curve","text":"tprs, fprs, ts = roc_curve(ŷ, y) = roc(ŷ, y)\n\nReturn the ROC curve for a two-class probabilistic prediction ŷ given the ground  truth y. The true positive rates, false positive rates over a range of thresholds ts are returned. Note that if there are k unique scores, there are correspondingly  k thresholds and k+1 \"bins\" over which the FPR and TPR are constant:\n\n[0.0 - thresh[1]]\n[thresh[1] - thresh[2]]\n...\n[thresh[k] - 1]\n\nconsequently, tprs and fprs are of length k+1 if ts is of length k.\n\nTo draw the curve using your favorite plotting backend, do plot(fprs, tprs).\n\n\n\n\n\n","category":"method"},{"location":"measures/#MLJBase._idx_unique_sorted-Tuple{AbstractArray{#s362,1} where #s362<:Real}","page":"Measures","title":"MLJBase._idx_unique_sorted","text":"_idx_unique_sorted(v)\n\nInternal function to return the index of unique elements in v under the assumption that the vector v is sorted in decreasing order.\n\n\n\n\n\n","category":"method"},{"location":"distributions/#Distributions-1","page":"Distributions","title":"Distributions","text":"","category":"section"},{"location":"distributions/#Univariate-Finite-Distribution-1","page":"Distributions","title":"Univariate Finite Distribution","text":"","category":"section"},{"location":"distributions/#","page":"Distributions","title":"Distributions","text":"Modules = [MLJBase]\nPages   = [\"interface/univariate_finite.jl\"]","category":"page"},{"location":"distributions/#MLJBase.UnivariateFinite-Tuple","page":"Distributions","title":"MLJBase.UnivariateFinite","text":"    UnivariateFinite(classes, p)\n\nA discrete univariate distribution whose finite support is the elements of the vector classes, and whose corresponding probabilities are elements of the vector p, which must sum to one (requires MLJBase to be loaded). Here classes must have type AbstractVector{<:CategoricalElement} where\n\n    CategoricalElement = Union{CategoricalValue,CategoricalString}\n\nand all classes are assumed to share the same categorical pool.\n\n    UnivariateFinite(prob_given_class)\n\nA discrete univariate distribution whose finite support is the set of keys of the provided dictionary, prob_given_class (requires MLJBase to be loaded). The dictionary keys must be of type CategoricalElement (see above) and the dictionary values specify the corresponding probabilities.\n\n\n\n\n\n","category":"method"},{"location":"distributions/#MLJModelInterface.classes-Tuple{UnivariateFinite}","page":"Distributions","title":"MLJModelInterface.classes","text":"classes(d::UnivariateFinite)\n\nA list of categorial elements in the common pool of classes used to construct d.\n\nv = categorical([\"yes\", \"maybe\", \"no\", \"yes\"])\nd = UnivariateFinite(v[1:2], [0.3, 0.7])\nclasses(d) # CategoricalArray{String,1,UInt32}[\"maybe\", \"no\", \"yes\"]\n\n\n\n\n\n","category":"method"},{"location":"distributions/#hyperparameters-1","page":"Distributions","title":"hyperparameters","text":"","category":"section"},{"location":"distributions/#","page":"Distributions","title":"Distributions","text":"Modules = [MLJBase]\nPages   = [\"hyperparam/one_dimensional_range_methods.jl\", \"hyperparam/one_dimensional_ranges.jl\"]","category":"page"},{"location":"distributions/#Distributions.sampler-Union{Tuple{T}, Tuple{NumericRange{T,B,D} where D where B<:MLJBase.Boundedness,Distributions.Distribution{Distributions.Univariate,S} where S<:Distributions.ValueSupport}} where T","page":"Distributions","title":"Distributions.sampler","text":"sampler(r::NominalRange, probs::AbstractVector{<:Real})\nsampler(r::NominalRange)\nsampler(r::NumericRange{T}, d)\n\nConstruct an object s which can be used to generate random samples from a ParamRange object r (a one-dimensional range) using one of the following calls:\n\nrand(s)             # for one sample\nrand(s, n)          # for n samples\nrand(rng, s [, n])  # to specify an RNG\n\nThe argument probs can be any probability vector with the same length as r.values. The second sampler method above calls the first with a uniform probs vector.\n\nThe argument d can be either an arbitrary instance of UnivariateDistribution from the Distributions.jl package, or one of a Distributions.jl types for which fit(d, ::NumericRange) is defined. These include: Arcsine, Uniform, Biweight, Cosine, Epanechnikov, SymTriangularDist, Triweight, Normal, Gamma, InverseGaussian, Logistic, LogNormal, Cauchy, Gumbel, Laplace, and Poisson; but see the doc-string for Distributions.fit for an up-to-date list.\n\nIf d is an instance, then sampling is from a truncated form of the supplied distribution d, the truncation bounds being r.lower and r.upper (the attributes r.origin and r.unit attributes are ignored). For discrete numeric ranges (T <: Integer) the samples are rounded.\n\nIf d is a type then a suitably truncated distribution is automatically generated using Distributions.fit(d, r).\n\nImportant. Values are generated with no regard to r.scale, except in the special case r.scale is a callable object f. In that case, f is applied to all values generated by rand as described above (prior to rounding, in the case of discrete numeric ranges).\n\nExamples\n\nr = range(Char, :letter, values=collect(\"abc\"))\ns = sampler(r, [0.1, 0.2, 0.7])\nsamples =  rand(s, 1000);\nStatsBase.countmap(samples)\nDict{Char,Int64} with 3 entries:\n  'a' => 107\n  'b' => 205\n  'c' => 688\n\nr = range(Int, :k, lower=2, upper=6) # numeric but discrete\ns = sampler(r, Normal)\nsamples = rand(s, 1000);\nUnicodePlots.histogram(samples)\n           ┌                                        ┐\n[2.0, 2.5) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 119\n[2.5, 3.0) ┤ 0\n[3.0, 3.5) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 296\n[3.5, 4.0) ┤ 0\n[4.0, 4.5) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 275\n[4.5, 5.0) ┤ 0\n[5.0, 5.5) ┤▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 221\n[5.5, 6.0) ┤ 0\n[6.0, 6.5) ┤▇▇▇▇▇▇▇▇▇▇▇ 89\n           └                                        ┘\n\n\n\n\n\n","category":"method"},{"location":"distributions/#MLJBase.iterator-Tuple{Random.AbstractRNG,ParamRange,Vararg{Any,N} where N}","page":"Distributions","title":"MLJBase.iterator","text":"iterator([rng, ], r::NominalRange, [,n])\niterator([rng, ], r::NumericRange, n)\n\nReturn an iterator (currently a vector) for a ParamRange object r. In the first case iteration is over all values stored in the range (or just the first n, if n is specified). In the second case, the iteration is over approximately n ordered values, generated as follows:\n\n(i) First, exactly n values are generated between U and L, with a spacing determined by r.scale (uniform if scale=:linear) where U and L are given by the following table:\n\nr.lower r.upper L U\nfinite finite r.lower r.upper\n-Inf finite r.upper - 2r.unit r.upper\nfinite Inf r.lower r.lower + 2r.unit\n-Inf Inf r.origin - r.unit r.origin + r.unit\n\n(ii) If a callable f is provided as scale, then a uniform spacing is always applied in (i) but f is broadcast over the results. (Unlike ordinary scales, this alters the effective range of values generated, instead of just altering the spacing.)\n\n(iii) If r is a discrete numeric range (r isa NumericRange{<:Integer}) then the values are additionally rounded, with any duplicate values removed. Otherwise all the values are used (and there are exacltly n of them).\n\n(iv) Finally, if a random number generator rng is specified, then the values are returned in random order (sampling without replacement), and otherwise they are returned in numeric order, or in the order provided to the range constructor, in the case of a NominalRange.\n\n\n\n\n\n","category":"method"},{"location":"distributions/#MLJBase.scale-Tuple{NominalRange}","page":"Distributions","title":"MLJBase.scale","text":"scale(r::ParamRange)\n\nReturn the scale associated with a ParamRange object r. The possible return values are: :none (for a NominalRange), :linear, :log, :log10, :log2, or :custom (if r.scale is a callable object).\n\n\n\n\n\n","category":"method"},{"location":"distributions/#StatsBase.fit-Union{Tuple{D}, Tuple{Type{D},NumericRange}} where D<:Distributions.Distribution","page":"Distributions","title":"StatsBase.fit","text":"Distributions.fit(D, r::MLJBase.NumericRange)\n\nFit and return a distribution d of type D to the one-dimensional range r.\n\nOnly types D in the table below are supported.\n\nThe distribution d is constructed in two stages. First, a distributon d0, characterized by the conditions in the second column of the table, is fit to r. Then d0 is truncated between r.lower and r.upper to obtain d.\n\nDistribution type D Characterization of d0\nArcsine, Uniform, Biweight, Cosine, Epanechnikov, SymTriangularDist, Triweight minimum(d) = r.lower, maximum(d) = r.upper\nNormal, Gamma, InverseGaussian, Logistic, LogNormal mean(d) = r.origin, std(d) = r.unit\nCauchy, Gumbel, Laplace, (Normal) Dist.location(d) = r.origin, Dist.scale(d)  = r.unit\nPoisson Dist.mean(d) = r.unit\n\nHere Dist = Distributions.\n\n\n\n\n\n","category":"method"},{"location":"distributions/#Base.range-Union{Tuple{D}, Tuple{Union{Model, Type},Union{Expr, Symbol}}} where D","page":"Distributions","title":"Base.range","text":"r = range(model, :hyper; values=nothing)\n\nDefine a one-dimensional NominalRange object for a field hyper of model. Note that r is not directly iterable but iterator(r) is. \n\nThe behaviour of range methods depends on the type of the value of the hyperparameter at model during range construction. To override (or if model is not available) specify a type in place of model.\n\nA nested hyperparameter is specified using dot notation. For example, :(atom.max_depth) specifies the max_depth hyperparameter of the submodel model.atom.\n\nr = range(model, :hyper; upper=nothing, lower=nothing,\n          scale=nothing, values=nothing)\n\nAssuming values is not specified, define a one-dimensional NumericRange object for a Real field hyper of model.  Note that r is not directly iteratable but iterator(r, n)is an iterator of length n. To generate random elements from r, instead apply rand methods to sampler(r). The supported scales are :linear,:log, :logminus, :log10, :log2, or a callable object.\n\nA nested hyperparameter is specified using dot notation (see above).\n\nIf scale is unspecified, it is set to :linear, :log, :logminus, or :linear, according to whether the interval (lower, upper) is bounded, right-unbounded, left-unbounded, or doubly unbounded, respectively.  Note upper=Inf and lower=-Inf are allowed.\n\nIf values is specified, the other keyword arguments are ignored and a NominalRange object is returned (see above).\n\nSee also: iterator, sampler\n\n\n\n\n\n","category":"method"},{"location":"distributions/#Utility-functions-1","page":"Distributions","title":"Utility functions","text":"","category":"section"},{"location":"distributions/#","page":"Distributions","title":"Distributions","text":"Modules = [MLJBase]\nPages   = [\"distributions.jl\"]","category":"page"},{"location":"distributions/#Distributions.pdf-Tuple{UnivariateFinite,Union{CategoricalArrays.CategoricalString, CategoricalArrays.CategoricalValue}}","page":"Distributions","title":"Distributions.pdf","text":"Distributions.pdf(d::UnivariateFinite, x)\n\nProbability of d at x.\n\nv = categorical([\"yes\", \"maybe\", \"no\", \"yes\"])\nd = UnivariateFinite(v[1:2], [0.3, 0.7])\npdf(d, \"yes\")     # 0.3\npdf(d, v[1])      # 0.3\npdf(d, \"no\")      # 0.0\npdf(d, \"house\")   # throws error\n\nOther similar methods are available too:\n\nmode(d)    # CategoricalString{UInt32} \"maybe\"\nrand(d, 5) # CategoricalArray{String,1,UInt32}[\"maybe\", \"no\", \"maybe\", \"maybe\", \"no\"] or similar\nd = fit(UnivariateFinite, v)\npdf(d, \"maybe\") # 0.25\n\nOne can also do weighted fits:\n\nw = [1, 4, 5, 1] # some weights\nd = fit(UnivariateFinite, v, w)\npdf(d, \"maybe\") ≈ 4/11 # true\n\nWarning: The pdf function will give wrong answers if the order of  levels of any categorical element passed to the UnivariateFinite  constructor is changed.\n\nSee also classes, support.\n\n\n\n\n\n","category":"method"},{"location":"distributions/#Missings.levels-Tuple{UnivariateFinite}","page":"Distributions","title":"Missings.levels","text":"levels(d::UnivariateFinite)\n\nA list of the raw levels in the common pool of classes used to construct d, equal to get.(classes(d)).\n\nv = categorical([\"yes\", \"maybe\", \"no\", \"yes\"])\nd = UnivariateFinite(v[1:2], [0.3, 0.7])\nlevels(d) # Array{String, 1}[\"maybe\", \"no\", \"yes\"]\n\n\n\n\n\n","category":"method"},{"location":"distributions/#Base.isapprox-Tuple{UnivariateFinite,UnivariateFinite}","page":"Distributions","title":"Base.isapprox","text":"isapprox(d1::UnivariateFinite, d2::UnivariateFinite; kwargs...)\n\nReturns true if and only if Set(classes(d1) == Set(classes(d2)) and the corresponding probabilities are approximately equal. The key-word arguments kwargs are passed through to each call of isapprox on probability pairs. Returns false otherwise.\n\n\n\n\n\n","category":"method"},{"location":"distributions/#Distributions.support-Tuple{UnivariateFinite}","page":"Distributions","title":"Distributions.support","text":"Distributions.support(d::UnivariateFinite)\n\nThose classes associated with non-zero probabilities.\n\nv = categorical([\"yes\", \"maybe\", \"no\", \"yes\"])\nd = UnivariateFinite(v[1:2], [0.3, 0.7])\nsupport(d) # CategoricalArray{String,1,UInt32}[\"maybe\", \"no\"]\n\n\n\n\n\n","category":"method"},{"location":"distributions/#MLJBase._cumulative-Union{Tuple{UnivariateFinite{L,U,T}}, Tuple{T}, Tuple{U}, Tuple{L}} where T<:Real where U where L","page":"Distributions","title":"MLJBase._cumulative","text":"_cumulative(d::UnivariateFinite)\n\nReturn the cumulative probability vector [0, ..., 1] for the distribution d, using whatever ordering is used in the dictionary d.prob_given_class. Used only for to implement random sampling from d.\n\n\n\n\n\n","category":"method"},{"location":"distributions/#MLJBase._rand-Tuple{Any}","page":"Distributions","title":"MLJBase._rand","text":"rand(pcumulative)\n\nRandomly sample the distribution with discrete support 1:n which has cumulative probability vector p_cumulative=[0, ..., 1] (of length n+1). Does not check the first and last elements of p_cumulative but does not use them either.\n\n\n\n\n\n","category":"method"},{"location":"utilities/#Utilities-1","page":"Utilities","title":"Utilities","text":"","category":"section"},{"location":"utilities/#Machines-1","page":"Utilities","title":"Machines","text":"","category":"section"},{"location":"utilities/#","page":"Utilities","title":"Utilities","text":"Modules = [MLJBase]\nPages   = [\"machines.jl\"]","category":"page"},{"location":"utilities/#MLJBase.report-Tuple{AbstractMachine}","page":"Utilities","title":"MLJBase.report","text":"report(mach)\n\nReturn the report for a machine mach that has been fit!, for example the coefficients in a linear model.\n\nThis is a named tuple and human-readable if possible.\n\nIf mach is a machine for a composite model, then the returned value has keys machines and report_given_machine, whose corresponding values are a vector of (nodal) machines appearing in the underlying learning network, and a dictionary of reports keyed on those machines.\n\nusing MLJ\nX, y = @load_crabs;\npipe = @pipeline MyPipe(\n    std = Standardizer(),\n    clf = @load LinearBinaryClassifier pkg=GLM\n)\nmach = machine(MyPipe(), X, y) |> fit!\nr = report(mach)\nr.machines\n2-element Array{Any,1}:\n NodalMachine{LinearBinaryClassifier{LogitLink}} @ 1…57\n NodalMachine{Standardizer} @ 7…33\n\nr.report_given_machine[machs[1]]\n(deviance = 3.8893386087844543e-7,\n dof_residual = 195.0,\n stderror = [18954.83496713119, ..., 2111.1294584763386],\n vcov = [3.592857686311793e8 ... .442545425533723e6;\n         ...\n         5.38856837634321e6 ... 2.1799125705781363e7 4.456867590446599e6],)\n\n\n\n\n\n","category":"method"},{"location":"utilities/#StatsBase.fit!-Tuple{AbstractMachine}","page":"Utilities","title":"StatsBase.fit!","text":"fit!(mach::Machine; rows=nothing, verbosity=1, force=false)\n\nWhen called for the first time, call\n\nfit(mach.model, verbosity, mach.args...)\n\nstoring the returned fit-result and report in mach. Subsequent calls do nothing unless: (i) force=true, or (ii) the specified rows are different from those used the last time a fit-result was computed, or (iii) mach.model has changed since the last time a fit-result was computed (the machine is stale). In cases (i) or (ii) MLJBase.fit is called again. Otherwise, MLJBase.update is called.\n\nfit!(mach::NodalMachine; rows=nothing, verbosity=1, force=false)\n\nWhen called for the first time, attempt to call\n\nfit(mach.model, verbosity, mach.args...)\n\nThis will fail if an argument of the machine depends ultimately on some other untrained machine for successful calling, but this is resolved by instead calling fit! any node N for which mach in machines(N) is true, which trains all necessary machines in an appropriate order. Subsequent fit! calls do nothing unless: (i) force=true, or (ii) some machine on which mach depends has computed a new fit-result since mach last computed its fit-result, or (iii) the specified rows have changed since the last time a fit-result was last computed, or (iv) mach is stale (see below). In cases (i), (ii) or (iii), MLJBase.fit is called. Otherwise MLJBase.update is called.\n\nA machine mach is stale if mach.model has changed since the last time a fit-result was computed, or if one of its training arguments is stale. A node N is stale if N.machine is stale or one of its arguments is stale. Source nodes are never stale.\n\nNote that a nodal machine obtains its training data by calling its node arguments on the specified rows (rather than indexing its arguments on those rows) and that this calling is a recursive operation on nodes upstream of those arguments.\n\n\n\n\n\n","category":"method"},{"location":"utilities/#MLJModelInterface.save-Tuple{Any,AbstractMachine}","page":"Utilities","title":"MLJModelInterface.save","text":"MLJ.save(filename, mach::AbstractMachine; kwargs...)\nMLJ.save(io, mach::Machine; kwargs...)\n\nMLJBase.save(filename, mach::AbstractMachine; kwargs...)\nMLJBase.save(io, mach::Machine; kwargs...)\n\nSerialize the machine mach to a file with path filename, or to an input/output stream io (at least IOBuffer instances are supported).\n\nThe format is JLSO (a wrapper for julia native or BSON serialization) unless a custom format has been implemented for the model type of mach.model. The keyword arguments kwargs are passed to the format-specific serializer, which in the JSLO case include these:\n\nkeyword values default\nformat :julia_serialize, :BSON :julia_serialize\ncompression :gzip, :none :none\n\nSee (see https://github.com/invenia/JLSO.jl for details.\n\nMachines are de-serialized using the machine constructor as shown in the example below. Data (or nodes) may be optionally passed to the constructor for retraining on new data using the saved model.\n\nExample\n\nusing MLJ\ntree = @load DecisionTreeClassifier\nX, y = @load_iris\nmach = fit!(machine(tree, X, y))\n\nMLJ.save(\"tree.jlso\", mach, compression=:none)\nmach_predict_only = machine(\"tree.jlso\")\npredict(mach_predict_only, X)\n\nmach2 = machine(\"tree.jlso\", selectrows(X, 1:100), y[1:100])\npredict(mach2, X) # same as above\n\nfit!(mach2) # saved learned parameters are over-written\npredict(mach2, X) # not same as above\n\n# using a buffer:\nio = IOBuffer()\nMLJ.save(io, mach)\nseekstart(io)\npredict_only_mach = machine(io)\npredict(predict_only_mach, X)\n\nwarning: Only load files from trusted sources\nMaliciously constructed JLSO files, like pickles, and most other general purpose serialization formats, can allow for arbitrary code execution during loading. This means it is possible for someone to use a JLSO file that looks like a serialized MLJ machine as a Trojan horse.\n\n\n\n\n\n","category":"method"},{"location":"utilities/#Parameter-Inspection-1","page":"Utilities","title":"Parameter Inspection","text":"","category":"section"},{"location":"utilities/#","page":"Utilities","title":"Utilities","text":"Modules = [MLJBase]\nPages   = [\"parameter_inspection.jl\"]","category":"page"},{"location":"utilities/#MLJBase.params-Tuple{Any}","page":"Utilities","title":"MLJBase.params","text":"params(m::MLJType)\n\nRecursively convert any transparent object m into a named tuple, keyed on the fields of m. An object is transparent if MLJBase.istransparent(m) == true. The named tuple is possibly nested because params is recursively applied to the field values, which themselves might be transparent.\n\nMost objects of type MLJType are transparent.\n\njulia> params(EnsembleModel(atom=ConstantClassifier()))\n(atom = (target_type = Bool,),\n weights = Float64[],\n bagging_fraction = 0.8,\n rng_seed = 0,\n n = 100,\n parallel = true,)\n\n\n\n\n\n","category":"method"},{"location":"utilities/#Show-1","page":"Utilities","title":"Show","text":"","category":"section"},{"location":"utilities/#","page":"Utilities","title":"Utilities","text":"Modules = [MLJBase]\nPages   = [\"show.jl\"]","category":"page"},{"location":"utilities/#MLJBase.color_off-Tuple{}","page":"Utilities","title":"MLJBase.color_off","text":"color_off()\n\nSuppress color and bold output at the REPL for displaying MLJ objects.\n\n\n\n\n\n","category":"method"},{"location":"utilities/#MLJBase.color_on-Tuple{}","page":"Utilities","title":"MLJBase.color_on","text":"color_on()\n\nEnable color and bold output at the REPL, for enhanced display of MLJ objects.\n\n\n\n\n\n","category":"method"},{"location":"utilities/#MLJBase.@constant-Tuple{Any}","page":"Utilities","title":"MLJBase.@constant","text":"@constant x = value\n\nEquivalent to const x = value but registers the binding thus:\n\nMLJBase.HANDLE_GIVEN_ID[objectid(value)] = :x\n\nRegistered objects get displayed using the variable name to which it was bound in calls to show(x), etc.\n\nWARNING: As with any const declaration, binding x to new value of the same type is not prevented and the registration will not be updated.\n\n\n\n\n\n","category":"macro"},{"location":"utilities/#MLJBase.@more-Tuple{}","page":"Utilities","title":"MLJBase.@more","text":"@more\n\nEntered at the REPL, equivalent to show(ans, 100). Use to get a recursive description of all fields of the last REPL value.\n\n\n\n\n\n","category":"macro"},{"location":"utilities/#MLJBase._recursive_show-Tuple{IO,MLJType,Any,Any}","page":"Utilities","title":"MLJBase._recursive_show","text":"_recursive_show(stream, object, current_depth, depth)\n\nGenerate a table of the field values of the MLJType object, dislaying each value by calling the method _show on it. The behaviour of _show(stream, f) is as follows:\n\nIf f is itself a MLJType object, then its short form is shown\n\nand _recursive_show generates as separate table for each of its field values (and so on, up to a depth of argument depth).\n\nOtherwise f is displayed as \"(omitted T)\" where T = typeof(f),\n\nunless istoobig(f) is false (the istoobig fall-back for arbitrary types being true). In the latter case, the long (ie, MIME\"plain/text\") form of f is shown. To override this behaviour, overload the _show method for the type in question.\n\n\n\n\n\n","category":"method"},{"location":"utilities/#MLJBase.abbreviated-Tuple{Any}","page":"Utilities","title":"MLJBase.abbreviated","text":"to display abbreviated versions of integers\n\n\n\n\n\n","category":"method"},{"location":"utilities/#MLJBase.handle-Tuple{Any}","page":"Utilities","title":"MLJBase.handle","text":"return abbreviated object id (as string) or it's registered handle (as string) if this exists\n\n\n\n\n\n","category":"method"},{"location":"utilities/#Utility-functions-1","page":"Utilities","title":"Utility functions","text":"","category":"section"},{"location":"utilities/#","page":"Utilities","title":"Utilities","text":"Modules = [MLJBase]\nPages   = [\"utilities.jl\"]","category":"page"},{"location":"utilities/#MLJBase.flat_values-Tuple{NamedTuple}","page":"Utilities","title":"MLJBase.flat_values","text":"flat_values(t::NamedTuple)\n\nView a nested named tuple t as a tree and return, as a tuple, the values at the leaves, in the order they appear in the original tuple.\n\njulia> t = (X = (x = 1, y = 2), Y = 3)\njulia> flat_values(t)\n(1, 2, 3)\n\n\n\n\n\n","category":"method"},{"location":"utilities/#MLJBase.recursive_getproperty-Tuple{Any,Symbol}","page":"Utilities","title":"MLJBase.recursive_getproperty","text":"recursive_getproperty(object, nested_name::Expr)\n\nCall getproperty recursively on object to extract the value of some nested property, as in the following example:\n\njulia> object = (X = (x = 1, y = 2), Y = 3)\njulia> recursive_getproperty(object, :(X.y))\n2\n\n\n\n\n\n","category":"method"},{"location":"utilities/#MLJBase.recursive_setproperty!-Tuple{Any,Symbol,Any}","page":"Utilities","title":"MLJBase.recursive_setproperty!","text":"recursively_setproperty!(object, nested_name::Expr, value)\n\nSet a nested property of an object to value, as in the following example:\n\njulia> mutable struct Foo\n           X\n           Y\n       end\n\njulia> mutable struct Bar\n           x\n           y\n       end\n\njulia> object = Foo(Bar(1, 2), 3)\nFoo(Bar(1, 2), 3)\n\njulia> recursively_setproperty!(object, :(X.y), 42)\n42\n\njulia> object\nFoo(Bar(1, 42), 3)\n\n\n\n\n\n","category":"method"},{"location":"utilities/#MLJBase.unwind-Tuple","page":"Utilities","title":"MLJBase.unwind","text":"unwind(iterators...)\n\nRepresent all possible combinations of values generated by iterators as rows of a matrix A. In more detail, A has one column for each iterator in iterators and one row for each distinct possible combination of values taken on by the iterators. Elements in the first column cycle fastest, those in the last clolumn slowest.\n\nExample\n\njulia> iterators = ([1, 2], [\"a\",\"b\"], [\"x\", \"y\", \"z\"]);\njulia> MLJTuning.unwind(iterators...)\n12×3 Array{Any,2}:\n 1  \"a\"  \"x\"\n 2  \"a\"  \"x\"\n 1  \"b\"  \"x\"\n 2  \"b\"  \"x\"\n 1  \"a\"  \"y\"\n 2  \"a\"  \"y\"\n 1  \"b\"  \"y\"\n 2  \"b\"  \"y\"\n 1  \"a\"  \"z\"\n 2  \"a\"  \"z\"\n 1  \"b\"  \"z\"\n 2  \"b\"  \"z\"\n\n\n\n\n\n","category":"method"},{"location":"utilities/#MLJBase.@set_defaults-Tuple{Any}","page":"Utilities","title":"MLJBase.@set_defaults","text":"@set_defaults ModelType(args...)\n@set_defaults ModelType args\n\nCreate a keyword constructor for any type ModelType<:MLJBase.Model, using as default values those listed in args. These must include a value for every field, and in the order appearing in fieldnames(ModelType).\n\nThe constructor does not call MLJBase.clean!(model) on the instantiated object model. This method is for internal use only (by @from_network macro) as it is depreciated by @mlj_model macro. \n\nExample\n\nmutable struct Foo       x::Int       y    end\n\n@set_defaults Foo(1,2)\n\njulia> Foo()    Foo(1, 2)\n\njulia> Foo(x=1, y=\"house\")    Foo(1, \"house\")\n\n@set_defaults Foo [4, 5]\n\njulia> Foo()    Foo(4, 5)\n\n\n\n\n\n","category":"macro"},{"location":"utilities/#MLJBase._permute_rows-Tuple{Union{AbstractArray{T,1}, AbstractArray{T,2}} where T,Array{Int64,1}}","page":"Utilities","title":"MLJBase._permute_rows","text":"permuterows(obj, perm)\n\nInternal function to return a vector or matrix with permuted rows given the permutation perm.\n\n\n\n\n\n","category":"method"},{"location":"utilities/#MLJBase.check_dimensions-Tuple{Union{AbstractArray{T,1}, AbstractArray{T,2}} where T,Union{AbstractArray{T,1}, AbstractArray{T,2}} where T}","page":"Utilities","title":"MLJBase.check_dimensions","text":"check_dimension(X, Y)\n\nCheck that two vectors or matrices have matching dimensions\n\n\n\n\n\n","category":"method"},{"location":"utilities/#MLJBase.shuffle_rows-Tuple{Union{AbstractArray{T,1}, AbstractArray{T,2}} where T,Union{AbstractArray{T,1}, AbstractArray{T,2}} where T}","page":"Utilities","title":"MLJBase.shuffle_rows","text":"shuffle_rows(X, Y, ...; rng=)\n\nReturn a shuffled view of a vector or  matrix X (or set of such) using a random permutation (which can be seeded specifying rng).\n\n\n\n\n\n","category":"method"},{"location":"resampling/#Resampling-1","page":"Resampling","title":"Resampling","text":"","category":"section"},{"location":"resampling/#","page":"Resampling","title":"Resampling","text":"Modules = [MLJBase]\nPages   = [\"resampling.jl\"]","category":"page"},{"location":"resampling/#MLJBase.CV","page":"Resampling","title":"MLJBase.CV","text":"cv = CV(; nfolds=6,  shuffle=nothing, rng=nothing)\n\nCross-validation resampling strategy, for use in evaluate!, evaluate and tuning.\n\ntrain_test_pairs(cv, rows)\n\nReturns an nfolds-length iterator of (train, test) pairs of vectors (row indices), where each train and test is a sub-vector of rows. The test vectors are mutually exclusive and exhaust rows. Each train vector is the complement of the corresponding test vector. With no row pre-shuffling, the order of rows is preserved, in the sense that rows coincides precisely with the concatenation of the test vectors, in the order they are generated. The first r test vectors have length n + 1, where n, r = divrem(length(rows), nfolds), and the remaining test vectors have length n.\n\nPre-shuffling of rows is controlled by rng and shuffle. If rng is an integer, then the CV keyword constructor resets it to MersenneTwister(rng). Otherwise some AbstractRNG object is expected.\n\nIf rng is left unspecified, rng is reset to Random.GLOBAL_RNG, in which case rows are only pre-shuffled if shuffle=true is explicitly specified.\n\n\n\n\n\n","category":"type"},{"location":"resampling/#MLJBase.Holdout","page":"Resampling","title":"MLJBase.Holdout","text":"holdout = Holdout(; fraction_train=0.7,\n                     shuffle=nothing,\n                     rng=nothing)\n\nHoldout resampling strategy, for use in evaluate!, evaluate and in tuning.\n\ntrain_test_pairs(holdout, rows)\n\nReturns the pair [(train, test)], where train and test are vectors such that rows=vcat(train, test) and length(train)/length(rows) is approximatey equal to fraction_train`.\n\nPre-shuffling of rows is controlled by rng and shuffle. If rng is an integer, then the Holdout keyword constructor resets it to MersenneTwister(rng). Otherwise some AbstractRNG object is expected.\n\nIf rng is left unspecified, rng is reset to Random.GLOBAL_RNG, in which case rows are only pre-shuffled if shuffle=true is specified.\n\n\n\n\n\n","category":"type"},{"location":"resampling/#MLJBase.Resampler","page":"Resampling","title":"MLJBase.Resampler","text":"resampler = Resampler(model=ConstantRegressor(),\n                      resampling=CV(),\n                      measure=nothing,\n                      weights=nothing,\n                      operation=predict,\n                      repeats = 1,\n                      acceleration=default_resource(),\n                      check_measure=true)\n\nResampling model wrapper, used internally by the fit method of TunedModel instances. See `evaluate! for options. Not intended for general use.\n\nGiven a machine mach = machine(resampler, args...) one obtains a performance evaluation of the specified model, performed according to the prescribed resampling strategy and other parameters, using data args..., by calling fit!(mach) followed by evaluate(mach). The advantage over using evaluate(model, X, y) is that the latter call always calls fit on the model but fit!(mach) only calls update after the first call.\n\nThe sample weights are passed to the specified performance measures that support weights for evaluation.\n\nImportant: If weights are left unspecified, then any weight vector w used in constructing the resampler machine, as in resampler_machine = machine(resampler, X, y, w) (which is then used in training the model) will also be used in evaluation.\n\n\n\n\n\n","category":"type"},{"location":"resampling/#MLJBase.StratifiedCV","page":"Resampling","title":"MLJBase.StratifiedCV","text":"stratified_cv = StratifiedCV(; nfolds=6,\n                               shuffle=false,\n                               rng=Random.GLOBAL_RNG)\n\nStratified cross-validation resampling strategy, for use in evaluate!, evaluate and in tuning. Applies only to classification problems (OrderedFactor or Multiclass targets).\n\ntrain_test_pairs(stratified_cv, rows, y)\n\nReturns an nfolds-length iterator of (train, test) pairs of vectors (row indices) where each train and test is a sub-vector of rows. The test vectors are mutually exclusive and exhaust rows. Each train vector is the complement of the corresponding test vector.\n\nUnlike regular cross-validation, the distribution of the levels of the target y corresponding to each train and test is constrained, as far as possible, to replicate that of y[rows] as a whole.\n\nSpecifically, the data is split into a number of groups on which y is constant, and each individual group is resampled according to the ordinary cross-validation strategy CV(nfolds=nfolds). To obtain the final (train, test) pairs of row indices, the per-group pairs are collated in such a way that each collated train and test respects the original order of rows (after shuffling, if shuffle=true).\n\nPre-shuffling of rows is controlled by rng and shuffle. If rng is an integer, then the StratifedCV keyword constructor resets it to MersenneTwister(rng). Otherwise some AbstractRNG object is expected.\n\nIf rng is left unspecified, rng is reset to Random.GLOBAL_RNG, in which case rows are only pre-shuffled if shuffle=true is explicitly specified.\n\n\n\n\n\n","category":"type"},{"location":"resampling/#MLJBase.evaluate!-Tuple{Machine{#s137} where #s137<:Supervised}","page":"Resampling","title":"MLJBase.evaluate!","text":"evaluate!(mach,\n          resampling=CV(),\n          measure=nothing,\n          weights=nothing,\n          operation=predict,\n          repeats = 1,\n          acceleration=default_resource(),\n          force=false,\n          verbosity=1,\n          check_measure=true)\n\nEstimate the performance of a machine mach wrapping a supervised model in data, using the specified resampling strategy (defaulting to 6-fold cross-validation) and measure, which can be a single measure or vector.\n\nDo subtypes(MLJ.ResamplingStrategy) to obtain a list of available resampling strategies. If resampling is not an object of type MLJ.ResamplingStrategy, then a vector of pairs (of the form (train_rows, test_rows) is expected. For example, setting\n\nresampling = [(1:100), (101:200)),\n               (101:200), (1:100)]\n\ngives two-fold cross-validation using the first 200 rows of data.\n\nThe resampling strategy is applied repeatedly if repeats > 1. For resampling = CV(nfolds=5), for example, this generates a total of 5n test folds for evaluation and subsequent aggregation.\n\nIf resampling isa MLJ.ResamplingStrategy then one may optionally restrict the data used in evaluation by specifying rows.\n\nAn optional weights vector may be passed for measures that support sample weights (MLJ.supports_weights(measure) == true), which is ignored by those that don't.\n\nImportant: If mach already wraps sample weights w (as in mach = machine(model, X, y, w)) then these weights, which are used for training, are automatically passed to the measures for evaluation. However, for evaluation purposes, any weights specified as a keyword argument will take precedence over w.\n\nUser-defined measures are supported; see the manual for details.\n\nIf no measure is specified, then default_measure(mach.model) is used, unless this default is nothing and an error is thrown.\n\nThe acceleration keyword argument is used to specify the compute resource (a subtype of ComputationalResources.AbstractResource) that will be used to accelerate/parallelize the resampling operation.\n\nAlthough evaluate! is mutating, mach.model and mach.args are untouched.\n\nReturn value\n\nA property-accessible object of type PerformanceEvaluation with these properties:\n\nmeasure: the vector of specified measures\nmeasurements: the corresponding measurements, aggregated across the test folds using the aggregation method defined for each measure (do aggregation(measure) to inspect)\nper_fold: a vector of vectors of individual test fold evaluations (one vector per measure)\nper_observation: a vector of vectors of individual observation evaluations of those measures for which reports_each_observation(measure) is true, which is otherwise reported missing.\n\nSee also evaluate\n\n\n\n\n\n","category":"method"},{"location":"resampling/#MLJModelInterface.evaluate-Tuple{Supervised,Vararg{Any,N} where N}","page":"Resampling","title":"MLJModelInterface.evaluate","text":"evaluate(model, X, y; measure=nothing, options...)\nevaluate(model, X, y, w; measure=nothing, options...)\n\nEvaluate the performance of a supervised model model on input data X and target y, optionally specifying sample weights w for training, where supported. The same weights are passed to measures that support sample weights, unless this behaviour is overridden by explicitly specifying the option weights=....\n\nSee the machine version evaluate! for the complete list of options.\n\n\n\n\n\n","category":"method"},{"location":"composition/#Composition-1","page":"Composition","title":"Composition","text":"","category":"section"},{"location":"composition/#Composites-1","page":"Composition","title":"Composites","text":"","category":"section"},{"location":"composition/#","page":"Composition","title":"Composition","text":"Modules = [MLJBase]\nPages   = [\"composition/composites.jl\"]","category":"page"},{"location":"composition/#MLJBase.anonymize!-Tuple","page":"Composition","title":"MLJBase.anonymize!","text":"anonymize!(sources...)\n\nReturns a named tuple (sources=..., data=....) whose values are the provided source nodes and their contents respectively, and clears the contents of those source nodes.\n\n\n\n\n\n","category":"method"},{"location":"composition/#MLJBase.@from_network-Tuple","page":"Composition","title":"MLJBase.@from_network","text":"@from_network(NewCompositeModel(fld1=model1, fld2=model2, ...) <= N\n@from_network(NewCompositeModel(fld1=model1, fld2=model2, ...) <= N is_probabilistic=false\n\nCreate a new stand-alone model type called NewCompositeModel, using a learning network as a blueprint. Here N refers to the terminal node of the learning network (from which final predictions or transformations are fetched).\n\nImportant. If the learning network is supervised (has a source with kind=:target) and makes probabilistic predictions, then one must declare is_probabilistic=true. In the deterministic case the keyword argument can be omitted.\n\nThe model type NewCompositeModel is equipped with fields named :fld1, :fld2, ..., which correspond to component models model1, model2, ...,  appearing in the network (which must therefore be elements of models(N)).  Deep copies of the specified component models are used as default values in an automatically generated keyword constructor for NewCompositeModel.\n\nReturn value\n\nA new NewCompositeModel instance, with default field values.\n\nFor details and examples refer to the \"Learning Networks\" section of the documentation.\n\n\n\n\n\n","category":"macro"},{"location":"composition/#Base.replace-Tuple{Node,Vararg{Pair,N} where N}","page":"Composition","title":"Base.replace","text":"replace(W::Node, a1=>b1, a2=>b2, ...; empty_unspecified_sources=false)\n\nCreate a deep copy of a node W, and thereby replicate the learning network terminating at W, but replacing any specified sources and models a1, a2, ... of the original network with b1, b2, ....\n\nIf empty_unspecified_sources=ture then any source nodes not specified are replaced with empty version of the same kind.\n\n\n\n\n\n","category":"method"},{"location":"composition/#Networks-1","page":"Composition","title":"Networks","text":"","category":"section"},{"location":"composition/#","page":"Composition","title":"Composition","text":"Modules = [MLJBase]\nPages   = [\"composition/networks.jl\"]","category":"page"},{"location":"composition/#MLJBase.node","page":"Composition","title":"MLJBase.node","text":"N = node(f::Function, args...)\n\nDefines a Node object N wrapping a static operation f and arguments args. Each of the n elements of args must be a Node or Source object. The node N has the following calling behaviour:\n\nN() = f(args[1](), args[2](), ..., args[n]())\nN(rows=r) = f(args[1](rows=r), args[2](rows=r), ..., args[n](rows=r))\nN(X) = f(args[1](X), args[2](X), ..., args[n](X))\n\nJ = node(f, mach::NodalMachine, args...)\n\nDefines a dynamic Node object J wrapping a dynamic operation f (predict, predict_mean, transform, etc), a nodal machine mach and arguments args. Its calling behaviour, which depends on the outcome of training mach (and, implicitly, on training outcomes affecting its arguments) is this:\n\nJ() = f(mach, args[1](), args[2](), ..., args[n]())\nJ(rows=r) = f(mach, args[1](rows=r), args[2](rows=r), ..., args[n](rows=r))\nJ(X) = f(mach, args[1](X), args[2](X), ..., args[n](X))\n\nGenerally n=1 or n=2 in this latter case.\n\npredict(mach, X::AbsractNode, y::AbstractNode)\npredict_mean(mach, X::AbstractNode, y::AbstractNode)\npredict_median(mach, X::AbstractNode, y::AbstractNode)\npredict_mode(mach, X::AbstractNode, y::AbstractNode)\ntransform(mach, X::AbstractNode)\ninverse_transform(mach, X::AbstractNode)\n\nShortcuts for J = node(predict, mach, X, y), etc.\n\nCalling a node is a recursive operation which terminates in the call to a source node (or nodes). Calling nodes on new data X fails unless the number of such nodes is one.\n\nSee also: source, origins.\n\n\n\n\n\n","category":"type"},{"location":"composition/#MLJBase.freeze!-Tuple{NodalMachine}","page":"Composition","title":"MLJBase.freeze!","text":"freeze!(mach)\n\nFreeze the machine mach so that it will never be retrained (unless thawed).\n\nSee also thaw!.\n\n\n\n\n\n","category":"method"},{"location":"composition/#MLJBase.machines-Tuple{Node}","page":"Composition","title":"MLJBase.machines","text":"machines(N)\n\nList all machines in the learning network terminating at node N.\n\n\n\n\n\n","category":"method"},{"location":"composition/#MLJBase.models-Tuple{AbstractNode}","page":"Composition","title":"MLJBase.models","text":"models(N::AbstractNode)\n\nA vector of all models referenced by a node N, each model appearing exactly once.\n\n\n\n\n\n","category":"method"},{"location":"composition/#MLJBase.nodes-Tuple{Node}","page":"Composition","title":"MLJBase.nodes","text":"nodes(N)\n\nReturn all nodes upstream of a node N, including N itself, in an order consistent with the extended directed acyclic graph of the network. Here \"extended\" means edges corresponding to training arguments are included.\n\n\n\n\n\n","category":"method"},{"location":"composition/#MLJBase.origins-Tuple{Source}","page":"Composition","title":"MLJBase.origins","text":"origins(N)\n\nReturn a list of all origins of a node N accessed by a call N(). These are the source nodes of the acyclic directed graph (DAG) associated with the learning network terminating at N, if edges corresponding to training arguments are excluded. A Node object cannot be called on new data unless it has a unique origin.\n\nNot to be confused with sources(N) which refers to the same graph but without the training edge deletions.\n\nSee also: node, source.\n\n\n\n\n\n","category":"method"},{"location":"composition/#MLJBase.rebind!-Tuple{Source,Any}","page":"Composition","title":"MLJBase.rebind!","text":"rebind!(s)\n\nAttach new data X to an existing source node s.\n\n\n\n\n\n","category":"method"},{"location":"composition/#MLJBase.source-Tuple{Any}","page":"Composition","title":"MLJBase.source","text":"Xs = source(X)\nys = source(y, kind=:target)\nws = source(w, kind=:weight)\n\nDefines, respectively, learning network Source objects for wrapping some input data X (kind=:input), some target data y, or some sample weights w.  The values of each variable X, y, w can be anything, even nothing, if the network is for exporting as a stand-alone model only. For training and testing the unexported network, appropriate vectors, tables, or other data containers are expected.\n\nXs = source()\nys = source(kind=:target)\nws = source(kind=:weight)\n\nDefine source nodes wrapping nothing instead of concrete data. Such definitions suffice if a learning network is to be exported without testing.\n\nThe calling behaviour of a Source object is this:\n\nXs() = X\nXs(rows=r) = selectrows(X, r)  # eg, X[r,:] for a DataFrame\nXs(Xnew) = Xnew\n\nSee also: [@from_network](@ref], sources, origins, node.\n\n\n\n\n\n","category":"method"},{"location":"composition/#MLJBase.sources-Tuple{AbstractNode}","page":"Composition","title":"MLJBase.sources","text":"sources(N::AbstractNode; kind=:any)\n\nA vector of all sources referenced by calls N() and fit!(N). These are the sources of the directed acyclic graph associated with the learning network terminating at N, including training edges. The return value can be restricted further by specifying kind=:input, kind=:target, kind=:weight, etc.\n\nNot to be confused with origins(N) which refers to the same graph with edges corresponding to training arguments deleted.\n\nSee also: origins, source.\n\n\n\n\n\n","category":"method"},{"location":"composition/#MLJBase.thaw!-Tuple{NodalMachine}","page":"Composition","title":"MLJBase.thaw!","text":"thaw!(mach)\n\nUnfreeze the machine mach so that it can be retrained.\n\nSee also freeze!.\n\n\n\n\n\n","category":"method"},{"location":"composition/#MLJModelInterface.selectcols-Tuple{AbstractNode,Any}","page":"Composition","title":"MLJModelInterface.selectcols","text":"selectcols(X::AbstractNode, c)\n\nReturns Node object N such that N() = selectcols(X(), c).\n\n\n\n\n\n","category":"method"},{"location":"composition/#MLJModelInterface.selectrows-Tuple{AbstractNode,Any}","page":"Composition","title":"MLJModelInterface.selectrows","text":"selectrows(X::AbstractNode, r)\n\nReturns a Node object N such that N() = selectrows(X(), r) (and N(rows=s) = selectrows(X(rows=s), r)).\n\n\n\n\n\n","category":"method"},{"location":"composition/#StatsBase.fit!-Tuple{Node}","page":"Composition","title":"StatsBase.fit!","text":"fit!(N::Node; rows=nothing, verbosity::Int=1, force::Bool=false)\n\nTrain all machines in the learning network terminating at node N, in an appropriate order. These machines are those returned by machines(N).\n\n\n\n\n\n","category":"method"},{"location":"composition/#MLJBase.args-Tuple{Any}","page":"Composition","title":"MLJBase.args","text":"args(tree; train=false)\n\nReturn a vector of the top level args of the tree associated with a node. If train=true, return the train_args.\n\n\n\n\n\n","category":"method"},{"location":"composition/#MLJBase.is_stale-Tuple{NodalMachine}","page":"Composition","title":"MLJBase.is_stale","text":"is_stale(mach)\n\nCheck if a machine mach is stale.\n\nSee also fit!\n\n\n\n\n\n","category":"method"},{"location":"composition/#MLJBase.is_stale-Tuple{Node}","page":"Composition","title":"MLJBase.is_stale","text":"is_stale(N)\n\nCheck if a node N is stale.\n\n\n\n\n\n","category":"method"},{"location":"composition/#MLJBase.reset!-Tuple{Node}","page":"Composition","title":"MLJBase.reset!","text":"reset!(N::Node)\n\nPlace the learning network terminating at node N into a state in which fit!(N) will retrain from scratch all machines in its dependency tape. Does not actually train any machine or alter fit-results. (The method simply resets m.state to zero, for every machine m in the network.)\n\n\n\n\n\n","category":"method"},{"location":"composition/#MLJBase.state-Tuple{NodalMachine}","page":"Composition","title":"MLJBase.state","text":"state(mach)\n\nReturn the state of a machine, mach.\n\n\n\n\n\n","category":"method"},{"location":"composition/#MLJBase.state-Tuple{Node}","page":"Composition","title":"MLJBase.state","text":"state(N)\n\nReturn the state of a node N\n\n\n\n\n\n","category":"method"},{"location":"composition/#MLJBase.tree-Tuple{Node}","page":"Composition","title":"MLJBase.tree","text":"tree(N)\n\nReturn a description of the tree N defined by the learning network terminating at a given node.\n\n\n\n\n\n","category":"method"},{"location":"composition/#Pipelines-1","page":"Composition","title":"Pipelines","text":"","category":"section"},{"location":"composition/#","page":"Composition","title":"Composition","text":"Modules = [MLJBase]\nPages   = [\"composition/pipeline_static.jl\", \"composition/pipelines.jl\"]","category":"page"},{"location":"composition/#MLJBase.@pipeline-Tuple","page":"Composition","title":"MLJBase.@pipeline","text":"@pipeline NewPipeType(fld1=model1, fld2=model2, ...)\n@pipeline NewPipeType(fld1=model1, fld2=model2, ...) prediction_type=:probabilistic\n\nCreate a new pipeline model type NewPipeType that composes the types of the specified models model1, model2, ... . The models are composed in the specified order, meaning the input(s) of the pipeline goes to model1, whose output is sent to model2, and so forth.\n\nAt most one of the models may be a supervised model, in which case NewPipeType is supervised. Otherwise it is unsupervised.\n\nThe new model type NewPipeType has hyperparameters (fields) named :fld1, :fld2, ..., whose default values for an automatically generated keyword constructor are deep copies of model1, model2, ... .\n\nImportant. If the overall pipeline is supervised and makes probabilistic predictions, then one must declare prediction_type=:probabilistic. In the deterministic case no declaration is necessary.\n\nStatic (unlearned) transformations - that is, ordinary functions - may also be inserted in the pipeline as shown in the following example (the classifier is probabilistic but the pipeline itself is deterministic):\n\n@pipeline MyPipe(X -> coerce(X, :age=>Continuous),\n                 hot=OneHotEncoder(),\n                 cnst=ConstantClassifier(),\n                 yhat -> mode.(yhat))\n\nReturn value\n\nAn instance of the new type, with default hyperparameters (see above), is returned.\n\nTarget transformation and inverse transformation\n\nA learned target transformation (such as standardization) can also be specified, using the keyword target, provided the transformer provides an inverse_transform method:\n\n@load KNNRegressor\n@pipeline MyPipe(hot=OneHotEncoder(),\n                 knn=KNNRegressor(),\n                 target=UnivariateTransformer())\n\nA static transformation can be specified instead, but then an inverse must also be given:\n\n@load KNNRegressor\n@pipeline MyPipe(hot=OneHotEncoder(),\n                 knn=KNNRegressor(),\n                 target = v -> log.(v),\n                 inverse = v -> exp.(v))\n\nImportant. While the supervised model in a pipeline providing a  target transformation can appear anywhere in the pipeline (as in  ConstantClassifier example above), the inverse operation is always  performed on the output of the final model or static  transformation in the pipeline.\n\nSee also: @from_network\n\n\n\n\n\n","category":"macro"},{"location":"composition/#MLJBase.StaticTransformer","page":"Composition","title":"MLJBase.StaticTransformer","text":"Applies a given data transformation f (either a function or callable).\n\nField\n\nf=identity: function or callable object to use for the data transformation.\n\n\n\n\n\n","category":"type"},{"location":"#MLJBase.jl-1","page":"Home","title":"MLJBase.jl","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"MLJ is a Julia framework for combining and tuning machine learning models. This repository provides core functionality for MLJ, including:","category":"page"},{"location":"#","page":"Home","title":"Home","text":"completing the functionality for methods defined \"minimally\" in MLJ's light-weight model interface MLJModelInterface\ndefinition of machines and their associated methods, such as fit! and predict/transform\nMLJ's model composition interface, including learning networks and pipelines\nbasic utilities for manipulating data\nan extension to Distributions.jl called UnivariateFinite for randomly sampling labeled categorical data\na small interface for resampling strategies and implementations, including CV(), StratifiedCV and Holdout\nmethods for performance evaluation, based on those resampling strategies\none-dimensional hyperparameter range types, constructors and associated methods, for use with MLJTuning\na small interface for performance measures (losses and scores), enabling the integration of the LossFunctions.jl library, user-defined measures, as well as about two dozen natively defined measures.\nintegration with OpenML","category":"page"},{"location":"#","page":"Home","title":"Home","text":"Previously MLJBase provided the model interface for integrating third party machine learning models into MLJ. That role has now shifted to the light-weight MLJModelInterface package.","category":"page"},{"location":"datasets/#Datasets-1","page":"Datasets","title":"Datasets","text":"","category":"section"},{"location":"datasets/#","page":"Datasets","title":"Datasets","text":"Pages   = [\"data/datasets_synthetic.jl\"]","category":"page"},{"location":"datasets/#Standard-datasets-1","page":"Datasets","title":"Standard datasets","text":"","category":"section"},{"location":"datasets/#","page":"Datasets","title":"Datasets","text":"To add a new dataset assuming it has a header and is, at path data/newdataset.csv","category":"page"},{"location":"datasets/#","page":"Datasets","title":"Datasets","text":"Start by loading it with CSV:","category":"page"},{"location":"datasets/#","page":"Datasets","title":"Datasets","text":"fpath = joinpath(\"datadir\", \"newdataset.csv\")\ndata = CSV.read(fpath, copycols=true,\n                categorical=true)","category":"page"},{"location":"datasets/#","page":"Datasets","title":"Datasets","text":"Load it with DelimitedFiles and Tables","category":"page"},{"location":"datasets/#","page":"Datasets","title":"Datasets","text":"data_raw, data_header = readdlm(fpath, ',', header=true)\ndata_table = Tables.table(data_raw; header=Symbol.(vec(data_header)))","category":"page"},{"location":"datasets/#","page":"Datasets","title":"Datasets","text":"Retrieve the conversions:","category":"page"},{"location":"datasets/#","page":"Datasets","title":"Datasets","text":"for (n, st) in zip(names(data), scitype_union.(eachcol(data)))\n    println(\":$n=>$st,\")\nend","category":"page"},{"location":"datasets/#","page":"Datasets","title":"Datasets","text":"Copy and paste the result in a coerce","category":"page"},{"location":"datasets/#","page":"Datasets","title":"Datasets","text":"data_table = coerce(data_table, ...)","category":"page"},{"location":"datasets/#","page":"Datasets","title":"Datasets","text":"Modules = [MLJBase]\nPages   = [\"data/datasets.jl\"]","category":"page"},{"location":"datasets/#MLJBase.@load_ames-Tuple{}","page":"Datasets","title":"MLJBase.@load_ames","text":"Load the full version of the well-known Ames Housing task.\n\n\n\n\n\n","category":"macro"},{"location":"datasets/#MLJBase.@load_boston-Tuple{}","page":"Datasets","title":"MLJBase.@load_boston","text":"Load a well-known public regression dataset with Continuous features.\n\n\n\n\n\n","category":"macro"},{"location":"datasets/#MLJBase.@load_crabs-Tuple{}","page":"Datasets","title":"MLJBase.@load_crabs","text":"Load a well-known crab classification dataset with nominal features.\n\n\n\n\n\n","category":"macro"},{"location":"datasets/#MLJBase.@load_iris-Tuple{}","page":"Datasets","title":"MLJBase.@load_iris","text":"Load a well-known public classification task with nominal features.\n\n\n\n\n\n","category":"macro"},{"location":"datasets/#MLJBase.@load_reduced_ames-Tuple{}","page":"Datasets","title":"MLJBase.@load_reduced_ames","text":"Load a reduced version of the well-known Ames Housing task\n\n\n\n\n\n","category":"macro"},{"location":"datasets/#MLJBase.@load_smarket-Tuple{}","page":"Datasets","title":"MLJBase.@load_smarket","text":"Load S&P Stock Market dataset, as used in (An Introduction to Statistical Learning with applications in R)https://rdrr.io/cran/ISLR/man/Smarket.html, by Witten et al (2013), Springer-Verlag, New York.\n\n\n\n\n\n","category":"macro"},{"location":"datasets/#MLJBase.load_dataset-Tuple{String,Tuple}","page":"Datasets","title":"MLJBase.load_dataset","text":"load_dataset(fpath, coercions)\n\nLoad one of standard dataset like Boston etc assuming the file is a comma separated file with a header.\n\n\n\n\n\n","category":"method"},{"location":"datasets/#Synthetic-datasets-1","page":"Datasets","title":"Synthetic datasets","text":"","category":"section"},{"location":"datasets/#","page":"Datasets","title":"Datasets","text":"Modules = [MLJBase]\nPages   = [\"data/datasets_synthetic.jl\"]","category":"page"},{"location":"datasets/#MLJBase.make_blobs","page":"Datasets","title":"MLJBase.make_blobs","text":"make_blobs(n=100, p=2; kwargs...)\n\nGenerate gaussian blobs with n examples of p features. The function returns a n x p matrix with the samples and a n integer vector indicating the membership of each point.\n\nKeyword arguments\n\nshuffle=true:             whether to shuffle the resulting points,\ncenters=3:                either a number of centers or a c x p matrix with c pre-determined centers,\ncluster_std=1.0:          the standard deviation(s) of each blob,\ncenter_box=(-10. => 10.): the limits of the p-dimensional cube within which the cluster centers are drawn if they are not provided,\nas_table=true:  whether to return the points as a table (true) or a\n\n\t\t\t\tmatrix (false). If true, the target vector is a\n\t\t\t\tcategorical vector.\n\neltype=Float64:\tto specify another type for the points, can be any\n\nsubtype of AbstractFloat.\n\nrng=nothing:    specify a number to make the points reproducible.\n\nExample\n\nX, y = make_blobs(100, 3; centers=2, cluster_std=[1.0, 3.0])\n\n\n\n\n\n","category":"function"},{"location":"datasets/#MLJBase.make_circles","page":"Datasets","title":"MLJBase.make_circles","text":"make_circles(n=100; kwargs...)\n\nGenerate n points along two circumscribed circles returning the n x 2 matrix of points and a vector of membership (0, 1) depending on whether the points are on the smaller circle (0) or the larger one (1).\n\nKeyword arguments\n\nshuffle=true:   whether to shuffle the resulting points,\nnoise=0:        standard deviation of the gaussian noise added to the data,\nfactor=0.8:     ratio of the smaller radius over the larger one,\nas_table=true:  whether to return the points as a table (true) or a\n\n\t\t\t\tmatrix (false). If true, the target vector is a\n\t\t\t\tcategorical vector.\n\neltype=Float64:\tto specify another type for the points, can be any\n\nsubtype of AbstractFloat.\n\nrng=nothing:    specify a number to make the points reproducible.\n\nExample\n\nX, y = make_circles(100; noise=0.5, factor=0.3)\n\n\n\n\n\n","category":"function"},{"location":"datasets/#MLJBase.make_moons","page":"Datasets","title":"MLJBase.make_moons","text":"make_moons(n::Int=100; kwargs...)\n\nGenerates n examples sampling from two interleaved half-circles returning the n x 2 matrix of points and a vector of membership (0, 1) depending on whether the points are on the half-circle on the left (0) or on the right (1).\n\nKeyword arguments\n\nshuffle=true:   whether to shuffle the resulting points,\nnoise=0.1:      standard deviation of the gaussian noise added to the data,\nxshift=1.0:     horizontal translation of the second center with respect to                   the first one.\nyshift=0.3:     vertical translation of the second center with respect to                   the first one.\nas_table=true:  whether to return the points as a table (true) or a\n\n\t\t\t\tmatrix (false). If true, the target vector is a\n\t\t\t\tcategorical vector.\n\neltype=Float64:\tto specify another type for the points, can be any\n\nsubtype of AbstractFloat.\n\nrng=nothing:    specify a number to make the points reproducible.\n\nExample\n\nX, y = make_moons(100; noise=0.5)\n\n\n\n\n\n","category":"function"},{"location":"datasets/#MLJBase.make_regression","page":"Datasets","title":"MLJBase.make_regression","text":"make_regression(n, p; kwargs...)\n\nKeywords\n\nintercept=true:\twhether to generate data from a model with intercept,\nsparse=0:\t\tportion of the generating weight vector that is sparse,\nnoise=0.1:\t\tstandard deviation of the gaussian noise added to the\n\nresponse,\n\noutliers=0:\t\tportion of the response vector to make as outliers by ading\n\n\t\t\t\ta random quantity with high variance. (Only applied if\n\t\t\t\t`binary` is `false`)\n\nbinary=false:\twhether the target should be binarized (via a sigmoid).\nas_table=true:  whether to return the points as a table (true) or a\n\n\t\t\t\tmatrix (false). If true, the target vector is a\n\t\t\t\tcategorical vector.\n\neltype=Float64:\tto specify another type for the points, can be any\n\nsubtype of AbstractFloat.\n\nrng=nothing:    specify a number to make the points reproducible.\n\nExample\n\nX, y = make_regression(100, 5; noise=0.5, sparse=0.2, outliers=0.1)\n\n\n\n\n\n","category":"function"},{"location":"datasets/#MLJBase.augment_X-Tuple{Array{#s362,2} where #s362<:Real,Bool}","page":"Datasets","title":"MLJBase.augment_X","text":"augmentX(X, fitintercept)\n\nGiven a matrix X, append a column of ones if fit_intercept is true. See make_regression.\n\n\n\n\n\n","category":"method"},{"location":"datasets/#MLJBase.finalize_Xy-NTuple{6,Any}","page":"Datasets","title":"MLJBase.finalize_Xy","text":"finalizeXy(X, y, shuffle, astable, eltype, rng; clf)\n\nInternal function to  finalize the make_* functions.\n\n\n\n\n\n","category":"method"},{"location":"datasets/#MLJBase.outlify!-Tuple{Any,Any}","page":"Datasets","title":"MLJBase.outlify!","text":"Add outliers to portion s of vector.\n\n\n\n\n\n","category":"method"},{"location":"datasets/#MLJBase.runif_ab-NTuple{4,Any}","page":"Datasets","title":"MLJBase.runif_ab","text":"runif_ab(n, p, a, b)\n\nInternal function to generate n points in [a, b]ᵖ uniformly at random.\n\n\n\n\n\n","category":"method"},{"location":"datasets/#MLJBase.sigmoid-Tuple{Float64}","page":"Datasets","title":"MLJBase.sigmoid","text":"sigmoid(x)\n\nReturn the sigmoid computed in a numerically stable way:\n\nσ(x) = 1(1+exp(-x))\n\n\n\n\n\n","category":"method"},{"location":"datasets/#MLJBase.sparsify!-Tuple{Any,Any}","page":"Datasets","title":"MLJBase.sparsify!","text":"sparsify!(θ, s)\n\nMake portion s of vector θ exactly 0.\n\n\n\n\n\n","category":"method"},{"location":"datasets/#Utility-functions-1","page":"Datasets","title":"Utility functions","text":"","category":"section"},{"location":"datasets/#","page":"Datasets","title":"Datasets","text":"Modules = [MLJBase]\nPages   = [\"data/data.jl\"]","category":"page"},{"location":"datasets/#MLJBase.complement-Tuple{Any,Any}","page":"Datasets","title":"MLJBase.complement","text":"complement(folds, i)\n\nThe complement of the ith fold of folds in the concatenation of all elements of folds. Here folds is a vector or tuple of integer vectors, typically representing row indices or a vector, matrix or table.\n\ncomplement(([1,2], [3,], [4, 5]), 2) # [1 ,2, 4, 5]\n\n\n\n\n\n","category":"method"},{"location":"datasets/#MLJBase.corestrict-Union{Tuple{N}, Tuple{Tuple{Vararg{T,N}} where T,Any}} where N","page":"Datasets","title":"MLJBase.corestrict","text":"corestrict(X, folds, i)\n\nThe restriction of X, a vector, matrix or table, to the complement of the ith fold of folds, where folds is a tuple of vectors of row indices.\n\nThe method is curried, so that corestrict(folds, i) is the operator on data defined by corestrict(folds, i)(X) = corestrict(X, folds, i).\n\nExample\n\nfolds = ([1, 2], [3, 4, 5],  [6,])\ncorestrict([:x1, :x2, :x3, :x4, :x5, :x6], folds, 2) # [:x1, :x2, :x6]\n\n\n\n\n\n","category":"method"},{"location":"datasets/#MLJBase.partition-Tuple{AbstractArray{Int64,1},Vararg{Real,N} where N}","page":"Datasets","title":"MLJBase.partition","text":"partition(rows::AbstractVector{Int}, fractions...;\n          shuffle=nothing, rng=Random.GLOBAL_RNG)\n\nSplits the vector rows into a tuple of vectors whose lengths are given by the corresponding fractions of length(rows) where valid fractions are in (0,1) and sum up to less than 1. The last fraction is not provided, as it is inferred from the preceding ones. So, for example,\n\njulia> partition(1:1000, 0.8)\n([1,...,800], [801,...,1000])\n\njulia> partition(1:1000, 0.2, 0.7)\n([1,...,200], [201,...,900], [901,...,1000])\n\nKeywords\n\nshuffle=nothing:        if set to  true, shuffles the rows before taking fractions.\nrng=Random.GLOBAL_RNG:  specifies the random number generator to be used, can be an integer                           seed. If specified, and shuffle === nothing is interpreted as true.\nstratify=nothing:       if a vector is specified, the partition will match the stratification                           of the given vector. In that case, shuffle cannot be false.\n\n\n\n\n\n","category":"method"},{"location":"datasets/#MLJBase.restrict-Union{Tuple{N}, Tuple{Tuple{Vararg{T,N}} where T,Any}} where N","page":"Datasets","title":"MLJBase.restrict","text":"restrict(X, folds, i)\n\nThe restriction of X, a vector, matrix or table, to the ith fold of folds, where folds is a tuple of vectors of row indices.\n\nThe method is curried, so that restrict(folds, i) is the operator on data defined by restrict(folds, i)(X) = restrict(X, folds, i).\n\nExample\n\nfolds = ([1, 2], [3, 4, 5],  [6,])\nrestrict([:x1, :x2, :x3, :x4, :x5, :x6], folds, 2) # [:x3, :x4, :x5]\n\nSee also corestrict\n\n\n\n\n\n","category":"method"},{"location":"datasets/#MLJBase.unpack-Tuple{Any,Vararg{Any,N} where N}","page":"Datasets","title":"MLJBase.unpack","text":"t1, t2, ...., tk = unnpack(table, t1, t2, ... tk; wrap_singles=false)\n\nSplit any Tables.jl compatible table into smaller tables (or vectors) t1, t2, ..., tk by making selections without replacement from the column names defined by the tests t1, t2, ..., tk. A test is any object t such that t(name) is true or false for each column name::Symbol of table.\n\nWhenever a returned table contains a single column, it is converted to a vector unless wrap_singles=true.\n\nScientific type conversions can be optionally specified (note semicolon):\n\nunpack(table, t...; wrap_singles=false, col1=>scitype1, col2=>scitype2, ... )\n\nExample\n\njulia> table = DataFrame(x=[1,2], y=['a', 'b'], z=[10.0, 20.0], w=[:A, :B])\njulia> Z, XY = unpack(table, ==(:z), !=(:w);\n               :x=>Continuous, :y=>Multiclass)\njulia> XY\n2×2 DataFrame\n│ Row │ x       │ y            │\n│     │ Float64 │ Categorical… │\n├─────┼─────────┼──────────────┤\n│ 1   │ 1.0     │ 'a'          │\n│ 2   │ 2.0     │ 'b'          │\n\njulia> Z\n2-element Array{Float64,1}:\n 10.0\n 20.0\n\n\n\n\n\n","category":"method"}]
}
